Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:47:55 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:47:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:47:56 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:47:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:47:56 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:47:56 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:47:56 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:47:56 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:47:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:47:56 INFO Utils: Successfully started service 'sparkDriver' on port 34063.
16/10/30 09:47:56 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:47:56 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:47:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7ab4dc56-89ca-4bdb-9bd6-f54a1743a9c5
16/10/30 09:47:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:47:56 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:47:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:47:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:47:56 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34063/jars/jni-spark-0.1.jar with timestamp 1477817276889
16/10/30 09:47:56 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:47:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46031.
16/10/30 09:47:56 INFO NettyBlockTransferService: Server created on 192.168.0.17:46031
16/10/30 09:47:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46031)
16/10/30 09:47:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46031 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46031)
16/10/30 09:47:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46031)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:47:58 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817278578
16/10/30 09:47:58 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a46610ac-be7d-4d86-8a48-d799b2d7d520/userFiles-c1b9d2bb-c104-4873-b694-94ca648f3e70/SparkJNIPi.so
16/10/30 09:47:58 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:47:58 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:47:58 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:47:58 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:47:58 INFO DAGScheduler: Missing parents: List()
16/10/30 09:47:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:47:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:47:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:47:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46031 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:47:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:47:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:47:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:47:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:47:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:47:59 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817278578
16/10/30 09:47:59 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a46610ac-be7d-4d86-8a48-d799b2d7d520/userFiles-c1b9d2bb-c104-4873-b694-94ca648f3e70/SparkJNIPi.so
16/10/30 09:47:59 INFO Executor: Fetching spark://192.168.0.17:34063/jars/jni-spark-0.1.jar with timestamp 1477817276889
16/10/30 09:47:59 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34063 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:47:59 INFO Utils: Fetching spark://192.168.0.17:34063/jars/jni-spark-0.1.jar to /tmp/spark-a46610ac-be7d-4d86-8a48-d799b2d7d520/userFiles-c1b9d2bb-c104-4873-b694-94ca648f3e70/fetchFileTemp6789211950983904403.tmp
16/10/30 09:47:59 INFO Executor: Adding file:/tmp/spark-a46610ac-be7d-4d86-8a48-d799b2d7d520/userFiles-c1b9d2bb-c104-4873-b694-94ca648f3e70/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:47:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:47:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 340 ms on localhost (1/1)
16/10/30 09:47:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:47:59 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.354 s
16/10/30 09:47:59 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.629187 s
Result: 3.1914062 in 0.862 seconds
16/10/30 09:47:59 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:47:59 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:47:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:47:59 INFO MemoryStore: MemoryStore cleared
16/10/30 09:47:59 INFO BlockManager: BlockManager stopped
16/10/30 09:47:59 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:47:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:47:59 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:47:59 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:47:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-a46610ac-be7d-4d86-8a48-d799b2d7d520
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:00 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:01 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:01 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:01 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:01 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:01 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:01 INFO Utils: Successfully started service 'sparkDriver' on port 33810.
16/10/30 09:48:01 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:01 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d26bec44-7f5d-446e-843a-a93022416e64
16/10/30 09:48:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:01 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:01 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33810/jars/jni-spark-0.1.jar with timestamp 1477817281988
16/10/30 09:48:02 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39656.
16/10/30 09:48:02 INFO NettyBlockTransferService: Server created on 192.168.0.17:39656
16/10/30 09:48:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39656)
16/10/30 09:48:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39656 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39656)
16/10/30 09:48:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39656)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:03 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817283774
16/10/30 09:48:03 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a3e0ac10-abe2-458d-af9e-ecb76d0041c8/userFiles-e49837dc-e201-49b2-9188-985453dc5a3d/SparkJNIPi.so
16/10/30 09:48:04 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:04 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:04 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:04 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:04 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39656 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:04 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817283774
16/10/30 09:48:04 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a3e0ac10-abe2-458d-af9e-ecb76d0041c8/userFiles-e49837dc-e201-49b2-9188-985453dc5a3d/SparkJNIPi.so
16/10/30 09:48:04 INFO Executor: Fetching spark://192.168.0.17:33810/jars/jni-spark-0.1.jar with timestamp 1477817281988
16/10/30 09:48:04 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33810 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:48:04 INFO Utils: Fetching spark://192.168.0.17:33810/jars/jni-spark-0.1.jar to /tmp/spark-a3e0ac10-abe2-458d-af9e-ecb76d0041c8/userFiles-e49837dc-e201-49b2-9188-985453dc5a3d/fetchFileTemp8146667099387764014.tmp
16/10/30 09:48:04 INFO Executor: Adding file:/tmp/spark-a3e0ac10-abe2-458d-af9e-ecb76d0041c8/userFiles-e49837dc-e201-49b2-9188-985453dc5a3d/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:48:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 386 ms on localhost (1/1)
16/10/30 09:48:04 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.400 s
16/10/30 09:48:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:04 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.672766 s
Result: 3.1132812 in 0.948 seconds
16/10/30 09:48:04 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:04 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:04 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:04 INFO BlockManager: BlockManager stopped
16/10/30 09:48:04 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:04 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:04 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3e0ac10-abe2-458d-af9e-ecb76d0041c8
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:05 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:06 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:06 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:06 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:06 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:06 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:06 INFO Utils: Successfully started service 'sparkDriver' on port 34739.
16/10/30 09:48:06 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:06 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-816b6dec-9610-4728-8495-d2b5157fedbb
16/10/30 09:48:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:06 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:07 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34739/jars/jni-spark-0.1.jar with timestamp 1477817287104
16/10/30 09:48:07 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41709.
16/10/30 09:48:07 INFO NettyBlockTransferService: Server created on 192.168.0.17:41709
16/10/30 09:48:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41709)
16/10/30 09:48:07 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41709 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41709)
16/10/30 09:48:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41709)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:08 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817288815
16/10/30 09:48:08 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2992fa01-3158-446b-8a50-b4c13f40bd3e/userFiles-878e1b9f-d706-4535-8b9d-9083b18ddfd3/SparkJNIPi.so
16/10/30 09:48:09 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:09 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:09 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:09 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:09 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41709 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:09 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817288815
16/10/30 09:48:09 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2992fa01-3158-446b-8a50-b4c13f40bd3e/userFiles-878e1b9f-d706-4535-8b9d-9083b18ddfd3/SparkJNIPi.so
16/10/30 09:48:09 INFO Executor: Fetching spark://192.168.0.17:34739/jars/jni-spark-0.1.jar with timestamp 1477817287104
16/10/30 09:48:09 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34739 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:48:09 INFO Utils: Fetching spark://192.168.0.17:34739/jars/jni-spark-0.1.jar to /tmp/spark-2992fa01-3158-446b-8a50-b4c13f40bd3e/userFiles-878e1b9f-d706-4535-8b9d-9083b18ddfd3/fetchFileTemp6743506440400299155.tmp
16/10/30 09:48:09 INFO Executor: Adding file:/tmp/spark-2992fa01-3158-446b-8a50-b4c13f40bd3e/userFiles-878e1b9f-d706-4535-8b9d-9083b18ddfd3/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:48:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 351 ms on localhost (1/1)
16/10/30 09:48:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:09 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.374 s
16/10/30 09:48:09 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.646139 s
Result: 3.203125 in 0.915 seconds
16/10/30 09:48:09 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:09 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:09 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:09 INFO BlockManager: BlockManager stopped
16/10/30 09:48:09 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:09 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:09 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-2992fa01-3158-446b-8a50-b4c13f40bd3e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:11 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:11 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:11 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:11 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:11 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:11 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:11 INFO Utils: Successfully started service 'sparkDriver' on port 44971.
16/10/30 09:48:11 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:11 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0d4c56e1-46d4-425a-8d84-5485bdd47332
16/10/30 09:48:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:12 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:12 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44971/jars/jni-spark-0.1.jar with timestamp 1477817292383
16/10/30 09:48:12 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33356.
16/10/30 09:48:12 INFO NettyBlockTransferService: Server created on 192.168.0.17:33356
16/10/30 09:48:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33356)
16/10/30 09:48:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33356 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33356)
16/10/30 09:48:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33356)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:14 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817294594
16/10/30 09:48:14 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-f345dbd5-af87-42b3-83ad-20240f9731d6/userFiles-6d5272c1-80fe-48e0-a1e1-bf6f4ec875e9/SparkJNIPi.so
16/10/30 09:48:14 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:14 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:14 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:14 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:14 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33356 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:15 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817294594
16/10/30 09:48:15 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-f345dbd5-af87-42b3-83ad-20240f9731d6/userFiles-6d5272c1-80fe-48e0-a1e1-bf6f4ec875e9/SparkJNIPi.so
16/10/30 09:48:15 INFO Executor: Fetching spark://192.168.0.17:44971/jars/jni-spark-0.1.jar with timestamp 1477817292383
16/10/30 09:48:15 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44971 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:48:15 INFO Utils: Fetching spark://192.168.0.17:44971/jars/jni-spark-0.1.jar to /tmp/spark-f345dbd5-af87-42b3-83ad-20240f9731d6/userFiles-6d5272c1-80fe-48e0-a1e1-bf6f4ec875e9/fetchFileTemp3341868279626867151.tmp
16/10/30 09:48:15 INFO Executor: Adding file:/tmp/spark-f345dbd5-af87-42b3-83ad-20240f9731d6/userFiles-6d5272c1-80fe-48e0-a1e1-bf6f4ec875e9/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:48:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 423 ms on localhost (1/1)
16/10/30 09:48:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:15 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.443 s
16/10/30 09:48:15 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.745556 s
Result: 3.1015625 in 1.032 seconds
16/10/30 09:48:15 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:15 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:15 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:15 INFO BlockManager: BlockManager stopped
16/10/30 09:48:15 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:15 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:15 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-f345dbd5-af87-42b3-83ad-20240f9731d6
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:16 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:17 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:17 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:17 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:17 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:17 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:17 INFO Utils: Successfully started service 'sparkDriver' on port 43792.
16/10/30 09:48:17 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:17 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-58b22931-92c7-4cdb-8a31-ffa9ebd8459c
16/10/30 09:48:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:17 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:18 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43792/jars/jni-spark-0.1.jar with timestamp 1477817298071
16/10/30 09:48:18 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38161.
16/10/30 09:48:18 INFO NettyBlockTransferService: Server created on 192.168.0.17:38161
16/10/30 09:48:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38161)
16/10/30 09:48:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38161 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38161)
16/10/30 09:48:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38161)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:19 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817299794
16/10/30 09:48:19 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-33b98d3a-d60e-4063-995d-ca9298cb2a91/userFiles-d84eec94-42f6-4b09-ba8c-7bb4981d1e72/SparkJNIPi.so
16/10/30 09:48:20 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:20 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:20 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:20 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:20 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38161 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:20 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817299794
16/10/30 09:48:20 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-33b98d3a-d60e-4063-995d-ca9298cb2a91/userFiles-d84eec94-42f6-4b09-ba8c-7bb4981d1e72/SparkJNIPi.so
16/10/30 09:48:20 INFO Executor: Fetching spark://192.168.0.17:43792/jars/jni-spark-0.1.jar with timestamp 1477817298071
16/10/30 09:48:20 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43792 after 27 ms (0 ms spent in bootstraps)
16/10/30 09:48:20 INFO Utils: Fetching spark://192.168.0.17:43792/jars/jni-spark-0.1.jar to /tmp/spark-33b98d3a-d60e-4063-995d-ca9298cb2a91/userFiles-d84eec94-42f6-4b09-ba8c-7bb4981d1e72/fetchFileTemp7868332872139570939.tmp
16/10/30 09:48:20 INFO Executor: Adding file:/tmp/spark-33b98d3a-d60e-4063-995d-ca9298cb2a91/userFiles-d84eec94-42f6-4b09-ba8c-7bb4981d1e72/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:48:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 349 ms on localhost (1/1)
16/10/30 09:48:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:20 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.363 s
16/10/30 09:48:20 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.652121 s
Result: 3.1264648 in 0.929 seconds
16/10/30 09:48:20 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:20 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:20 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:20 INFO BlockManager: BlockManager stopped
16/10/30 09:48:20 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:20 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:20 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-33b98d3a-d60e-4063-995d-ca9298cb2a91
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:21 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:22 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:22 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:22 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:22 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:22 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:22 INFO Utils: Successfully started service 'sparkDriver' on port 46075.
16/10/30 09:48:22 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:22 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54e0c799-86c2-45b8-978c-af4bedf33eea
16/10/30 09:48:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:22 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:23 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46075/jars/jni-spark-0.1.jar with timestamp 1477817303177
16/10/30 09:48:23 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45072.
16/10/30 09:48:23 INFO NettyBlockTransferService: Server created on 192.168.0.17:45072
16/10/30 09:48:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45072)
16/10/30 09:48:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45072 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45072)
16/10/30 09:48:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45072)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:24 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817304900
16/10/30 09:48:24 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-da171194-b12a-49fb-8c17-c7bfbf965795/userFiles-91b7136b-32f3-41b6-843e-bc930a544714/SparkJNIPi.so
16/10/30 09:48:25 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:25 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:25 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:25 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:25 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45072 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:25 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817304900
16/10/30 09:48:25 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-da171194-b12a-49fb-8c17-c7bfbf965795/userFiles-91b7136b-32f3-41b6-843e-bc930a544714/SparkJNIPi.so
16/10/30 09:48:25 INFO Executor: Fetching spark://192.168.0.17:46075/jars/jni-spark-0.1.jar with timestamp 1477817303177
16/10/30 09:48:25 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46075 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:48:25 INFO Utils: Fetching spark://192.168.0.17:46075/jars/jni-spark-0.1.jar to /tmp/spark-da171194-b12a-49fb-8c17-c7bfbf965795/userFiles-91b7136b-32f3-41b6-843e-bc930a544714/fetchFileTemp6197761533883984478.tmp
16/10/30 09:48:25 INFO Executor: Adding file:/tmp/spark-da171194-b12a-49fb-8c17-c7bfbf965795/userFiles-91b7136b-32f3-41b6-843e-bc930a544714/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:25 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:48:25 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 347 ms on localhost (1/1)
16/10/30 09:48:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:25 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.375 s
16/10/30 09:48:25 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.679673 s
Result: 3.1274414 in 0.942 seconds
16/10/30 09:48:25 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:25 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:25 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:25 INFO BlockManager: BlockManager stopped
16/10/30 09:48:25 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:25 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:25 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-da171194-b12a-49fb-8c17-c7bfbf965795
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:27 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:27 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:27 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:27 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:27 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:27 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:27 INFO Utils: Successfully started service 'sparkDriver' on port 42902.
16/10/30 09:48:27 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:27 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ddb4c368-132f-4f91-bc58-19bd35225143
16/10/30 09:48:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:28 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:28 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42902/jars/jni-spark-0.1.jar with timestamp 1477817308314
16/10/30 09:48:28 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43950.
16/10/30 09:48:28 INFO NettyBlockTransferService: Server created on 192.168.0.17:43950
16/10/30 09:48:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43950)
16/10/30 09:48:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43950 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43950)
16/10/30 09:48:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43950)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:30 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817310111
16/10/30 09:48:30 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-e9d8efbf-8a81-4ffe-9fc3-b14093a918d4/userFiles-f3c6f329-308e-4720-b2cd-ba63d1c1eac5/SparkJNIPi.so
16/10/30 09:48:30 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:30 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:30 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:30 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:30 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:30 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:30 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43950 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:30 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817310111
16/10/30 09:48:30 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-e9d8efbf-8a81-4ffe-9fc3-b14093a918d4/userFiles-f3c6f329-308e-4720-b2cd-ba63d1c1eac5/SparkJNIPi.so
16/10/30 09:48:30 INFO Executor: Fetching spark://192.168.0.17:42902/jars/jni-spark-0.1.jar with timestamp 1477817308314
16/10/30 09:48:30 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42902 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:48:30 INFO Utils: Fetching spark://192.168.0.17:42902/jars/jni-spark-0.1.jar to /tmp/spark-e9d8efbf-8a81-4ffe-9fc3-b14093a918d4/userFiles-f3c6f329-308e-4720-b2cd-ba63d1c1eac5/fetchFileTemp379660304008761460.tmp
16/10/30 09:48:30 INFO Executor: Adding file:/tmp/spark-e9d8efbf-8a81-4ffe-9fc3-b14093a918d4/userFiles-f3c6f329-308e-4720-b2cd-ba63d1c1eac5/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:48:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 356 ms on localhost (1/1)
16/10/30 09:48:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:31 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.376 s
16/10/30 09:48:31 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.630855 s
Result: 3.1552734 in 0.928 seconds
16/10/30 09:48:31 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:31 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:31 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:31 INFO BlockManager: BlockManager stopped
16/10/30 09:48:31 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:31 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:31 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-e9d8efbf-8a81-4ffe-9fc3-b14093a918d4
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:32 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:32 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:32 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:32 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:32 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:32 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:33 INFO Utils: Successfully started service 'sparkDriver' on port 39852.
16/10/30 09:48:33 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:33 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:33 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bee66d8c-6a70-456d-b6ca-052aae38c96d
16/10/30 09:48:33 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:33 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:33 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39852/jars/jni-spark-0.1.jar with timestamp 1477817313488
16/10/30 09:48:33 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44554.
16/10/30 09:48:33 INFO NettyBlockTransferService: Server created on 192.168.0.17:44554
16/10/30 09:48:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44554)
16/10/30 09:48:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44554 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44554)
16/10/30 09:48:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44554)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:35 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817315080
16/10/30 09:48:35 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a97896f3-bca9-4588-a86f-0b474f73d484/userFiles-62aeb3cd-82aa-4b78-b79e-dc412c1f2683/SparkJNIPi.so
16/10/30 09:48:35 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:35 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:35 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:35 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:35 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44554 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:35 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817315080
16/10/30 09:48:35 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a97896f3-bca9-4588-a86f-0b474f73d484/userFiles-62aeb3cd-82aa-4b78-b79e-dc412c1f2683/SparkJNIPi.so
16/10/30 09:48:35 INFO Executor: Fetching spark://192.168.0.17:39852/jars/jni-spark-0.1.jar with timestamp 1477817313488
16/10/30 09:48:35 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39852 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:48:35 INFO Utils: Fetching spark://192.168.0.17:39852/jars/jni-spark-0.1.jar to /tmp/spark-a97896f3-bca9-4588-a86f-0b474f73d484/userFiles-62aeb3cd-82aa-4b78-b79e-dc412c1f2683/fetchFileTemp2943421847900419646.tmp
16/10/30 09:48:35 INFO Executor: Adding file:/tmp/spark-a97896f3-bca9-4588-a86f-0b474f73d484/userFiles-62aeb3cd-82aa-4b78-b79e-dc412c1f2683/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:48:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 322 ms on localhost (1/1)
16/10/30 09:48:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:35 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.339 s
16/10/30 09:48:35 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.596657 s
Result: 3.1484375 in 0.854 seconds
16/10/30 09:48:35 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:35 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:35 INFO BlockManager: BlockManager stopped
16/10/30 09:48:35 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:35 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:35 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-a97896f3-bca9-4588-a86f-0b474f73d484
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:37 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:37 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:37 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:37 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:37 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:37 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:37 INFO Utils: Successfully started service 'sparkDriver' on port 46154.
16/10/30 09:48:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:38 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5d1c6b1c-4d3c-4e1b-9624-8e9fd4ea0ff6
16/10/30 09:48:38 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:38 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:38 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46154/jars/jni-spark-0.1.jar with timestamp 1477817318362
16/10/30 09:48:38 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38768.
16/10/30 09:48:38 INFO NettyBlockTransferService: Server created on 192.168.0.17:38768
16/10/30 09:48:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38768)
16/10/30 09:48:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38768 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38768)
16/10/30 09:48:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38768)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:40 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817320096
16/10/30 09:48:40 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8ee1eb4a-d492-44ef-ad9b-866adfbda6ba/userFiles-55366de5-c462-4595-8223-3c2a6be4c9e5/SparkJNIPi.so
16/10/30 09:48:40 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:40 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:40 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:40 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:40 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38768 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817320096
16/10/30 09:48:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8ee1eb4a-d492-44ef-ad9b-866adfbda6ba/userFiles-55366de5-c462-4595-8223-3c2a6be4c9e5/SparkJNIPi.so
16/10/30 09:48:40 INFO Executor: Fetching spark://192.168.0.17:46154/jars/jni-spark-0.1.jar with timestamp 1477817318362
16/10/30 09:48:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46154 after 33 ms (0 ms spent in bootstraps)
16/10/30 09:48:40 INFO Utils: Fetching spark://192.168.0.17:46154/jars/jni-spark-0.1.jar to /tmp/spark-8ee1eb4a-d492-44ef-ad9b-866adfbda6ba/userFiles-55366de5-c462-4595-8223-3c2a6be4c9e5/fetchFileTemp7434303536835888552.tmp
16/10/30 09:48:40 INFO Executor: Adding file:/tmp/spark-8ee1eb4a-d492-44ef-ad9b-866adfbda6ba/userFiles-55366de5-c462-4595-8223-3c2a6be4c9e5/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:48:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 348 ms on localhost (1/1)
16/10/30 09:48:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:40 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.411 s
16/10/30 09:48:40 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.624179 s
Result: 3.1363525 in 0.887 seconds
16/10/30 09:48:41 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:41 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:41 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:41 INFO BlockManager: BlockManager stopped
16/10/30 09:48:41 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:41 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:41 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-8ee1eb4a-d492-44ef-ad9b-866adfbda6ba
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:42 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:42 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:42 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:42 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:42 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:42 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:43 INFO Utils: Successfully started service 'sparkDriver' on port 45883.
16/10/30 09:48:43 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:43 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3d7e6782-0ab7-4097-bb30-6cc5ee031204
16/10/30 09:48:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:43 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:43 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45883/jars/jni-spark-0.1.jar with timestamp 1477817323483
16/10/30 09:48:43 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38907.
16/10/30 09:48:43 INFO NettyBlockTransferService: Server created on 192.168.0.17:38907
16/10/30 09:48:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38907)
16/10/30 09:48:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38907 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38907)
16/10/30 09:48:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38907)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:45 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817325214
16/10/30 09:48:45 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c4dd10fa-015e-4c94-b275-01705441f472/userFiles-087b437e-d548-47e2-b7aa-fe106e58ac65/SparkJNIPi.so
16/10/30 09:48:45 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:45 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38907 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817325214
16/10/30 09:48:45 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c4dd10fa-015e-4c94-b275-01705441f472/userFiles-087b437e-d548-47e2-b7aa-fe106e58ac65/SparkJNIPi.so
16/10/30 09:48:45 INFO Executor: Fetching spark://192.168.0.17:45883/jars/jni-spark-0.1.jar with timestamp 1477817323483
16/10/30 09:48:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45883 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:48:45 INFO Utils: Fetching spark://192.168.0.17:45883/jars/jni-spark-0.1.jar to /tmp/spark-c4dd10fa-015e-4c94-b275-01705441f472/userFiles-087b437e-d548-47e2-b7aa-fe106e58ac65/fetchFileTemp1345340999918164390.tmp
16/10/30 09:48:45 INFO Executor: Adding file:/tmp/spark-c4dd10fa-015e-4c94-b275-01705441f472/userFiles-087b437e-d548-47e2-b7aa-fe106e58ac65/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:48:46 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.407 s
16/10/30 09:48:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 383 ms on localhost (1/1)
16/10/30 09:48:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:46 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.675351 s
Result: 3.1363525 in 0.959 seconds
16/10/30 09:48:46 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:46 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:46 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:46 INFO BlockManager: BlockManager stopped
16/10/30 09:48:46 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:46 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:46 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-c4dd10fa-015e-4c94-b275-01705441f472
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:47 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:47 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:47 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:47 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:47 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:47 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:48 INFO Utils: Successfully started service 'sparkDriver' on port 40463.
16/10/30 09:48:48 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:48 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c29ed51c-04fb-4eb8-a646-179987edb689
16/10/30 09:48:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:48 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:48 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40463/jars/jni-spark-0.1.jar with timestamp 1477817328523
16/10/30 09:48:48 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36813.
16/10/30 09:48:48 INFO NettyBlockTransferService: Server created on 192.168.0.17:36813
16/10/30 09:48:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36813)
16/10/30 09:48:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36813 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36813)
16/10/30 09:48:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36813)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:50 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817330235
16/10/30 09:48:50 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2108c7bf-e398-45bc-a345-0253b76c43be/userFiles-4da569e6-cbb4-46e6-b42d-cf61eb6dd78d/SparkJNIPi.so
16/10/30 09:48:50 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:50 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:50 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:50 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:50 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36813 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:50 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:50 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817330235
16/10/30 09:48:50 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2108c7bf-e398-45bc-a345-0253b76c43be/userFiles-4da569e6-cbb4-46e6-b42d-cf61eb6dd78d/SparkJNIPi.so
16/10/30 09:48:50 INFO Executor: Fetching spark://192.168.0.17:40463/jars/jni-spark-0.1.jar with timestamp 1477817328523
16/10/30 09:48:50 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40463 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:48:50 INFO Utils: Fetching spark://192.168.0.17:40463/jars/jni-spark-0.1.jar to /tmp/spark-2108c7bf-e398-45bc-a345-0253b76c43be/userFiles-4da569e6-cbb4-46e6-b42d-cf61eb6dd78d/fetchFileTemp1469173204595430373.tmp
16/10/30 09:48:50 INFO Executor: Adding file:/tmp/spark-2108c7bf-e398-45bc-a345-0253b76c43be/userFiles-4da569e6-cbb4-46e6-b42d-cf61eb6dd78d/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:48:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 363 ms on localhost (1/1)
16/10/30 09:48:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:51 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.389 s
16/10/30 09:48:51 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.637903 s
Result: 3.1495361 in 0.913 seconds
16/10/30 09:48:51 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:51 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:51 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:51 INFO BlockManager: BlockManager stopped
16/10/30 09:48:51 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:51 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:51 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-2108c7bf-e398-45bc-a345-0253b76c43be
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:52 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:52 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:52 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:52 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:52 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:52 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:53 INFO Utils: Successfully started service 'sparkDriver' on port 37985.
16/10/30 09:48:53 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:53 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0a0b99ee-d55d-4e30-acf2-587e0893aab3
16/10/30 09:48:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:53 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:53 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37985/jars/jni-spark-0.1.jar with timestamp 1477817333596
16/10/30 09:48:53 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34065.
16/10/30 09:48:53 INFO NettyBlockTransferService: Server created on 192.168.0.17:34065
16/10/30 09:48:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34065)
16/10/30 09:48:53 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34065 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34065)
16/10/30 09:48:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34065)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:48:55 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817335618
16/10/30 09:48:55 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-b0c458d6-d670-449c-8d3f-a5e35c5bbba6/userFiles-cc4813d8-ec9f-45ef-814c-779d077340b7/SparkJNIPi.so
16/10/30 09:48:55 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:48:55 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:48:55 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:48:55 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:48:55 INFO DAGScheduler: Missing parents: List()
16/10/30 09:48:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:48:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:48:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:48:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34065 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:48:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:48:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:48:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:48:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:48:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:48:56 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817335618
16/10/30 09:48:56 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-b0c458d6-d670-449c-8d3f-a5e35c5bbba6/userFiles-cc4813d8-ec9f-45ef-814c-779d077340b7/SparkJNIPi.so
16/10/30 09:48:56 INFO Executor: Fetching spark://192.168.0.17:37985/jars/jni-spark-0.1.jar with timestamp 1477817333596
16/10/30 09:48:56 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37985 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:48:56 INFO Utils: Fetching spark://192.168.0.17:37985/jars/jni-spark-0.1.jar to /tmp/spark-b0c458d6-d670-449c-8d3f-a5e35c5bbba6/userFiles-cc4813d8-ec9f-45ef-814c-779d077340b7/fetchFileTemp1885542709933018501.tmp
16/10/30 09:48:56 INFO Executor: Adding file:/tmp/spark-b0c458d6-d670-449c-8d3f-a5e35c5bbba6/userFiles-cc4813d8-ec9f-45ef-814c-779d077340b7/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:48:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:48:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 392 ms on localhost (1/1)
16/10/30 09:48:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:48:56 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.406 s
16/10/30 09:48:56 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.698704 s
Result: 3.149414 in 0.958 seconds
16/10/30 09:48:56 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:48:56 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:48:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:48:56 INFO MemoryStore: MemoryStore cleared
16/10/30 09:48:56 INFO BlockManager: BlockManager stopped
16/10/30 09:48:56 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:48:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:48:56 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:48:56 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:48:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0c458d6-d670-449c-8d3f-a5e35c5bbba6
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:48:57 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:48:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:48:58 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:48:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:48:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:48:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:48:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:48:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:48:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:48:58 INFO Utils: Successfully started service 'sparkDriver' on port 38682.
16/10/30 09:48:58 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:48:58 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:48:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d75415de-aadc-4ed4-8787-a30378a0e6ab
16/10/30 09:48:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:48:58 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:48:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:48:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:48:59 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38682/jars/jni-spark-0.1.jar with timestamp 1477817339081
16/10/30 09:48:59 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:48:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35337.
16/10/30 09:48:59 INFO NettyBlockTransferService: Server created on 192.168.0.17:35337
16/10/30 09:48:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35337)
16/10/30 09:48:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35337 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35337)
16/10/30 09:48:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35337)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:00 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817340887
16/10/30 09:49:00 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-bd218f25-91f3-4d0a-b2f6-7b121f6dfaa4/userFiles-c643ffab-7874-478c-8176-0882614061c0/SparkJNIPi.so
16/10/30 09:49:01 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:01 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:01 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:01 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:01 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35337 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:01 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817340887
16/10/30 09:49:01 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-bd218f25-91f3-4d0a-b2f6-7b121f6dfaa4/userFiles-c643ffab-7874-478c-8176-0882614061c0/SparkJNIPi.so
16/10/30 09:49:01 INFO Executor: Fetching spark://192.168.0.17:38682/jars/jni-spark-0.1.jar with timestamp 1477817339081
16/10/30 09:49:01 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38682 after 36 ms (0 ms spent in bootstraps)
16/10/30 09:49:01 INFO Utils: Fetching spark://192.168.0.17:38682/jars/jni-spark-0.1.jar to /tmp/spark-bd218f25-91f3-4d0a-b2f6-7b121f6dfaa4/userFiles-c643ffab-7874-478c-8176-0882614061c0/fetchFileTemp8636954856697356186.tmp
16/10/30 09:49:01 INFO Executor: Adding file:/tmp/spark-bd218f25-91f3-4d0a-b2f6-7b121f6dfaa4/userFiles-c643ffab-7874-478c-8176-0882614061c0/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:01 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:49:01 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:35337 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:49:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:49:01 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35337 after 7 ms (0 ms spent in bootstraps)
16/10/30 09:49:01 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:35337 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:49:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 494 ms on localhost (1/1)
16/10/30 09:49:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:02 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.639 s
16/10/30 09:49:02 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.942638 s
Result: 3.1383667 in 1.194 seconds
16/10/30 09:49:02 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:02 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:02 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:02 INFO BlockManager: BlockManager stopped
16/10/30 09:49:02 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:02 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:02 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-bd218f25-91f3-4d0a-b2f6-7b121f6dfaa4
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:03 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:03 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:03 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:03 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:03 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:03 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:04 INFO Utils: Successfully started service 'sparkDriver' on port 45882.
16/10/30 09:49:04 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:04 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07b8d105-4e75-4d05-9736-7cee8344cfce
16/10/30 09:49:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:04 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:04 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45882/jars/jni-spark-0.1.jar with timestamp 1477817344502
16/10/30 09:49:04 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44576.
16/10/30 09:49:04 INFO NettyBlockTransferService: Server created on 192.168.0.17:44576
16/10/30 09:49:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44576)
16/10/30 09:49:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44576 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44576)
16/10/30 09:49:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44576)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:06 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817346181
16/10/30 09:49:06 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-b8fc0b8e-e6e1-4eaa-9d21-2fcd5670c4cd/userFiles-b62d30f9-a1ec-4897-a958-2247080dbdd5/SparkJNIPi.so
16/10/30 09:49:06 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:06 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:06 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:06 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:06 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44576 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:06 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817346181
16/10/30 09:49:06 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-b8fc0b8e-e6e1-4eaa-9d21-2fcd5670c4cd/userFiles-b62d30f9-a1ec-4897-a958-2247080dbdd5/SparkJNIPi.so
16/10/30 09:49:06 INFO Executor: Fetching spark://192.168.0.17:45882/jars/jni-spark-0.1.jar with timestamp 1477817344502
16/10/30 09:49:06 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45882 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:49:06 INFO Utils: Fetching spark://192.168.0.17:45882/jars/jni-spark-0.1.jar to /tmp/spark-b8fc0b8e-e6e1-4eaa-9d21-2fcd5670c4cd/userFiles-b62d30f9-a1ec-4897-a958-2247080dbdd5/fetchFileTemp6734958662594304872.tmp
16/10/30 09:49:06 INFO Executor: Adding file:/tmp/spark-b8fc0b8e-e6e1-4eaa-9d21-2fcd5670c4cd/userFiles-b62d30f9-a1ec-4897-a958-2247080dbdd5/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:07 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:49:07 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:44576 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:49:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:49:07 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44576 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:07 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:44576 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:49:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 463 ms on localhost (1/1)
16/10/30 09:49:07 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.498 s
16/10/30 09:49:07 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.785914 s
16/10/30 09:49:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result: 3.1415787 in 1.052 seconds
16/10/30 09:49:07 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:07 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:07 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:07 INFO BlockManager: BlockManager stopped
16/10/30 09:49:07 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:07 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:07 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-b8fc0b8e-e6e1-4eaa-9d21-2fcd5670c4cd
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:08 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:08 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:09 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:09 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:09 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:09 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:09 INFO Utils: Successfully started service 'sparkDriver' on port 33541.
16/10/30 09:49:09 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:09 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cddcd3bb-4f0f-4f8d-9fee-9b79b6eb48da
16/10/30 09:49:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:09 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:09 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33541/jars/jni-spark-0.1.jar with timestamp 1477817349764
16/10/30 09:49:09 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33656.
16/10/30 09:49:09 INFO NettyBlockTransferService: Server created on 192.168.0.17:33656
16/10/30 09:49:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33656)
16/10/30 09:49:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33656 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33656)
16/10/30 09:49:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33656)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:11 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817351596
16/10/30 09:49:11 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-683484f3-bae0-4751-867d-80f58bb7a625/userFiles-537fde64-7289-4b74-9aaf-347731c40dc7/SparkJNIPi.so
16/10/30 09:49:11 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:11 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:11 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:11 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:11 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33656 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:12 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817351596
16/10/30 09:49:12 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-683484f3-bae0-4751-867d-80f58bb7a625/userFiles-537fde64-7289-4b74-9aaf-347731c40dc7/SparkJNIPi.so
16/10/30 09:49:12 INFO Executor: Fetching spark://192.168.0.17:33541/jars/jni-spark-0.1.jar with timestamp 1477817349764
16/10/30 09:49:12 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33541 after 27 ms (0 ms spent in bootstraps)
16/10/30 09:49:12 INFO Utils: Fetching spark://192.168.0.17:33541/jars/jni-spark-0.1.jar to /tmp/spark-683484f3-bae0-4751-867d-80f58bb7a625/userFiles-537fde64-7289-4b74-9aaf-347731c40dc7/fetchFileTemp6880363698122243267.tmp
16/10/30 09:49:12 INFO Executor: Adding file:/tmp/spark-683484f3-bae0-4751-867d-80f58bb7a625/userFiles-537fde64-7289-4b74-9aaf-347731c40dc7/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:12 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:49:12 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:33656 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:49:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:49:12 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33656 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:12 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:33656 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:49:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 502 ms on localhost (1/1)
16/10/30 09:49:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:12 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.533 s
16/10/30 09:49:12 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.925321 s
Result: 3.1417542 in 1.232 seconds
16/10/30 09:49:12 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:12 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:12 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:12 INFO BlockManager: BlockManager stopped
16/10/30 09:49:12 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:12 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:12 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-683484f3-bae0-4751-867d-80f58bb7a625
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:14 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:14 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:14 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:14 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:14 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:14 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:14 INFO Utils: Successfully started service 'sparkDriver' on port 43970.
16/10/30 09:49:14 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:14 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a54346b7-4f53-43d1-9755-f7c1765e1fe9
16/10/30 09:49:14 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:15 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:15 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43970/jars/jni-spark-0.1.jar with timestamp 1477817355254
16/10/30 09:49:15 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34211.
16/10/30 09:49:15 INFO NettyBlockTransferService: Server created on 192.168.0.17:34211
16/10/30 09:49:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34211)
16/10/30 09:49:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34211 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34211)
16/10/30 09:49:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34211)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:16 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817356958
16/10/30 09:49:16 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-ae66a836-072e-4e57-96da-a5d6b6c2a156/userFiles-7b63a884-7b5d-4bc1-a0be-b29b773931e2/SparkJNIPi.so
16/10/30 09:49:17 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:17 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:17 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:17 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:17 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34211 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:17 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817356958
16/10/30 09:49:17 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-ae66a836-072e-4e57-96da-a5d6b6c2a156/userFiles-7b63a884-7b5d-4bc1-a0be-b29b773931e2/SparkJNIPi.so
16/10/30 09:49:17 INFO Executor: Fetching spark://192.168.0.17:43970/jars/jni-spark-0.1.jar with timestamp 1477817355254
16/10/30 09:49:17 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43970 after 28 ms (0 ms spent in bootstraps)
16/10/30 09:49:17 INFO Utils: Fetching spark://192.168.0.17:43970/jars/jni-spark-0.1.jar to /tmp/spark-ae66a836-072e-4e57-96da-a5d6b6c2a156/userFiles-7b63a884-7b5d-4bc1-a0be-b29b773931e2/fetchFileTemp2522806876521253047.tmp
16/10/30 09:49:17 INFO Executor: Adding file:/tmp/spark-ae66a836-072e-4e57-96da-a5d6b6c2a156/userFiles-7b63a884-7b5d-4bc1-a0be-b29b773931e2/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:17 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:49:17 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:34211 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:49:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:49:17 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34211 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:17 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:34211 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:49:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 469 ms on localhost (1/1)
16/10/30 09:49:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:18 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.605 s
16/10/30 09:49:18 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.846430 s
Result: 3.1441498 in 1.129 seconds
16/10/30 09:49:18 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:18 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:18 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:18 INFO BlockManager: BlockManager stopped
16/10/30 09:49:18 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:18 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:18 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-ae66a836-072e-4e57-96da-a5d6b6c2a156
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:19 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:19 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:19 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:19 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:19 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:19 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:20 INFO Utils: Successfully started service 'sparkDriver' on port 46469.
16/10/30 09:49:20 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:20 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:20 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-95aedca3-33a2-4f9d-8ff5-2cfd121b51a3
16/10/30 09:49:20 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:20 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:20 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46469/jars/jni-spark-0.1.jar with timestamp 1477817360564
16/10/30 09:49:20 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35421.
16/10/30 09:49:20 INFO NettyBlockTransferService: Server created on 192.168.0.17:35421
16/10/30 09:49:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35421)
16/10/30 09:49:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35421 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35421)
16/10/30 09:49:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35421)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:22 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817362253
16/10/30 09:49:22 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-097a7693-0ff0-46d2-9f15-594da2500686/userFiles-fa734792-57e8-42e9-a1ed-01fa26180b0b/SparkJNIPi.so
16/10/30 09:49:22 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:22 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:22 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:22 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:22 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35421 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:22 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817362253
16/10/30 09:49:22 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-097a7693-0ff0-46d2-9f15-594da2500686/userFiles-fa734792-57e8-42e9-a1ed-01fa26180b0b/SparkJNIPi.so
16/10/30 09:49:22 INFO Executor: Fetching spark://192.168.0.17:46469/jars/jni-spark-0.1.jar with timestamp 1477817360564
16/10/30 09:49:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46469 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:49:22 INFO Utils: Fetching spark://192.168.0.17:46469/jars/jni-spark-0.1.jar to /tmp/spark-097a7693-0ff0-46d2-9f15-594da2500686/userFiles-fa734792-57e8-42e9-a1ed-01fa26180b0b/fetchFileTemp1369310711602340179.tmp
16/10/30 09:49:22 INFO Executor: Adding file:/tmp/spark-097a7693-0ff0-46d2-9f15-594da2500686/userFiles-fa734792-57e8-42e9-a1ed-01fa26180b0b/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:23 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:49:23 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:35421 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:49:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:49:23 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35421 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:23 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:35421 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:49:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 856 ms on localhost (1/1)
16/10/30 09:49:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:23 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.905 s
16/10/30 09:49:23 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.179324 s
Result: 3.140707 in 1.441 seconds
16/10/30 09:49:23 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:23 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:23 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:23 INFO BlockManager: BlockManager stopped
16/10/30 09:49:23 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:23 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:23 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-097a7693-0ff0-46d2-9f15-594da2500686
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:24 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:25 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:25 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:25 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:25 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:25 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:25 INFO Utils: Successfully started service 'sparkDriver' on port 43189.
16/10/30 09:49:25 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:25 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-30f4e733-c926-45fa-8add-681bacf4340d
16/10/30 09:49:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:25 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:26 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43189/jars/jni-spark-0.1.jar with timestamp 1477817366180
16/10/30 09:49:26 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39105.
16/10/30 09:49:26 INFO NettyBlockTransferService: Server created on 192.168.0.17:39105
16/10/30 09:49:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39105)
16/10/30 09:49:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39105 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39105)
16/10/30 09:49:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39105)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:27 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817367921
16/10/30 09:49:27 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-53240cb5-634d-413c-b736-550d58d7115e/userFiles-96dd3a70-ade5-479b-9ad4-1066646b1403/SparkJNIPi.so
16/10/30 09:49:28 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:28 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:28 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:28 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39105 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:28 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817367921
16/10/30 09:49:28 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-53240cb5-634d-413c-b736-550d58d7115e/userFiles-96dd3a70-ade5-479b-9ad4-1066646b1403/SparkJNIPi.so
16/10/30 09:49:28 INFO Executor: Fetching spark://192.168.0.17:43189/jars/jni-spark-0.1.jar with timestamp 1477817366180
16/10/30 09:49:28 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43189 after 37 ms (0 ms spent in bootstraps)
16/10/30 09:49:28 INFO Utils: Fetching spark://192.168.0.17:43189/jars/jni-spark-0.1.jar to /tmp/spark-53240cb5-634d-413c-b736-550d58d7115e/userFiles-96dd3a70-ade5-479b-9ad4-1066646b1403/fetchFileTemp5960583074521377158.tmp
16/10/30 09:49:28 INFO Executor: Adding file:/tmp/spark-53240cb5-634d-413c-b736-550d58d7115e/userFiles-96dd3a70-ade5-479b-9ad4-1066646b1403/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:29 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:49:29 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:39105 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:49:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:49:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39105 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:29 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:39105 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:49:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 961 ms on localhost (1/1)
16/10/30 09:49:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:29 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.023 s
16/10/30 09:49:29 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.286268 s
Result: 3.1421595 in 1.563 seconds
16/10/30 09:49:29 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:29 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:29 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:29 INFO BlockManager: BlockManager stopped
16/10/30 09:49:29 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:29 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:29 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-53240cb5-634d-413c-b736-550d58d7115e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:30 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:31 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:31 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:31 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:31 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:31 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:31 INFO Utils: Successfully started service 'sparkDriver' on port 37304.
16/10/30 09:49:31 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:31 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f2a9aaf8-d548-489b-b5e7-ccb235e29207
16/10/30 09:49:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:31 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:31 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37304/jars/jni-spark-0.1.jar with timestamp 1477817371950
16/10/30 09:49:32 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38551.
16/10/30 09:49:32 INFO NettyBlockTransferService: Server created on 192.168.0.17:38551
16/10/30 09:49:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38551)
16/10/30 09:49:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38551 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38551)
16/10/30 09:49:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38551)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:33 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817373726
16/10/30 09:49:33 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-4428a2f4-a898-4b3e-8c46-2de4fba2510b/userFiles-0be41f34-0367-4888-b948-a466c5d09f33/SparkJNIPi.so
16/10/30 09:49:34 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:34 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:34 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:34 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:34 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38551 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:34 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817373726
16/10/30 09:49:34 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-4428a2f4-a898-4b3e-8c46-2de4fba2510b/userFiles-0be41f34-0367-4888-b948-a466c5d09f33/SparkJNIPi.so
16/10/30 09:49:34 INFO Executor: Fetching spark://192.168.0.17:37304/jars/jni-spark-0.1.jar with timestamp 1477817371950
16/10/30 09:49:34 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37304 after 36 ms (0 ms spent in bootstraps)
16/10/30 09:49:34 INFO Utils: Fetching spark://192.168.0.17:37304/jars/jni-spark-0.1.jar to /tmp/spark-4428a2f4-a898-4b3e-8c46-2de4fba2510b/userFiles-0be41f34-0367-4888-b948-a466c5d09f33/fetchFileTemp3350084052467081426.tmp
16/10/30 09:49:34 INFO Executor: Adding file:/tmp/spark-4428a2f4-a898-4b3e-8c46-2de4fba2510b/userFiles-0be41f34-0367-4888-b948-a466c5d09f33/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:35 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:49:35 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:38551 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:49:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860225 bytes result sent via BlockManager)
16/10/30 09:49:35 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38551 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:35 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:38551 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:49:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 870 ms on localhost (1/1)
16/10/30 09:49:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:35 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.920 s
16/10/30 09:49:35 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.164484 s
Result: 3.1405754 in 1.47 seconds
16/10/30 09:49:35 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:35 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:35 INFO BlockManager: BlockManager stopped
16/10/30 09:49:35 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:35 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:35 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-4428a2f4-a898-4b3e-8c46-2de4fba2510b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:36 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:36 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:36 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:36 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:36 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:37 INFO Utils: Successfully started service 'sparkDriver' on port 35923.
16/10/30 09:49:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a39c1c55-c92e-435d-9f69-87876e4dc1c1
16/10/30 09:49:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35923/jars/jni-spark-0.1.jar with timestamp 1477817377565
16/10/30 09:49:37 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42235.
16/10/30 09:49:37 INFO NettyBlockTransferService: Server created on 192.168.0.17:42235
16/10/30 09:49:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42235)
16/10/30 09:49:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42235 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42235)
16/10/30 09:49:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42235)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817379299
16/10/30 09:49:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-59173322-7292-4c77-96f7-9c21e554d891/userFiles-de4701cf-f1a3-4c09-b562-aa264f68585c/SparkJNIPi.so
16/10/30 09:49:39 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:39 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:39 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:39 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:39 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42235 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:39 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817379299
16/10/30 09:49:39 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-59173322-7292-4c77-96f7-9c21e554d891/userFiles-de4701cf-f1a3-4c09-b562-aa264f68585c/SparkJNIPi.so
16/10/30 09:49:39 INFO Executor: Fetching spark://192.168.0.17:35923/jars/jni-spark-0.1.jar with timestamp 1477817377565
16/10/30 09:49:39 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35923 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:49:39 INFO Utils: Fetching spark://192.168.0.17:35923/jars/jni-spark-0.1.jar to /tmp/spark-59173322-7292-4c77-96f7-9c21e554d891/userFiles-de4701cf-f1a3-4c09-b562-aa264f68585c/fetchFileTemp3010433975373763204.tmp
16/10/30 09:49:40 INFO Executor: Adding file:/tmp/spark-59173322-7292-4c77-96f7-9c21e554d891/userFiles-de4701cf-f1a3-4c09-b562-aa264f68585c/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:40 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:49:40 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:42235 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:49:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:49:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42235 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:40 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:42235 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:49:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 877 ms on localhost (1/1)
16/10/30 09:49:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:40 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.927 s
16/10/30 09:49:40 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.204427 s
Result: 3.141446 in 1.491 seconds
16/10/30 09:49:40 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:40 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:40 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:40 INFO BlockManager: BlockManager stopped
16/10/30 09:49:40 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:40 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:40 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-59173322-7292-4c77-96f7-9c21e554d891
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:42 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:42 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:42 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:42 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:42 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:42 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:42 INFO Utils: Successfully started service 'sparkDriver' on port 35079.
16/10/30 09:49:42 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:42 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-15f75797-0abc-47dd-be06-1ee29aae7234
16/10/30 09:49:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:43 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:43 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35079/jars/jni-spark-0.1.jar with timestamp 1477817383247
16/10/30 09:49:43 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38636.
16/10/30 09:49:43 INFO NettyBlockTransferService: Server created on 192.168.0.17:38636
16/10/30 09:49:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38636)
16/10/30 09:49:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38636 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38636)
16/10/30 09:49:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38636)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:45 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817385004
16/10/30 09:49:45 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-163a1ac7-8f82-4985-bc03-e0a6584dff92/userFiles-7dfda98e-2840-4db7-b745-7ea3cb4898a8/SparkJNIPi.so
16/10/30 09:49:45 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:45 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38636 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817385004
16/10/30 09:49:45 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-163a1ac7-8f82-4985-bc03-e0a6584dff92/userFiles-7dfda98e-2840-4db7-b745-7ea3cb4898a8/SparkJNIPi.so
16/10/30 09:49:45 INFO Executor: Fetching spark://192.168.0.17:35079/jars/jni-spark-0.1.jar with timestamp 1477817383247
16/10/30 09:49:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35079 after 34 ms (0 ms spent in bootstraps)
16/10/30 09:49:45 INFO Utils: Fetching spark://192.168.0.17:35079/jars/jni-spark-0.1.jar to /tmp/spark-163a1ac7-8f82-4985-bc03-e0a6584dff92/userFiles-7dfda98e-2840-4db7-b745-7ea3cb4898a8/fetchFileTemp5280061475297135287.tmp
16/10/30 09:49:45 INFO Executor: Adding file:/tmp/spark-163a1ac7-8f82-4985-bc03-e0a6584dff92/userFiles-7dfda98e-2840-4db7-b745-7ea3cb4898a8/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:48 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:49:48 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:38636 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:49:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:49:48 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38636 after 2 ms (0 ms spent in bootstraps)
16/10/30 09:49:48 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:38636 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:49:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3393 ms on localhost (1/1)
16/10/30 09:49:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:49 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.544 s
16/10/30 09:49:49 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.802618 s
Result: 3.141458 in 4.11 seconds
16/10/30 09:49:49 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:49 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:49 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:49 INFO BlockManager: BlockManager stopped
16/10/30 09:49:49 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:49 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:49 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-163a1ac7-8f82-4985-bc03-e0a6584dff92
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:50 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:50 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:51 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:51 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:51 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:51 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:51 INFO Utils: Successfully started service 'sparkDriver' on port 35198.
16/10/30 09:49:51 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:51 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-58374283-ac40-4494-a5ce-5c426c261b2b
16/10/30 09:49:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:51 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:49:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:49:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:49:51 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35198/jars/jni-spark-0.1.jar with timestamp 1477817391757
16/10/30 09:49:51 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:49:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45423.
16/10/30 09:49:51 INFO NettyBlockTransferService: Server created on 192.168.0.17:45423
16/10/30 09:49:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45423)
16/10/30 09:49:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45423 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45423)
16/10/30 09:49:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45423)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:49:53 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817393546
16/10/30 09:49:53 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-824bcbf4-66ea-4dc9-9ae1-53a1f0b8c496/userFiles-9003aeae-563b-4572-baf7-dc9f3049f3d3/SparkJNIPi.so
16/10/30 09:49:53 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:49:53 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:49:53 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:49:53 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:49:53 INFO DAGScheduler: Missing parents: List()
16/10/30 09:49:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:49:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:49:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:49:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45423 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:49:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:49:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:49:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:49:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:49:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:49:54 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817393546
16/10/30 09:49:54 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-824bcbf4-66ea-4dc9-9ae1-53a1f0b8c496/userFiles-9003aeae-563b-4572-baf7-dc9f3049f3d3/SparkJNIPi.so
16/10/30 09:49:54 INFO Executor: Fetching spark://192.168.0.17:35198/jars/jni-spark-0.1.jar with timestamp 1477817391757
16/10/30 09:49:54 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35198 after 34 ms (0 ms spent in bootstraps)
16/10/30 09:49:54 INFO Utils: Fetching spark://192.168.0.17:35198/jars/jni-spark-0.1.jar to /tmp/spark-824bcbf4-66ea-4dc9-9ae1-53a1f0b8c496/userFiles-9003aeae-563b-4572-baf7-dc9f3049f3d3/fetchFileTemp7406097897053544050.tmp
16/10/30 09:49:54 INFO Executor: Adding file:/tmp/spark-824bcbf4-66ea-4dc9-9ae1-53a1f0b8c496/userFiles-9003aeae-563b-4572-baf7-dc9f3049f3d3/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:49:56 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:49:56 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:45423 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:49:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:49:56 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45423 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:49:57 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:45423 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:49:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3336 ms on localhost (1/1)
16/10/30 09:49:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:49:57 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.575 s
16/10/30 09:49:57 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.850758 s
16/10/30 09:49:57 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.17:45423 in memory (size: 1698.0 B, free: 366.3 MB)
Result: 3.1419535 in 4.261 seconds
16/10/30 09:49:57 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:49:57 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:49:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:49:57 INFO MemoryStore: MemoryStore cleared
16/10/30 09:49:57 INFO BlockManager: BlockManager stopped
16/10/30 09:49:57 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:49:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:49:57 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:49:57 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:49:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-824bcbf4-66ea-4dc9-9ae1-53a1f0b8c496
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:49:59 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:49:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:49:59 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:49:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:49:59 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:49:59 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:49:59 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:49:59 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:49:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:49:59 INFO Utils: Successfully started service 'sparkDriver' on port 45628.
16/10/30 09:49:59 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:49:59 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:49:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ff17f540-5eb1-4e12-bf33-dc7aa3057f7f
16/10/30 09:49:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:49:59 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:00 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45628/jars/jni-spark-0.1.jar with timestamp 1477817400178
16/10/30 09:50:00 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40404.
16/10/30 09:50:00 INFO NettyBlockTransferService: Server created on 192.168.0.17:40404
16/10/30 09:50:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40404)
16/10/30 09:50:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40404 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40404)
16/10/30 09:50:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40404)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:01 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817401792
16/10/30 09:50:01 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-9f872987-ca65-4b60-a67e-a7293b832ce2/userFiles-001a5780-3b30-4d87-8762-57514059c6e1/SparkJNIPi.so
16/10/30 09:50:02 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:02 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:02 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:02 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:02 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40404 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:50:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:02 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817401792
16/10/30 09:50:02 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-9f872987-ca65-4b60-a67e-a7293b832ce2/userFiles-001a5780-3b30-4d87-8762-57514059c6e1/SparkJNIPi.so
16/10/30 09:50:02 INFO Executor: Fetching spark://192.168.0.17:45628/jars/jni-spark-0.1.jar with timestamp 1477817400178
16/10/30 09:50:02 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45628 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:50:02 INFO Utils: Fetching spark://192.168.0.17:45628/jars/jni-spark-0.1.jar to /tmp/spark-9f872987-ca65-4b60-a67e-a7293b832ce2/userFiles-001a5780-3b30-4d87-8762-57514059c6e1/fetchFileTemp3617802465228474697.tmp
16/10/30 09:50:02 INFO Executor: Adding file:/tmp/spark-9f872987-ca65-4b60-a67e-a7293b832ce2/userFiles-001a5780-3b30-4d87-8762-57514059c6e1/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:50:05 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:50:05 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:40404 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:50:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:50:05 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40404 after 2 ms (0 ms spent in bootstraps)
16/10/30 09:50:05 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:40404 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:50:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3287 ms on localhost (1/1)
16/10/30 09:50:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:05 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.461 s
16/10/30 09:50:05 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.737692 s
Result: 3.1414258 in 4.012 seconds
16/10/30 09:50:05 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:05 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:05 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:05 INFO BlockManager: BlockManager stopped
16/10/30 09:50:05 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:05 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:05 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-9f872987-ca65-4b60-a67e-a7293b832ce2
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:07 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:07 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:07 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:07 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:07 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:07 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:08 INFO Utils: Successfully started service 'sparkDriver' on port 44175.
16/10/30 09:50:08 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:08 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5b264a3d-9b66-4860-b806-7a2aea8642fc
16/10/30 09:50:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:08 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:08 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44175/jars/jni-spark-0.1.jar with timestamp 1477817408467
16/10/30 09:50:08 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44606.
16/10/30 09:50:08 INFO NettyBlockTransferService: Server created on 192.168.0.17:44606
16/10/30 09:50:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44606)
16/10/30 09:50:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44606 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44606)
16/10/30 09:50:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44606)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:10 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817410322
16/10/30 09:50:10 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d70f2980-4735-4081-bedf-6140d1553102/userFiles-4ec9b74d-a520-47b9-af93-ec94172ae537/SparkJNIPi.so
16/10/30 09:50:10 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:10 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:10 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:10 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:10 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44606 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:50:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:10 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817410322
16/10/30 09:50:10 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d70f2980-4735-4081-bedf-6140d1553102/userFiles-4ec9b74d-a520-47b9-af93-ec94172ae537/SparkJNIPi.so
16/10/30 09:50:10 INFO Executor: Fetching spark://192.168.0.17:44175/jars/jni-spark-0.1.jar with timestamp 1477817408467
16/10/30 09:50:11 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44175 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:50:11 INFO Utils: Fetching spark://192.168.0.17:44175/jars/jni-spark-0.1.jar to /tmp/spark-d70f2980-4735-4081-bedf-6140d1553102/userFiles-4ec9b74d-a520-47b9-af93-ec94172ae537/fetchFileTemp4105259179376763007.tmp
16/10/30 09:50:11 INFO Executor: Adding file:/tmp/spark-d70f2980-4735-4081-bedf-6140d1553102/userFiles-4ec9b74d-a520-47b9-af93-ec94172ae537/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:50:13 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:50:13 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:44606 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:50:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:50:13 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44606 after 67 ms (0 ms spent in bootstraps)
16/10/30 09:50:14 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:44606 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:50:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3317 ms on localhost (1/1)
16/10/30 09:50:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:14 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.486 s
16/10/30 09:50:14 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.743574 s
Result: 3.1414957 in 4.066 seconds
16/10/30 09:50:14 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:14 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:14 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:14 INFO BlockManager: BlockManager stopped
16/10/30 09:50:14 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:14 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:14 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-d70f2980-4735-4081-bedf-6140d1553102
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:15 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:16 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:16 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:16 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:16 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:16 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:16 INFO Utils: Successfully started service 'sparkDriver' on port 38312.
16/10/30 09:50:16 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:16 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d8fe7c0-7d98-47d9-b273-caea2827168c
16/10/30 09:50:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:16 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:16 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38312/jars/jni-spark-0.1.jar with timestamp 1477817416797
16/10/30 09:50:16 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45142.
16/10/30 09:50:16 INFO NettyBlockTransferService: Server created on 192.168.0.17:45142
16/10/30 09:50:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45142)
16/10/30 09:50:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45142 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45142)
16/10/30 09:50:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45142)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:18 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817418440
16/10/30 09:50:18 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c4208e4d-baa8-4c68-af8c-37352a91f261/userFiles-abd19361-27b8-48f8-9306-e83a18bd84e9/SparkJNIPi.so
16/10/30 09:50:18 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:18 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:18 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:18 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:18 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45142 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:19 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817418440
16/10/30 09:50:19 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c4208e4d-baa8-4c68-af8c-37352a91f261/userFiles-abd19361-27b8-48f8-9306-e83a18bd84e9/SparkJNIPi.so
16/10/30 09:50:19 INFO Executor: Fetching spark://192.168.0.17:38312/jars/jni-spark-0.1.jar with timestamp 1477817416797
16/10/30 09:50:19 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38312 after 37 ms (0 ms spent in bootstraps)
16/10/30 09:50:19 INFO Utils: Fetching spark://192.168.0.17:38312/jars/jni-spark-0.1.jar to /tmp/spark-c4208e4d-baa8-4c68-af8c-37352a91f261/userFiles-abd19361-27b8-48f8-9306-e83a18bd84e9/fetchFileTemp5072218820602006616.tmp
16/10/30 09:50:19 INFO Executor: Adding file:/tmp/spark-c4208e4d-baa8-4c68-af8c-37352a91f261/userFiles-abd19361-27b8-48f8-9306-e83a18bd84e9/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:50:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 433 ms on localhost (1/1)
16/10/30 09:50:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:19 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.447 s
16/10/30 09:50:19 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.702966 s
Result: 3.1484375 in 1.006 seconds
16/10/30 09:50:19 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:19 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:19 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:19 INFO BlockManager: BlockManager stopped
16/10/30 09:50:19 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:19 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:19 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-c4208e4d-baa8-4c68-af8c-37352a91f261
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:20 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:21 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:21 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:21 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:21 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:21 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:21 INFO Utils: Successfully started service 'sparkDriver' on port 41408.
16/10/30 09:50:21 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:21 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6b3ac0fa-7796-4267-8a59-27d114191c09
16/10/30 09:50:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:21 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:22 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41408/jars/jni-spark-0.1.jar with timestamp 1477817422032
16/10/30 09:50:22 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35147.
16/10/30 09:50:22 INFO NettyBlockTransferService: Server created on 192.168.0.17:35147
16/10/30 09:50:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35147)
16/10/30 09:50:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35147 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35147)
16/10/30 09:50:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35147)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:23 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817423799
16/10/30 09:50:23 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-839d3192-758a-4bd9-880b-a1a6558b5ed5/userFiles-b3f29135-7e31-418e-8245-7aff8c704b35/SparkJNIPi.so
16/10/30 09:50:24 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:24 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:24 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:24 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:24 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35147 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:24 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817423799
16/10/30 09:50:24 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-839d3192-758a-4bd9-880b-a1a6558b5ed5/userFiles-b3f29135-7e31-418e-8245-7aff8c704b35/SparkJNIPi.so
16/10/30 09:50:24 INFO Executor: Fetching spark://192.168.0.17:41408/jars/jni-spark-0.1.jar with timestamp 1477817422032
16/10/30 09:50:24 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41408 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:50:24 INFO Utils: Fetching spark://192.168.0.17:41408/jars/jni-spark-0.1.jar to /tmp/spark-839d3192-758a-4bd9-880b-a1a6558b5ed5/userFiles-b3f29135-7e31-418e-8245-7aff8c704b35/fetchFileTemp7558355457658865142.tmp
16/10/30 09:50:24 INFO Executor: Adding file:/tmp/spark-839d3192-758a-4bd9-880b-a1a6558b5ed5/userFiles-b3f29135-7e31-418e-8245-7aff8c704b35/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:50:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 418 ms on localhost (1/1)
16/10/30 09:50:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:24 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.437 s
16/10/30 09:50:24 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.697469 s
Result: 3.1367188 in 1.0 seconds
16/10/30 09:50:24 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:24 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:24 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:24 INFO BlockManager: BlockManager stopped
16/10/30 09:50:24 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:24 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:24 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-839d3192-758a-4bd9-880b-a1a6558b5ed5
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:26 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:26 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:26 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:26 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:26 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:26 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:26 INFO Utils: Successfully started service 'sparkDriver' on port 46103.
16/10/30 09:50:26 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:26 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b0fa039c-8d0e-4719-bfd5-ad929d512cd0
16/10/30 09:50:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:27 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:27 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46103/jars/jni-spark-0.1.jar with timestamp 1477817427277
16/10/30 09:50:27 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40910.
16/10/30 09:50:27 INFO NettyBlockTransferService: Server created on 192.168.0.17:40910
16/10/30 09:50:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40910)
16/10/30 09:50:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40910 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40910)
16/10/30 09:50:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40910)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:29 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817429091
16/10/30 09:50:29 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-7cb9a361-6c87-4716-b007-d453fdc8fde0/userFiles-7f5526d4-f17d-4c3e-8302-d90dfbd31f3f/SparkJNIPi.so
16/10/30 09:50:29 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:29 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:29 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:29 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:29 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40910 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:29 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817429091
16/10/30 09:50:29 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-7cb9a361-6c87-4716-b007-d453fdc8fde0/userFiles-7f5526d4-f17d-4c3e-8302-d90dfbd31f3f/SparkJNIPi.so
16/10/30 09:50:29 INFO Executor: Fetching spark://192.168.0.17:46103/jars/jni-spark-0.1.jar with timestamp 1477817427277
16/10/30 09:50:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46103 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:50:29 INFO Utils: Fetching spark://192.168.0.17:46103/jars/jni-spark-0.1.jar to /tmp/spark-7cb9a361-6c87-4716-b007-d453fdc8fde0/userFiles-7f5526d4-f17d-4c3e-8302-d90dfbd31f3f/fetchFileTemp2072336177620474267.tmp
16/10/30 09:50:29 INFO Executor: Adding file:/tmp/spark-7cb9a361-6c87-4716-b007-d453fdc8fde0/userFiles-7f5526d4-f17d-4c3e-8302-d90dfbd31f3f/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:50:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 434 ms on localhost (1/1)
16/10/30 09:50:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:30 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.452 s
16/10/30 09:50:30 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.761886 s
Result: 3.1152344 in 1.029 seconds
16/10/30 09:50:30 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:30 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:30 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:30 INFO BlockManager: BlockManager stopped
16/10/30 09:50:30 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:30 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:30 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-7cb9a361-6c87-4716-b007-d453fdc8fde0
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:31 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:31 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:31 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:31 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:31 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:31 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:32 INFO Utils: Successfully started service 'sparkDriver' on port 44201.
16/10/30 09:50:32 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:32 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3ad765d4-b612-41c2-a660-ead449a19b97
16/10/30 09:50:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:32 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:32 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44201/jars/jni-spark-0.1.jar with timestamp 1477817432555
16/10/30 09:50:32 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44827.
16/10/30 09:50:32 INFO NettyBlockTransferService: Server created on 192.168.0.17:44827
16/10/30 09:50:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44827)
16/10/30 09:50:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44827 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44827)
16/10/30 09:50:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44827)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:34 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817434270
16/10/30 09:50:34 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-f7f88985-b8b3-4b1f-be22-a1bbd548ae31/userFiles-029b59de-2bc9-45ae-8e04-24a14fb230a6/SparkJNIPi.so
16/10/30 09:50:34 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:34 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:34 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:34 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:34 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44827 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:34 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817434270
16/10/30 09:50:34 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-f7f88985-b8b3-4b1f-be22-a1bbd548ae31/userFiles-029b59de-2bc9-45ae-8e04-24a14fb230a6/SparkJNIPi.so
16/10/30 09:50:34 INFO Executor: Fetching spark://192.168.0.17:44201/jars/jni-spark-0.1.jar with timestamp 1477817432555
16/10/30 09:50:34 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44201 after 37 ms (0 ms spent in bootstraps)
16/10/30 09:50:34 INFO Utils: Fetching spark://192.168.0.17:44201/jars/jni-spark-0.1.jar to /tmp/spark-f7f88985-b8b3-4b1f-be22-a1bbd548ae31/userFiles-029b59de-2bc9-45ae-8e04-24a14fb230a6/fetchFileTemp5515307785421031761.tmp
16/10/30 09:50:35 INFO Executor: Adding file:/tmp/spark-f7f88985-b8b3-4b1f-be22-a1bbd548ae31/userFiles-029b59de-2bc9-45ae-8e04-24a14fb230a6/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:50:35 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.429 s
16/10/30 09:50:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 414 ms on localhost (1/1)
16/10/30 09:50:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:35 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.683918 s
Result: 3.0761719 in 0.976 seconds
16/10/30 09:50:35 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:35 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:35 INFO BlockManager: BlockManager stopped
16/10/30 09:50:35 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:35 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:35 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-f7f88985-b8b3-4b1f-be22-a1bbd548ae31
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:36 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:36 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:36 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:36 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:36 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:37 INFO Utils: Successfully started service 'sparkDriver' on port 37805.
16/10/30 09:50:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c8011e8a-590c-41e0-b68f-32004a60788b
16/10/30 09:50:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37805/jars/jni-spark-0.1.jar with timestamp 1477817437655
16/10/30 09:50:37 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44022.
16/10/30 09:50:37 INFO NettyBlockTransferService: Server created on 192.168.0.17:44022
16/10/30 09:50:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44022)
16/10/30 09:50:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44022 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44022)
16/10/30 09:50:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44022)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817439424
16/10/30 09:50:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d5913fbf-2610-4952-b801-27f20ddd6eff/userFiles-da28bbb4-4bcf-4bf8-aeba-18c559d6456e/SparkJNIPi.so
16/10/30 09:50:39 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:39 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:39 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:39 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:39 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44022 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817439424
16/10/30 09:50:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d5913fbf-2610-4952-b801-27f20ddd6eff/userFiles-da28bbb4-4bcf-4bf8-aeba-18c559d6456e/SparkJNIPi.so
16/10/30 09:50:40 INFO Executor: Fetching spark://192.168.0.17:37805/jars/jni-spark-0.1.jar with timestamp 1477817437655
16/10/30 09:50:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37805 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:50:40 INFO Utils: Fetching spark://192.168.0.17:37805/jars/jni-spark-0.1.jar to /tmp/spark-d5913fbf-2610-4952-b801-27f20ddd6eff/userFiles-da28bbb4-4bcf-4bf8-aeba-18c559d6456e/fetchFileTemp7895080037783157671.tmp
16/10/30 09:50:40 INFO Executor: Adding file:/tmp/spark-d5913fbf-2610-4952-b801-27f20ddd6eff/userFiles-da28bbb4-4bcf-4bf8-aeba-18c559d6456e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:50:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 417 ms on localhost (1/1)
16/10/30 09:50:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:40 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.435 s
16/10/30 09:50:40 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.706612 s
Result: 3.1218262 in 0.997 seconds
16/10/30 09:50:40 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:40 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:40 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:40 INFO BlockManager: BlockManager stopped
16/10/30 09:50:40 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:40 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:40 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5913fbf-2610-4952-b801-27f20ddd6eff
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:41 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:42 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:42 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:42 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:42 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:42 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:42 INFO Utils: Successfully started service 'sparkDriver' on port 45134.
16/10/30 09:50:42 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:42 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bb22b1e7-a0ef-4099-9298-4e20951f5e41
16/10/30 09:50:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:42 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:42 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45134/jars/jni-spark-0.1.jar with timestamp 1477817442873
16/10/30 09:50:42 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37377.
16/10/30 09:50:42 INFO NettyBlockTransferService: Server created on 192.168.0.17:37377
16/10/30 09:50:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37377)
16/10/30 09:50:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37377 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37377)
16/10/30 09:50:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37377)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:44 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817444695
16/10/30 09:50:44 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c15d4466-94af-40bf-a08a-88a0e55cec7b/userFiles-d7cc6fec-d2bd-482c-95ca-aa34555b990d/SparkJNIPi.so
16/10/30 09:50:44 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:45 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37377 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817444695
16/10/30 09:50:45 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c15d4466-94af-40bf-a08a-88a0e55cec7b/userFiles-d7cc6fec-d2bd-482c-95ca-aa34555b990d/SparkJNIPi.so
16/10/30 09:50:45 INFO Executor: Fetching spark://192.168.0.17:45134/jars/jni-spark-0.1.jar with timestamp 1477817442873
16/10/30 09:50:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45134 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:50:45 INFO Utils: Fetching spark://192.168.0.17:45134/jars/jni-spark-0.1.jar to /tmp/spark-c15d4466-94af-40bf-a08a-88a0e55cec7b/userFiles-d7cc6fec-d2bd-482c-95ca-aa34555b990d/fetchFileTemp1134669230744973947.tmp
16/10/30 09:50:45 INFO Executor: Adding file:/tmp/spark-c15d4466-94af-40bf-a08a-88a0e55cec7b/userFiles-d7cc6fec-d2bd-482c-95ca-aa34555b990d/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:50:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 440 ms on localhost (1/1)
16/10/30 09:50:45 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:45 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.505 s
16/10/30 09:50:45 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.754115 s
Result: 3.147705 in 1.033 seconds
16/10/30 09:50:45 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:45 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:45 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:45 INFO BlockManager: BlockManager stopped
16/10/30 09:50:45 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:45 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:45 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-c15d4466-94af-40bf-a08a-88a0e55cec7b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:46 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:47 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:47 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:47 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:47 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:47 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:47 INFO Utils: Successfully started service 'sparkDriver' on port 35627.
16/10/30 09:50:47 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:47 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-33063622-d1e1-4dc4-903b-0a646b0df0a2
16/10/30 09:50:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:47 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:48 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35627/jars/jni-spark-0.1.jar with timestamp 1477817448119
16/10/30 09:50:48 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43039.
16/10/30 09:50:48 INFO NettyBlockTransferService: Server created on 192.168.0.17:43039
16/10/30 09:50:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43039)
16/10/30 09:50:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43039 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43039)
16/10/30 09:50:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43039)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:49 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817449802
16/10/30 09:50:49 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c0d1f556-7779-45e0-9652-a0f0ed8e89aa/userFiles-05cb7d6e-4454-4d93-bf54-6e7f3ebb166d/SparkJNIPi.so
16/10/30 09:50:50 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:50 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:50 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:50 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:50 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43039 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:50 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:50 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817449802
16/10/30 09:50:50 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c0d1f556-7779-45e0-9652-a0f0ed8e89aa/userFiles-05cb7d6e-4454-4d93-bf54-6e7f3ebb166d/SparkJNIPi.so
16/10/30 09:50:50 INFO Executor: Fetching spark://192.168.0.17:35627/jars/jni-spark-0.1.jar with timestamp 1477817448119
16/10/30 09:50:50 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35627 after 30 ms (0 ms spent in bootstraps)
16/10/30 09:50:50 INFO Utils: Fetching spark://192.168.0.17:35627/jars/jni-spark-0.1.jar to /tmp/spark-c0d1f556-7779-45e0-9652-a0f0ed8e89aa/userFiles-05cb7d6e-4454-4d93-bf54-6e7f3ebb166d/fetchFileTemp4653460608885658445.tmp
16/10/30 09:50:50 INFO Executor: Adding file:/tmp/spark-c0d1f556-7779-45e0-9652-a0f0ed8e89aa/userFiles-05cb7d6e-4454-4d93-bf54-6e7f3ebb166d/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:50:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 395 ms on localhost (1/1)
16/10/30 09:50:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:50 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.413 s
16/10/30 09:50:50 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.674890 s
Result: 3.1325684 in 0.957 seconds
16/10/30 09:50:50 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:50 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:50 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:50 INFO BlockManager: BlockManager stopped
16/10/30 09:50:50 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:50 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:50 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-c0d1f556-7779-45e0-9652-a0f0ed8e89aa
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:52 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:52 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:52 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:52 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:52 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:52 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:52 INFO Utils: Successfully started service 'sparkDriver' on port 45948.
16/10/30 09:50:52 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:52 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6dda68e4-b9d9-4326-8d2c-6117d93a53eb
16/10/30 09:50:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:53 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:53 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45948/jars/jni-spark-0.1.jar with timestamp 1477817453221
16/10/30 09:50:53 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40084.
16/10/30 09:50:53 INFO NettyBlockTransferService: Server created on 192.168.0.17:40084
16/10/30 09:50:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40084)
16/10/30 09:50:53 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40084 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40084)
16/10/30 09:50:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40084)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:50:54 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817454935
16/10/30 09:50:54 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6d0a8908-4ced-417b-96ea-5b438c595ac0/userFiles-58dff269-c4da-4f3a-8300-ebcf2bc3da3e/SparkJNIPi.so
16/10/30 09:50:55 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:50:55 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:50:55 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:50:55 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:50:55 INFO DAGScheduler: Missing parents: List()
16/10/30 09:50:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:50:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:50:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:50:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40084 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:50:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:50:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:50:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:50:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:50:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:50:55 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817454935
16/10/30 09:50:55 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6d0a8908-4ced-417b-96ea-5b438c595ac0/userFiles-58dff269-c4da-4f3a-8300-ebcf2bc3da3e/SparkJNIPi.so
16/10/30 09:50:55 INFO Executor: Fetching spark://192.168.0.17:45948/jars/jni-spark-0.1.jar with timestamp 1477817453221
16/10/30 09:50:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45948 after 22 ms (0 ms spent in bootstraps)
16/10/30 09:50:55 INFO Utils: Fetching spark://192.168.0.17:45948/jars/jni-spark-0.1.jar to /tmp/spark-6d0a8908-4ced-417b-96ea-5b438c595ac0/userFiles-58dff269-c4da-4f3a-8300-ebcf2bc3da3e/fetchFileTemp7775168628936433150.tmp
16/10/30 09:50:55 INFO Executor: Adding file:/tmp/spark-6d0a8908-4ced-417b-96ea-5b438c595ac0/userFiles-58dff269-c4da-4f3a-8300-ebcf2bc3da3e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:50:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:50:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 417 ms on localhost (1/1)
16/10/30 09:50:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:50:55 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.432 s
16/10/30 09:50:55 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.684796 s
Result: 3.1428223 in 0.993 seconds
16/10/30 09:50:55 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:50:55 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:50:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:50:55 INFO MemoryStore: MemoryStore cleared
16/10/30 09:50:55 INFO BlockManager: BlockManager stopped
16/10/30 09:50:55 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:50:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:50:55 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:50:55 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:50:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d0a8908-4ced-417b-96ea-5b438c595ac0
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:50:57 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:50:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:50:57 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:50:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:50:57 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:50:57 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:50:57 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:50:57 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:50:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:50:58 INFO Utils: Successfully started service 'sparkDriver' on port 44926.
16/10/30 09:50:58 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:50:58 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:50:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-859ea1d9-578f-4dc3-88bd-0d2a7439565d
16/10/30 09:50:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:50:58 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:50:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:50:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:50:58 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44926/jars/jni-spark-0.1.jar with timestamp 1477817458416
16/10/30 09:50:58 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:50:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34663.
16/10/30 09:50:58 INFO NettyBlockTransferService: Server created on 192.168.0.17:34663
16/10/30 09:50:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34663)
16/10/30 09:50:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34663 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34663)
16/10/30 09:50:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34663)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:00 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817460182
16/10/30 09:51:00 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-3185f68a-82b6-45f2-89c1-8732d4d22730/userFiles-9406099f-1378-463e-8772-a2e7e477c9b5/SparkJNIPi.so
16/10/30 09:51:00 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:00 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:00 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:00 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:00 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34663 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:00 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817460182
16/10/30 09:51:00 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-3185f68a-82b6-45f2-89c1-8732d4d22730/userFiles-9406099f-1378-463e-8772-a2e7e477c9b5/SparkJNIPi.so
16/10/30 09:51:00 INFO Executor: Fetching spark://192.168.0.17:44926/jars/jni-spark-0.1.jar with timestamp 1477817458416
16/10/30 09:51:00 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44926 after 25 ms (0 ms spent in bootstraps)
16/10/30 09:51:00 INFO Utils: Fetching spark://192.168.0.17:44926/jars/jni-spark-0.1.jar to /tmp/spark-3185f68a-82b6-45f2-89c1-8732d4d22730/userFiles-9406099f-1378-463e-8772-a2e7e477c9b5/fetchFileTemp2045793132227673241.tmp
16/10/30 09:51:00 INFO Executor: Adding file:/tmp/spark-3185f68a-82b6-45f2-89c1-8732d4d22730/userFiles-9406099f-1378-463e-8772-a2e7e477c9b5/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:51:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 428 ms on localhost (1/1)
16/10/30 09:51:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:01 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.445 s
16/10/30 09:51:01 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.748835 s
Result: 3.1454773 in 1.036 seconds
16/10/30 09:51:01 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:01 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:01 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:01 INFO BlockManager: BlockManager stopped
16/10/30 09:51:01 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:01 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:01 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-3185f68a-82b6-45f2-89c1-8732d4d22730
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:02 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:02 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:03 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:03 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:03 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:03 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:03 INFO Utils: Successfully started service 'sparkDriver' on port 42511.
16/10/30 09:51:03 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:03 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eda91879-3423-486a-9efa-db3ebff87807
16/10/30 09:51:03 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:03 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:03 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42511/jars/jni-spark-0.1.jar with timestamp 1477817463688
16/10/30 09:51:03 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35811.
16/10/30 09:51:03 INFO NettyBlockTransferService: Server created on 192.168.0.17:35811
16/10/30 09:51:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35811)
16/10/30 09:51:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35811 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35811)
16/10/30 09:51:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35811)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:05 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817465322
16/10/30 09:51:05 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c6dae372-fb53-4e78-bf3c-dc7e18d2ee3b/userFiles-919cf021-db13-4ec2-848d-f27c93f79c02/SparkJNIPi.so
16/10/30 09:51:05 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:05 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:05 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:05 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:05 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35811 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:05 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817465322
16/10/30 09:51:05 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c6dae372-fb53-4e78-bf3c-dc7e18d2ee3b/userFiles-919cf021-db13-4ec2-848d-f27c93f79c02/SparkJNIPi.so
16/10/30 09:51:05 INFO Executor: Fetching spark://192.168.0.17:42511/jars/jni-spark-0.1.jar with timestamp 1477817463688
16/10/30 09:51:05 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42511 after 30 ms (0 ms spent in bootstraps)
16/10/30 09:51:06 INFO Utils: Fetching spark://192.168.0.17:42511/jars/jni-spark-0.1.jar to /tmp/spark-c6dae372-fb53-4e78-bf3c-dc7e18d2ee3b/userFiles-919cf021-db13-4ec2-848d-f27c93f79c02/fetchFileTemp3858089207442870824.tmp
16/10/30 09:51:06 INFO Executor: Adding file:/tmp/spark-c6dae372-fb53-4e78-bf3c-dc7e18d2ee3b/userFiles-919cf021-db13-4ec2-848d-f27c93f79c02/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:51:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 416 ms on localhost (1/1)
16/10/30 09:51:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:06 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.434 s
16/10/30 09:51:06 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.718415 s
Result: 3.1427307 in 0.967 seconds
16/10/30 09:51:06 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:06 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:06 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:06 INFO BlockManager: BlockManager stopped
16/10/30 09:51:06 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:06 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:06 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-c6dae372-fb53-4e78-bf3c-dc7e18d2ee3b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:07 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:08 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:08 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:08 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:08 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:08 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:08 INFO Utils: Successfully started service 'sparkDriver' on port 33641.
16/10/30 09:51:08 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:08 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-886b392f-b998-4021-9ca3-9efdc65f3613
16/10/30 09:51:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:08 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:08 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33641/jars/jni-spark-0.1.jar with timestamp 1477817468758
16/10/30 09:51:08 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43259.
16/10/30 09:51:08 INFO NettyBlockTransferService: Server created on 192.168.0.17:43259
16/10/30 09:51:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43259)
16/10/30 09:51:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43259 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43259)
16/10/30 09:51:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43259)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:10 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817470520
16/10/30 09:51:10 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-576e2335-6e8f-47c4-9346-c464e32e45dc/userFiles-ecf5b013-bd1b-441a-8736-a9f31f3a55b1/SparkJNIPi.so
16/10/30 09:51:10 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:10 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:10 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:10 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:10 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43259 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:11 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817470520
16/10/30 09:51:11 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-576e2335-6e8f-47c4-9346-c464e32e45dc/userFiles-ecf5b013-bd1b-441a-8736-a9f31f3a55b1/SparkJNIPi.so
16/10/30 09:51:11 INFO Executor: Fetching spark://192.168.0.17:33641/jars/jni-spark-0.1.jar with timestamp 1477817468758
16/10/30 09:51:11 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33641 after 43 ms (0 ms spent in bootstraps)
16/10/30 09:51:11 INFO Utils: Fetching spark://192.168.0.17:33641/jars/jni-spark-0.1.jar to /tmp/spark-576e2335-6e8f-47c4-9346-c464e32e45dc/userFiles-ecf5b013-bd1b-441a-8736-a9f31f3a55b1/fetchFileTemp8604931407188330345.tmp
16/10/30 09:51:11 INFO Executor: Adding file:/tmp/spark-576e2335-6e8f-47c4-9346-c464e32e45dc/userFiles-ecf5b013-bd1b-441a-8736-a9f31f3a55b1/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:51:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 468 ms on localhost (1/1)
16/10/30 09:51:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:11 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.482 s
16/10/30 09:51:11 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.769246 s
Result: 3.1390991 in 1.012 seconds
16/10/30 09:51:11 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:11 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:11 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:11 INFO BlockManager: BlockManager stopped
16/10/30 09:51:11 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:11 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:11 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-576e2335-6e8f-47c4-9346-c464e32e45dc
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:12 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:13 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:13 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:13 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:13 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:13 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:13 INFO Utils: Successfully started service 'sparkDriver' on port 35888.
16/10/30 09:51:13 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:13 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-11f46a15-7dad-48e5-9d20-df8f41c7a0d8
16/10/30 09:51:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:13 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:14 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35888/jars/jni-spark-0.1.jar with timestamp 1477817474039
16/10/30 09:51:14 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44158.
16/10/30 09:51:14 INFO NettyBlockTransferService: Server created on 192.168.0.17:44158
16/10/30 09:51:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44158)
16/10/30 09:51:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44158 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44158)
16/10/30 09:51:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44158)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:15 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817475820
16/10/30 09:51:15 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a4377dc0-cdfd-484d-a745-10ffc17d0096/userFiles-b71224a8-6384-41e8-99a0-415407857624/SparkJNIPi.so
16/10/30 09:51:16 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:16 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:16 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:16 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:16 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44158 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:16 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817475820
16/10/30 09:51:16 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a4377dc0-cdfd-484d-a745-10ffc17d0096/userFiles-b71224a8-6384-41e8-99a0-415407857624/SparkJNIPi.so
16/10/30 09:51:16 INFO Executor: Fetching spark://192.168.0.17:35888/jars/jni-spark-0.1.jar with timestamp 1477817474039
16/10/30 09:51:16 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35888 after 39 ms (0 ms spent in bootstraps)
16/10/30 09:51:16 INFO Utils: Fetching spark://192.168.0.17:35888/jars/jni-spark-0.1.jar to /tmp/spark-a4377dc0-cdfd-484d-a745-10ffc17d0096/userFiles-b71224a8-6384-41e8-99a0-415407857624/fetchFileTemp3798580015644103315.tmp
16/10/30 09:51:16 INFO Executor: Adding file:/tmp/spark-a4377dc0-cdfd-484d-a745-10ffc17d0096/userFiles-b71224a8-6384-41e8-99a0-415407857624/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:51:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 448 ms on localhost (1/1)
16/10/30 09:51:16 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.503 s
16/10/30 09:51:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:16 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.770898 s
Result: 3.1401062 in 1.036 seconds
16/10/30 09:51:16 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:16 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:16 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:16 INFO BlockManager: BlockManager stopped
16/10/30 09:51:16 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:16 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:16 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-a4377dc0-cdfd-484d-a745-10ffc17d0096
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:18 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:18 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:18 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:18 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:18 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:18 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:18 INFO Utils: Successfully started service 'sparkDriver' on port 34330.
16/10/30 09:51:19 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:19 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-adcdec71-c77b-4ac4-b2f2-562550e91d24
16/10/30 09:51:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:19 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:19 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34330/jars/jni-spark-0.1.jar with timestamp 1477817479400
16/10/30 09:51:19 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41241.
16/10/30 09:51:19 INFO NettyBlockTransferService: Server created on 192.168.0.17:41241
16/10/30 09:51:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41241)
16/10/30 09:51:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41241 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41241)
16/10/30 09:51:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41241)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:20 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817480945
16/10/30 09:51:20 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-befbe8e9-3149-43b3-aa9a-f9c54d72b282/userFiles-9e8a4217-4506-4f7a-9b11-a16c4ce1e174/SparkJNIPi.so
16/10/30 09:51:21 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:21 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:21 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:21 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:21 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41241 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:21 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817480945
16/10/30 09:51:21 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-befbe8e9-3149-43b3-aa9a-f9c54d72b282/userFiles-9e8a4217-4506-4f7a-9b11-a16c4ce1e174/SparkJNIPi.so
16/10/30 09:51:21 INFO Executor: Fetching spark://192.168.0.17:34330/jars/jni-spark-0.1.jar with timestamp 1477817479400
16/10/30 09:51:21 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34330 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:51:21 INFO Utils: Fetching spark://192.168.0.17:34330/jars/jni-spark-0.1.jar to /tmp/spark-befbe8e9-3149-43b3-aa9a-f9c54d72b282/userFiles-9e8a4217-4506-4f7a-9b11-a16c4ce1e174/fetchFileTemp5063952038440875362.tmp
16/10/30 09:51:21 INFO Executor: Adding file:/tmp/spark-befbe8e9-3149-43b3-aa9a-f9c54d72b282/userFiles-9e8a4217-4506-4f7a-9b11-a16c4ce1e174/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:22 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:51:22 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:41241 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:51:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:51:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41241 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:51:22 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:41241 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:51:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 669 ms on localhost (1/1)
16/10/30 09:51:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:22 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.698 s
16/10/30 09:51:22 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.974264 s
Result: 3.1425476 in 1.259 seconds
16/10/30 09:51:22 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:22 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:22 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:22 INFO BlockManager: BlockManager stopped
16/10/30 09:51:22 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:22 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:22 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-befbe8e9-3149-43b3-aa9a-f9c54d72b282
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:23 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:23 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:23 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:23 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:23 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:23 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:24 INFO Utils: Successfully started service 'sparkDriver' on port 37357.
16/10/30 09:51:24 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:24 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f549062b-7df2-433d-a0c9-52e8b82a05b7
16/10/30 09:51:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:24 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:24 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37357/jars/jni-spark-0.1.jar with timestamp 1477817484720
16/10/30 09:51:24 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44394.
16/10/30 09:51:24 INFO NettyBlockTransferService: Server created on 192.168.0.17:44394
16/10/30 09:51:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44394)
16/10/30 09:51:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44394 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44394)
16/10/30 09:51:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44394)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:26 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817486518
16/10/30 09:51:26 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-e908673a-1dc3-4489-8a97-0e72ed189665/userFiles-692eb822-8de6-4131-8f92-e1a2a8b42c2d/SparkJNIPi.so
16/10/30 09:51:26 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:26 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:26 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:26 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:26 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44394 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:27 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817486518
16/10/30 09:51:27 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-e908673a-1dc3-4489-8a97-0e72ed189665/userFiles-692eb822-8de6-4131-8f92-e1a2a8b42c2d/SparkJNIPi.so
16/10/30 09:51:27 INFO Executor: Fetching spark://192.168.0.17:37357/jars/jni-spark-0.1.jar with timestamp 1477817484720
16/10/30 09:51:27 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37357 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:51:27 INFO Utils: Fetching spark://192.168.0.17:37357/jars/jni-spark-0.1.jar to /tmp/spark-e908673a-1dc3-4489-8a97-0e72ed189665/userFiles-692eb822-8de6-4131-8f92-e1a2a8b42c2d/fetchFileTemp4176588790222539987.tmp
16/10/30 09:51:27 INFO Executor: Adding file:/tmp/spark-e908673a-1dc3-4489-8a97-0e72ed189665/userFiles-692eb822-8de6-4131-8f92-e1a2a8b42c2d/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:27 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:51:27 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:44394 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:51:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:51:27 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44394 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:51:27 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:44394 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:51:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 604 ms on localhost (1/1)
16/10/30 09:51:27 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.634 s
16/10/30 09:51:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:27 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.948044 s
Result: 3.1443062 in 1.233 seconds
16/10/30 09:51:27 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:27 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:27 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:27 INFO BlockManager: BlockManager stopped
16/10/30 09:51:27 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:27 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:27 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-e908673a-1dc3-4489-8a97-0e72ed189665
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:28 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:29 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:29 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:29 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:29 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:29 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:29 INFO Utils: Successfully started service 'sparkDriver' on port 42627.
16/10/30 09:51:29 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:29 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-910b9f1a-ba1f-4c13-bfca-04d85a16c38b
16/10/30 09:51:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:29 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:30 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42627/jars/jni-spark-0.1.jar with timestamp 1477817490139
16/10/30 09:51:30 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33058.
16/10/30 09:51:30 INFO NettyBlockTransferService: Server created on 192.168.0.17:33058
16/10/30 09:51:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33058)
16/10/30 09:51:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33058 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33058)
16/10/30 09:51:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33058)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:31 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817491844
16/10/30 09:51:31 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-72a5217b-5ac6-4dda-b1be-3d335bc30e43/userFiles-bd551b23-d71c-4d7a-b2f8-144ed8260f9a/SparkJNIPi.so
16/10/30 09:51:32 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:32 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:32 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:32 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:32 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33058 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:32 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817491844
16/10/30 09:51:32 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-72a5217b-5ac6-4dda-b1be-3d335bc30e43/userFiles-bd551b23-d71c-4d7a-b2f8-144ed8260f9a/SparkJNIPi.so
16/10/30 09:51:32 INFO Executor: Fetching spark://192.168.0.17:42627/jars/jni-spark-0.1.jar with timestamp 1477817490139
16/10/30 09:51:32 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42627 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:51:32 INFO Utils: Fetching spark://192.168.0.17:42627/jars/jni-spark-0.1.jar to /tmp/spark-72a5217b-5ac6-4dda-b1be-3d335bc30e43/userFiles-bd551b23-d71c-4d7a-b2f8-144ed8260f9a/fetchFileTemp3303669322068450364.tmp
16/10/30 09:51:32 INFO Executor: Adding file:/tmp/spark-72a5217b-5ac6-4dda-b1be-3d335bc30e43/userFiles-bd551b23-d71c-4d7a-b2f8-144ed8260f9a/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:32 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:51:32 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:33058 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:51:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:51:32 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33058 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:51:32 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:33058 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:51:32 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.561 s
16/10/30 09:51:32 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.842508 s
16/10/30 09:51:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 532 ms on localhost (1/1)
16/10/30 09:51:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result: 3.1419296 in 1.111 seconds
16/10/30 09:51:32 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:32 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:32 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:32 INFO BlockManager: BlockManager stopped
16/10/30 09:51:32 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:32 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:32 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-72a5217b-5ac6-4dda-b1be-3d335bc30e43
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:34 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:34 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:34 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:34 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:34 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:34 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:35 INFO Utils: Successfully started service 'sparkDriver' on port 43465.
16/10/30 09:51:35 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:35 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-25710022-b7dc-4be3-b454-1b809c9915ed
16/10/30 09:51:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:35 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:35 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43465/jars/jni-spark-0.1.jar with timestamp 1477817495384
16/10/30 09:51:35 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37434.
16/10/30 09:51:35 INFO NettyBlockTransferService: Server created on 192.168.0.17:37434
16/10/30 09:51:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37434)
16/10/30 09:51:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37434 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37434)
16/10/30 09:51:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37434)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:37 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817497119
16/10/30 09:51:37 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-85058426-08da-4faf-b939-a1b3cd74945a/userFiles-888c8dbf-9d41-4460-badb-cac6d244a0d7/SparkJNIPi.so
16/10/30 09:51:37 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:37 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:37 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:37 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:37 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37434 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:37 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817497119
16/10/30 09:51:37 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-85058426-08da-4faf-b939-a1b3cd74945a/userFiles-888c8dbf-9d41-4460-badb-cac6d244a0d7/SparkJNIPi.so
16/10/30 09:51:37 INFO Executor: Fetching spark://192.168.0.17:43465/jars/jni-spark-0.1.jar with timestamp 1477817495384
16/10/30 09:51:37 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43465 after 37 ms (0 ms spent in bootstraps)
16/10/30 09:51:37 INFO Utils: Fetching spark://192.168.0.17:43465/jars/jni-spark-0.1.jar to /tmp/spark-85058426-08da-4faf-b939-a1b3cd74945a/userFiles-888c8dbf-9d41-4460-badb-cac6d244a0d7/fetchFileTemp8788651077709282290.tmp
16/10/30 09:51:37 INFO Executor: Adding file:/tmp/spark-85058426-08da-4faf-b939-a1b3cd74945a/userFiles-888c8dbf-9d41-4460-badb-cac6d244a0d7/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:38 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:51:38 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:37434 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:51:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:51:38 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37434 after 6 ms (0 ms spent in bootstraps)
16/10/30 09:51:38 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:37434 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:51:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 647 ms on localhost (1/1)
16/10/30 09:51:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:38 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.678 s
16/10/30 09:51:38 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.953871 s
Result: 3.1411133 in 1.211 seconds
16/10/30 09:51:38 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:38 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:38 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:38 INFO BlockManager: BlockManager stopped
16/10/30 09:51:38 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:38 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:38 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-85058426-08da-4faf-b939-a1b3cd74945a
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:39 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:40 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:40 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:40 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:40 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:40 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:40 INFO Utils: Successfully started service 'sparkDriver' on port 36066.
16/10/30 09:51:40 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:40 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-df9f6578-a667-48be-993c-c47263edad31
16/10/30 09:51:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:40 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:40 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36066/jars/jni-spark-0.1.jar with timestamp 1477817500724
16/10/30 09:51:40 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34165.
16/10/30 09:51:40 INFO NettyBlockTransferService: Server created on 192.168.0.17:34165
16/10/30 09:51:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34165)
16/10/30 09:51:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34165 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34165)
16/10/30 09:51:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34165)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:42 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817502503
16/10/30 09:51:42 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-1dd55518-3d17-4af5-bec6-a22b7bc8ad5b/userFiles-8e846374-580b-460b-976d-13ab72345543/SparkJNIPi.so
16/10/30 09:51:42 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:42 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:42 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:42 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:42 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34165 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:43 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817502503
16/10/30 09:51:43 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-1dd55518-3d17-4af5-bec6-a22b7bc8ad5b/userFiles-8e846374-580b-460b-976d-13ab72345543/SparkJNIPi.so
16/10/30 09:51:43 INFO Executor: Fetching spark://192.168.0.17:36066/jars/jni-spark-0.1.jar with timestamp 1477817500724
16/10/30 09:51:43 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36066 after 28 ms (0 ms spent in bootstraps)
16/10/30 09:51:43 INFO Utils: Fetching spark://192.168.0.17:36066/jars/jni-spark-0.1.jar to /tmp/spark-1dd55518-3d17-4af5-bec6-a22b7bc8ad5b/userFiles-8e846374-580b-460b-976d-13ab72345543/fetchFileTemp4185901398924766990.tmp
16/10/30 09:51:43 INFO Executor: Adding file:/tmp/spark-1dd55518-3d17-4af5-bec6-a22b7bc8ad5b/userFiles-8e846374-580b-460b-976d-13ab72345543/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:44 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:51:44 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:34165 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:51:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:51:44 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34165 after 8 ms (0 ms spent in bootstraps)
16/10/30 09:51:44 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:34165 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:51:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1187 ms on localhost (1/1)
16/10/30 09:51:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:44 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.276 s
16/10/30 09:51:44 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.525445 s
Result: 3.141571 in 1.782 seconds
16/10/30 09:51:44 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:44 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:44 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:44 INFO BlockManager: BlockManager stopped
16/10/30 09:51:44 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:44 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:44 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-1dd55518-3d17-4af5-bec6-a22b7bc8ad5b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:45 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:46 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:46 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:46 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:46 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:46 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:46 INFO Utils: Successfully started service 'sparkDriver' on port 41201.
16/10/30 09:51:46 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:46 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d355cc09-174e-4b71-b3c9-dfc3c537a064
16/10/30 09:51:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:46 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:46 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41201/jars/jni-spark-0.1.jar with timestamp 1477817506770
16/10/30 09:51:46 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38784.
16/10/30 09:51:46 INFO NettyBlockTransferService: Server created on 192.168.0.17:38784
16/10/30 09:51:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38784)
16/10/30 09:51:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38784 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38784)
16/10/30 09:51:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38784)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:48 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817508556
16/10/30 09:51:48 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-48a3bc70-a845-4afe-84c6-d520ea46ee55/userFiles-d01ad7c3-bee0-42e6-a3c2-0074d5932141/SparkJNIPi.so
16/10/30 09:51:48 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:48 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:48 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:48 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:48 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38784 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:49 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817508556
16/10/30 09:51:49 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-48a3bc70-a845-4afe-84c6-d520ea46ee55/userFiles-d01ad7c3-bee0-42e6-a3c2-0074d5932141/SparkJNIPi.so
16/10/30 09:51:49 INFO Executor: Fetching spark://192.168.0.17:41201/jars/jni-spark-0.1.jar with timestamp 1477817506770
16/10/30 09:51:49 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41201 after 38 ms (0 ms spent in bootstraps)
16/10/30 09:51:49 INFO Utils: Fetching spark://192.168.0.17:41201/jars/jni-spark-0.1.jar to /tmp/spark-48a3bc70-a845-4afe-84c6-d520ea46ee55/userFiles-d01ad7c3-bee0-42e6-a3c2-0074d5932141/fetchFileTemp8637391338911148203.tmp
16/10/30 09:51:49 INFO Executor: Adding file:/tmp/spark-48a3bc70-a845-4afe-84c6-d520ea46ee55/userFiles-d01ad7c3-bee0-42e6-a3c2-0074d5932141/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:50 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:51:50 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:38784 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:51:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:51:50 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38784 after 4 ms (0 ms spent in bootstraps)
16/10/30 09:51:50 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:38784 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:51:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1185 ms on localhost (1/1)
16/10/30 09:51:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:50 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.246 s
16/10/30 09:51:50 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.541309 s
Result: 3.1422758 in 1.804 seconds
16/10/30 09:51:50 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:50 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:50 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:50 INFO BlockManager: BlockManager stopped
16/10/30 09:51:50 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:50 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:50 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-48a3bc70-a845-4afe-84c6-d520ea46ee55
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:51 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:52 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:52 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:52 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:52 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:52 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:52 INFO Utils: Successfully started service 'sparkDriver' on port 42383.
16/10/30 09:51:52 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:52 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-38cd7a64-06ef-44db-a390-77aeffcf7d34
16/10/30 09:51:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:52 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:52 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42383/jars/jni-spark-0.1.jar with timestamp 1477817512728
16/10/30 09:51:52 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42471.
16/10/30 09:51:52 INFO NettyBlockTransferService: Server created on 192.168.0.17:42471
16/10/30 09:51:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42471)
16/10/30 09:51:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42471 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42471)
16/10/30 09:51:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42471)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:51:54 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817514511
16/10/30 09:51:54 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a538fc83-6f87-4c0b-9361-e8c45052a790/userFiles-20d8f22c-c526-41f5-85eb-5a69e8e1402c/SparkJNIPi.so
16/10/30 09:51:54 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:51:54 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:51:54 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:51:54 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:51:54 INFO DAGScheduler: Missing parents: List()
16/10/30 09:51:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:51:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:51:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:51:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42471 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:51:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:51:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:51:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:51:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:51:55 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817514511
16/10/30 09:51:55 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a538fc83-6f87-4c0b-9361-e8c45052a790/userFiles-20d8f22c-c526-41f5-85eb-5a69e8e1402c/SparkJNIPi.so
16/10/30 09:51:55 INFO Executor: Fetching spark://192.168.0.17:42383/jars/jni-spark-0.1.jar with timestamp 1477817512728
16/10/30 09:51:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42383 after 35 ms (0 ms spent in bootstraps)
16/10/30 09:51:55 INFO Utils: Fetching spark://192.168.0.17:42383/jars/jni-spark-0.1.jar to /tmp/spark-a538fc83-6f87-4c0b-9361-e8c45052a790/userFiles-20d8f22c-c526-41f5-85eb-5a69e8e1402c/fetchFileTemp6906218494096351711.tmp
16/10/30 09:51:55 INFO Executor: Adding file:/tmp/spark-a538fc83-6f87-4c0b-9361-e8c45052a790/userFiles-20d8f22c-c526-41f5-85eb-5a69e8e1402c/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:51:56 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:51:56 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:42471 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:51:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:51:56 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42471 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:51:56 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:42471 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:51:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1174 ms on localhost (1/1)
16/10/30 09:51:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:51:56 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.226 s
16/10/30 09:51:56 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.509011 s
Result: 3.140893 in 1.77 seconds
16/10/30 09:51:56 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:51:56 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:51:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:51:56 INFO MemoryStore: MemoryStore cleared
16/10/30 09:51:56 INFO BlockManager: BlockManager stopped
16/10/30 09:51:56 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:51:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:51:56 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:51:56 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:51:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-a538fc83-6f87-4c0b-9361-e8c45052a790
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:51:57 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:51:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:51:57 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:51:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:51:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:51:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:51:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:51:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:51:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:51:58 INFO Utils: Successfully started service 'sparkDriver' on port 43355.
16/10/30 09:51:58 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:51:58 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:51:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8cc12270-38cd-4cc2-9371-95d631adfaa7
16/10/30 09:51:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:51:58 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:51:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:51:58 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43355/jars/jni-spark-0.1.jar with timestamp 1477817518674
16/10/30 09:51:58 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:51:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36445.
16/10/30 09:51:58 INFO NettyBlockTransferService: Server created on 192.168.0.17:36445
16/10/30 09:51:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36445)
16/10/30 09:51:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36445 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36445)
16/10/30 09:51:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36445)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:00 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817520475
16/10/30 09:52:00 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-57baf16b-b78c-4e7c-b90b-f49458068d58/userFiles-85436386-b6a2-46b1-99b8-8a20e73a9bf9/SparkJNIPi.so
16/10/30 09:52:00 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:52:00 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:52:00 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:52:00 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:52:00 INFO DAGScheduler: Missing parents: List()
16/10/30 09:52:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:52:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:52:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:52:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36445 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:52:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:52:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:52:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:52:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:52:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:52:01 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817520475
16/10/30 09:52:01 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-57baf16b-b78c-4e7c-b90b-f49458068d58/userFiles-85436386-b6a2-46b1-99b8-8a20e73a9bf9/SparkJNIPi.so
16/10/30 09:52:01 INFO Executor: Fetching spark://192.168.0.17:43355/jars/jni-spark-0.1.jar with timestamp 1477817518674
16/10/30 09:52:01 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43355 after 24 ms (0 ms spent in bootstraps)
16/10/30 09:52:01 INFO Utils: Fetching spark://192.168.0.17:43355/jars/jni-spark-0.1.jar to /tmp/spark-57baf16b-b78c-4e7c-b90b-f49458068d58/userFiles-85436386-b6a2-46b1-99b8-8a20e73a9bf9/fetchFileTemp1119254176378812557.tmp
16/10/30 09:52:01 INFO Executor: Adding file:/tmp/spark-57baf16b-b78c-4e7c-b90b-f49458068d58/userFiles-85436386-b6a2-46b1-99b8-8a20e73a9bf9/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:52:02 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:52:02 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:36445 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:52:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:52:02 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36445 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:52:02 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:36445 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:52:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1155 ms on localhost (1/1)
16/10/30 09:52:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:52:02 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.210 s
16/10/30 09:52:02 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.519224 s
Result: 3.1419358 in 1.769 seconds
16/10/30 09:52:02 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:52:02 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:52:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:52:02 INFO MemoryStore: MemoryStore cleared
16/10/30 09:52:02 INFO BlockManager: BlockManager stopped
16/10/30 09:52:02 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:52:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:52:02 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:52:02 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:52:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-57baf16b-b78c-4e7c-b90b-f49458068d58
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:52:03 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:52:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:52:03 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:52:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:52:04 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:52:04 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:52:04 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:52:04 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:52:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:52:04 INFO Utils: Successfully started service 'sparkDriver' on port 42367.
16/10/30 09:52:04 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:52:04 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:52:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d9511bc4-8021-4396-b0ec-28be2fd16a9f
16/10/30 09:52:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:52:04 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:52:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:52:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:52:04 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42367/jars/jni-spark-0.1.jar with timestamp 1477817524708
16/10/30 09:52:04 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:52:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33877.
16/10/30 09:52:04 INFO NettyBlockTransferService: Server created on 192.168.0.17:33877
16/10/30 09:52:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33877)
16/10/30 09:52:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33877 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33877)
16/10/30 09:52:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33877)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:06 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817526534
16/10/30 09:52:06 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a3339c7c-4494-4568-b449-9521461df9b4/userFiles-af4913fe-1514-45d9-960b-05a35b47793a/SparkJNIPi.so
16/10/30 09:52:06 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:52:06 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:52:06 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:52:06 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:52:06 INFO DAGScheduler: Missing parents: List()
16/10/30 09:52:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:52:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:52:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:52:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33877 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:52:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:52:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:52:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:52:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:52:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:52:07 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817526534
16/10/30 09:52:07 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a3339c7c-4494-4568-b449-9521461df9b4/userFiles-af4913fe-1514-45d9-960b-05a35b47793a/SparkJNIPi.so
16/10/30 09:52:07 INFO Executor: Fetching spark://192.168.0.17:42367/jars/jni-spark-0.1.jar with timestamp 1477817524708
16/10/30 09:52:07 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42367 after 24 ms (0 ms spent in bootstraps)
16/10/30 09:52:07 INFO Utils: Fetching spark://192.168.0.17:42367/jars/jni-spark-0.1.jar to /tmp/spark-a3339c7c-4494-4568-b449-9521461df9b4/userFiles-af4913fe-1514-45d9-960b-05a35b47793a/fetchFileTemp87027295929727451.tmp
16/10/30 09:52:07 INFO Executor: Adding file:/tmp/spark-a3339c7c-4494-4568-b449-9521461df9b4/userFiles-af4913fe-1514-45d9-960b-05a35b47793a/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:52:12 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:52:12 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:33877 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:52:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:52:12 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33877 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:52:12 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:33877 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:52:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5662 ms on localhost (1/1)
16/10/30 09:52:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:52:12 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 5.833 s
16/10/30 09:52:12 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 6.108622 s
Result: 3.1421223 in 6.413 seconds
16/10/30 09:52:12 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:52:12 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:52:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:52:12 INFO MemoryStore: MemoryStore cleared
16/10/30 09:52:12 INFO BlockManager: BlockManager stopped
16/10/30 09:52:12 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:52:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:52:12 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:52:12 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:52:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3339c7c-4494-4568-b449-9521461df9b4
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:52:14 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:52:14 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:52:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:52:14 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:52:14 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:52:14 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:52:14 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:52:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:52:15 INFO Utils: Successfully started service 'sparkDriver' on port 37876.
16/10/30 09:52:15 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:52:15 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:52:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-853ce157-6ac0-4791-a138-b7ffe02fb2e4
16/10/30 09:52:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:52:15 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:52:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:52:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:52:15 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37876/jars/jni-spark-0.1.jar with timestamp 1477817535524
16/10/30 09:52:15 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:52:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46261.
16/10/30 09:52:15 INFO NettyBlockTransferService: Server created on 192.168.0.17:46261
16/10/30 09:52:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46261)
16/10/30 09:52:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46261 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46261)
16/10/30 09:52:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46261)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:17 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817537248
16/10/30 09:52:17 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-00bce254-c052-4c57-bdc7-c886a4a0be95/userFiles-d1429d53-3d1a-43ef-9af6-52d3b6e377b1/SparkJNIPi.so
16/10/30 09:52:17 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:52:17 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:52:17 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:52:17 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:52:17 INFO DAGScheduler: Missing parents: List()
16/10/30 09:52:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:52:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:52:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:52:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46261 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:52:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:52:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:52:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:52:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:52:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:52:17 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817537248
16/10/30 09:52:17 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-00bce254-c052-4c57-bdc7-c886a4a0be95/userFiles-d1429d53-3d1a-43ef-9af6-52d3b6e377b1/SparkJNIPi.so
16/10/30 09:52:17 INFO Executor: Fetching spark://192.168.0.17:37876/jars/jni-spark-0.1.jar with timestamp 1477817535524
16/10/30 09:52:17 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37876 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:52:18 INFO Utils: Fetching spark://192.168.0.17:37876/jars/jni-spark-0.1.jar to /tmp/spark-00bce254-c052-4c57-bdc7-c886a4a0be95/userFiles-d1429d53-3d1a-43ef-9af6-52d3b6e377b1/fetchFileTemp1247410410264199295.tmp
16/10/30 09:52:18 INFO Executor: Adding file:/tmp/spark-00bce254-c052-4c57-bdc7-c886a4a0be95/userFiles-d1429d53-3d1a-43ef-9af6-52d3b6e377b1/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:52:22 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:52:22 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:46261 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:52:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:52:23 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46261 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:52:23 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:46261 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:52:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5673 ms on localhost (1/1)
16/10/30 09:52:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:52:23 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 5.821 s
16/10/30 09:52:23 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 6.096456 s
Result: 3.1417036 in 6.404 seconds
16/10/30 09:52:23 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:52:23 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:52:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:52:23 INFO MemoryStore: MemoryStore cleared
16/10/30 09:52:23 INFO BlockManager: BlockManager stopped
16/10/30 09:52:23 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:52:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:52:23 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:52:23 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:52:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-00bce254-c052-4c57-bdc7-c886a4a0be95
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:52:24 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:52:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:52:25 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:52:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:52:25 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:52:25 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:52:25 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:52:25 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:52:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:52:25 INFO Utils: Successfully started service 'sparkDriver' on port 46259.
16/10/30 09:52:25 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:52:25 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:52:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f8c81f3a-256d-42fd-acb0-eb4b4c802850
16/10/30 09:52:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:52:25 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:52:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:52:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:52:26 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46259/jars/jni-spark-0.1.jar with timestamp 1477817546114
16/10/30 09:52:26 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:52:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46343.
16/10/30 09:52:26 INFO NettyBlockTransferService: Server created on 192.168.0.17:46343
16/10/30 09:52:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46343)
16/10/30 09:52:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46343 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46343)
16/10/30 09:52:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46343)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:27 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817547811
16/10/30 09:52:27 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-9c2c6b1a-e791-4148-ace3-7d2b83ac164b/userFiles-b8570888-71af-4319-9858-2fe71b49e373/SparkJNIPi.so
16/10/30 09:52:28 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:52:28 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:52:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:52:28 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:52:28 INFO DAGScheduler: Missing parents: List()
16/10/30 09:52:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:52:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:52:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:52:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46343 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:52:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:52:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:52:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:52:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:52:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:52:28 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817547811
16/10/30 09:52:28 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-9c2c6b1a-e791-4148-ace3-7d2b83ac164b/userFiles-b8570888-71af-4319-9858-2fe71b49e373/SparkJNIPi.so
16/10/30 09:52:28 INFO Executor: Fetching spark://192.168.0.17:46259/jars/jni-spark-0.1.jar with timestamp 1477817546114
16/10/30 09:52:28 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46259 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:52:28 INFO Utils: Fetching spark://192.168.0.17:46259/jars/jni-spark-0.1.jar to /tmp/spark-9c2c6b1a-e791-4148-ace3-7d2b83ac164b/userFiles-b8570888-71af-4319-9858-2fe71b49e373/fetchFileTemp8585783241556522268.tmp
16/10/30 09:52:28 INFO Executor: Adding file:/tmp/spark-9c2c6b1a-e791-4148-ace3-7d2b83ac164b/userFiles-b8570888-71af-4319-9858-2fe71b49e373/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:52:33 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:52:33 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:46343 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:52:33 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:52:33 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46343 after 2 ms (0 ms spent in bootstraps)
16/10/30 09:52:33 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:46343 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:52:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5534 ms on localhost (1/1)
16/10/30 09:52:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:52:34 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 5.686 s
16/10/30 09:52:34 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 5.946595 s
Result: 3.141433 in 6.243 seconds
16/10/30 09:52:34 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:52:34 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:52:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:52:34 INFO MemoryStore: MemoryStore cleared
16/10/30 09:52:34 INFO BlockManager: BlockManager stopped
16/10/30 09:52:34 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:52:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:52:34 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:52:34 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:52:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-9c2c6b1a-e791-4148-ace3-7d2b83ac164b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:52:35 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:52:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:52:35 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:52:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:52:35 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:52:35 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:52:35 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:52:35 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:52:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:52:36 INFO Utils: Successfully started service 'sparkDriver' on port 45484.
16/10/30 09:52:36 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:52:36 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:52:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a512a974-1c60-4def-adc2-83895a61ad48
16/10/30 09:52:36 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:52:36 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:52:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:52:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:52:36 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45484/jars/jni-spark-0.1.jar with timestamp 1477817556494
16/10/30 09:52:36 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:52:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42273.
16/10/30 09:52:36 INFO NettyBlockTransferService: Server created on 192.168.0.17:42273
16/10/30 09:52:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42273)
16/10/30 09:52:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42273 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42273)
16/10/30 09:52:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42273)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:38 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817558313
16/10/30 09:52:38 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8257b89b-115d-4bda-ad95-0d6ea930f757/userFiles-bb397322-29ce-482b-9c07-4ce68c7876ce/SparkJNIPi.so
16/10/30 09:52:38 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:52:38 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:52:38 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:52:38 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:52:38 INFO DAGScheduler: Missing parents: List()
16/10/30 09:52:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:52:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:52:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:52:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42273 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:52:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:52:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:52:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:52:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 09:52:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:52:38 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817558313
16/10/30 09:52:39 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8257b89b-115d-4bda-ad95-0d6ea930f757/userFiles-bb397322-29ce-482b-9c07-4ce68c7876ce/SparkJNIPi.so
16/10/30 09:52:39 INFO Executor: Fetching spark://192.168.0.17:45484/jars/jni-spark-0.1.jar with timestamp 1477817556494
16/10/30 09:52:39 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45484 after 25 ms (0 ms spent in bootstraps)
16/10/30 09:52:39 INFO Utils: Fetching spark://192.168.0.17:45484/jars/jni-spark-0.1.jar to /tmp/spark-8257b89b-115d-4bda-ad95-0d6ea930f757/userFiles-bb397322-29ce-482b-9c07-4ce68c7876ce/fetchFileTemp6803753569786715530.tmp
16/10/30 09:52:39 INFO Executor: Adding file:/tmp/spark-8257b89b-115d-4bda-ad95-0d6ea930f757/userFiles-bb397322-29ce-482b-9c07-4ce68c7876ce/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:52:44 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:52:44 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:42273 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:52:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:52:44 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42273 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:52:44 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:42273 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:52:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5872 ms on localhost (1/1)
16/10/30 09:52:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:52:44 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 6.031 s
16/10/30 09:52:44 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 6.319218 s
Result: 3.141816 in 6.636 seconds
16/10/30 09:52:44 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:52:44 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:52:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:52:44 INFO MemoryStore: MemoryStore cleared
16/10/30 09:52:44 INFO BlockManager: BlockManager stopped
16/10/30 09:52:44 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:52:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:52:45 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:52:45 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:52:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-8257b89b-115d-4bda-ad95-0d6ea930f757
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:52:46 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:52:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:52:46 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:52:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:52:46 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:52:46 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:52:46 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:52:46 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:52:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:52:47 INFO Utils: Successfully started service 'sparkDriver' on port 34055.
16/10/30 09:52:47 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:52:47 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:52:47 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9b6e0cea-edbb-4ae8-9dae-a4bc663f3d40
16/10/30 09:52:47 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:52:47 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:52:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:52:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:52:47 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34055/jars/jni-spark-0.1.jar with timestamp 1477817567523
16/10/30 09:52:47 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:52:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33210.
16/10/30 09:52:47 INFO NettyBlockTransferService: Server created on 192.168.0.17:33210
16/10/30 09:52:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33210)
16/10/30 09:52:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33210 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33210)
16/10/30 09:52:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33210)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:49 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817569237
16/10/30 09:52:49 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-473ba5af-af06-43ed-a5ce-610c739893d1/userFiles-2f3b65a8-2ab5-45d0-8650-5ed4b4dd0dbf/SparkJNIPi.so
16/10/30 09:52:49 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:52:49 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:52:49 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:52:49 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:52:49 INFO DAGScheduler: Missing parents: List()
16/10/30 09:52:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:52:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:52:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:52:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33210 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:52:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:52:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:52:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:52:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:52:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:52:49 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817569237
16/10/30 09:52:49 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-473ba5af-af06-43ed-a5ce-610c739893d1/userFiles-2f3b65a8-2ab5-45d0-8650-5ed4b4dd0dbf/SparkJNIPi.so
16/10/30 09:52:49 INFO Executor: Fetching spark://192.168.0.17:34055/jars/jni-spark-0.1.jar with timestamp 1477817567523
16/10/30 09:52:49 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34055 after 35 ms (0 ms spent in bootstraps)
16/10/30 09:52:49 INFO Utils: Fetching spark://192.168.0.17:34055/jars/jni-spark-0.1.jar to /tmp/spark-473ba5af-af06-43ed-a5ce-610c739893d1/userFiles-2f3b65a8-2ab5-45d0-8650-5ed4b4dd0dbf/fetchFileTemp8678834509021777601.tmp
16/10/30 09:52:50 INFO Executor: Adding file:/tmp/spark-473ba5af-af06-43ed-a5ce-610c739893d1/userFiles-2f3b65a8-2ab5-45d0-8650-5ed4b4dd0dbf/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:52:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:52:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 537 ms on localhost (1/1)
16/10/30 09:52:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:52:50 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.555 s
16/10/30 09:52:50 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.834048 s
Result: 3.1230469 in 1.129 seconds
16/10/30 09:52:50 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:52:50 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:52:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:52:50 INFO MemoryStore: MemoryStore cleared
16/10/30 09:52:50 INFO BlockManager: BlockManager stopped
16/10/30 09:52:50 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:52:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:52:50 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:52:50 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:52:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-473ba5af-af06-43ed-a5ce-610c739893d1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:52:51 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:52:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:52:52 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:52:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:52:52 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:52:52 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:52:52 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:52:52 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:52:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:52:52 INFO Utils: Successfully started service 'sparkDriver' on port 33468.
16/10/30 09:52:52 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:52:52 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:52:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aa52b3a9-edf4-4567-8468-403ef7b88e50
16/10/30 09:52:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:52:52 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:52:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:52:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:52:52 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33468/jars/jni-spark-0.1.jar with timestamp 1477817572826
16/10/30 09:52:52 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:52:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32989.
16/10/30 09:52:52 INFO NettyBlockTransferService: Server created on 192.168.0.17:32989
16/10/30 09:52:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 32989)
16/10/30 09:52:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:32989 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 32989)
16/10/30 09:52:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 32989)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:54 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817574603
16/10/30 09:52:54 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-7ef23bfb-fcb8-4b6d-97e4-bdfb6c9cd4dd/userFiles-071660f0-881c-46e2-8cd3-9aac7eb8e152/SparkJNIPi.so
16/10/30 09:52:54 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:52:54 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:52:54 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:52:54 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:52:54 INFO DAGScheduler: Missing parents: List()
16/10/30 09:52:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:52:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:52:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:52:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:32989 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:52:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:52:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:52:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:52:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:52:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:52:55 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817574603
16/10/30 09:52:55 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-7ef23bfb-fcb8-4b6d-97e4-bdfb6c9cd4dd/userFiles-071660f0-881c-46e2-8cd3-9aac7eb8e152/SparkJNIPi.so
16/10/30 09:52:55 INFO Executor: Fetching spark://192.168.0.17:33468/jars/jni-spark-0.1.jar with timestamp 1477817572826
16/10/30 09:52:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33468 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:52:55 INFO Utils: Fetching spark://192.168.0.17:33468/jars/jni-spark-0.1.jar to /tmp/spark-7ef23bfb-fcb8-4b6d-97e4-bdfb6c9cd4dd/userFiles-071660f0-881c-46e2-8cd3-9aac7eb8e152/fetchFileTemp3067009735340572526.tmp
16/10/30 09:52:55 INFO Executor: Adding file:/tmp/spark-7ef23bfb-fcb8-4b6d-97e4-bdfb6c9cd4dd/userFiles-071660f0-881c-46e2-8cd3-9aac7eb8e152/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:52:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:52:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 552 ms on localhost (1/1)
16/10/30 09:52:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:52:55 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.622 s
16/10/30 09:52:55 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.888807 s
Result: 3.118164 in 1.148 seconds
16/10/30 09:52:55 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:52:55 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:52:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:52:55 INFO MemoryStore: MemoryStore cleared
16/10/30 09:52:55 INFO BlockManager: BlockManager stopped
16/10/30 09:52:55 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:52:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:52:55 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:52:55 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:52:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ef23bfb-fcb8-4b6d-97e4-bdfb6c9cd4dd
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:52:57 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:52:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:52:57 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:52:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:52:57 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:52:57 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:52:57 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:52:57 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:52:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:52:57 INFO Utils: Successfully started service 'sparkDriver' on port 36416.
16/10/30 09:52:57 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:52:57 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:52:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-366c7346-beb1-4bd2-b4a7-cea2231b9069
16/10/30 09:52:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:52:57 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:52:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:52:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:52:58 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36416/jars/jni-spark-0.1.jar with timestamp 1477817578224
16/10/30 09:52:58 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:52:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40175.
16/10/30 09:52:58 INFO NettyBlockTransferService: Server created on 192.168.0.17:40175
16/10/30 09:52:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40175)
16/10/30 09:52:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40175 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40175)
16/10/30 09:52:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40175)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:52:59 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817579984
16/10/30 09:52:59 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-e262661f-e7b4-4257-819f-e9e5e46762d2/userFiles-2483740d-5499-4418-bea6-8bcb5e0140d3/SparkJNIPi.so
16/10/30 09:53:00 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:00 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:00 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:00 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:00 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40175 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:00 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817579984
16/10/30 09:53:00 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-e262661f-e7b4-4257-819f-e9e5e46762d2/userFiles-2483740d-5499-4418-bea6-8bcb5e0140d3/SparkJNIPi.so
16/10/30 09:53:00 INFO Executor: Fetching spark://192.168.0.17:36416/jars/jni-spark-0.1.jar with timestamp 1477817578224
16/10/30 09:53:00 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36416 after 25 ms (0 ms spent in bootstraps)
16/10/30 09:53:00 INFO Utils: Fetching spark://192.168.0.17:36416/jars/jni-spark-0.1.jar to /tmp/spark-e262661f-e7b4-4257-819f-e9e5e46762d2/userFiles-2483740d-5499-4418-bea6-8bcb5e0140d3/fetchFileTemp4645341945187565287.tmp
16/10/30 09:53:00 INFO Executor: Adding file:/tmp/spark-e262661f-e7b4-4257-819f-e9e5e46762d2/userFiles-2483740d-5499-4418-bea6-8bcb5e0140d3/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:53:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 524 ms on localhost (1/1)
16/10/30 09:53:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:01 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.537 s
16/10/30 09:53:01 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.815805 s
Result: 3.1386719 in 1.097 seconds
16/10/30 09:53:01 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:01 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:01 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:01 INFO BlockManager: BlockManager stopped
16/10/30 09:53:01 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:01 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:01 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-e262661f-e7b4-4257-819f-e9e5e46762d2
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:02 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:02 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:02 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:02 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:02 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:02 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:03 INFO Utils: Successfully started service 'sparkDriver' on port 44090.
16/10/30 09:53:03 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:03 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-92d6087f-54ab-4946-bb08-0f79ac825251
16/10/30 09:53:03 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:03 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:03 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44090/jars/jni-spark-0.1.jar with timestamp 1477817583548
16/10/30 09:53:03 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46637.
16/10/30 09:53:03 INFO NettyBlockTransferService: Server created on 192.168.0.17:46637
16/10/30 09:53:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46637)
16/10/30 09:53:03 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46637 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46637)
16/10/30 09:53:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46637)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:05 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817585298
16/10/30 09:53:05 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-5d5b460c-33eb-4ce0-a648-3205a0a6eca3/userFiles-e82f2950-d126-4327-86a4-1d75bac0ecef/SparkJNIPi.so
16/10/30 09:53:05 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:05 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:05 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:05 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:05 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:05 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46637 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:05 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817585298
16/10/30 09:53:05 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-5d5b460c-33eb-4ce0-a648-3205a0a6eca3/userFiles-e82f2950-d126-4327-86a4-1d75bac0ecef/SparkJNIPi.so
16/10/30 09:53:05 INFO Executor: Fetching spark://192.168.0.17:44090/jars/jni-spark-0.1.jar with timestamp 1477817583548
16/10/30 09:53:06 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44090 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:53:06 INFO Utils: Fetching spark://192.168.0.17:44090/jars/jni-spark-0.1.jar to /tmp/spark-5d5b460c-33eb-4ce0-a648-3205a0a6eca3/userFiles-e82f2950-d126-4327-86a4-1d75bac0ecef/fetchFileTemp3379387711431351365.tmp
16/10/30 09:53:06 INFO Executor: Adding file:/tmp/spark-5d5b460c-33eb-4ce0-a648-3205a0a6eca3/userFiles-e82f2950-d126-4327-86a4-1d75bac0ecef/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 09:53:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 533 ms on localhost (1/1)
16/10/30 09:53:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:06 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.550 s
16/10/30 09:53:06 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.807018 s
Result: 3.0917969 in 1.119 seconds
16/10/30 09:53:06 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:06 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:06 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:06 INFO BlockManager: BlockManager stopped
16/10/30 09:53:06 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:06 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:06 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d5b460c-33eb-4ce0-a648-3205a0a6eca3
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:07 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:08 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:08 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:08 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:08 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:08 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:08 INFO Utils: Successfully started service 'sparkDriver' on port 35914.
16/10/30 09:53:08 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:08 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ebbae82c-e223-4fe7-88dc-93bb738ee998
16/10/30 09:53:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:08 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:08 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35914/jars/jni-spark-0.1.jar with timestamp 1477817588785
16/10/30 09:53:08 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38676.
16/10/30 09:53:08 INFO NettyBlockTransferService: Server created on 192.168.0.17:38676
16/10/30 09:53:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38676)
16/10/30 09:53:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38676 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38676)
16/10/30 09:53:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38676)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:10 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817590690
16/10/30 09:53:10 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-0345c445-de01-4158-bb5f-16cc32b5cc0f/userFiles-ac1afae3-4e15-4dc7-a6c9-5b712a11d2d9/SparkJNIPi.so
16/10/30 09:53:10 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:11 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:11 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:11 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:11 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38676 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:11 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817590690
16/10/30 09:53:11 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-0345c445-de01-4158-bb5f-16cc32b5cc0f/userFiles-ac1afae3-4e15-4dc7-a6c9-5b712a11d2d9/SparkJNIPi.so
16/10/30 09:53:11 INFO Executor: Fetching spark://192.168.0.17:35914/jars/jni-spark-0.1.jar with timestamp 1477817588785
16/10/30 09:53:11 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35914 after 24 ms (0 ms spent in bootstraps)
16/10/30 09:53:11 INFO Utils: Fetching spark://192.168.0.17:35914/jars/jni-spark-0.1.jar to /tmp/spark-0345c445-de01-4158-bb5f-16cc32b5cc0f/userFiles-ac1afae3-4e15-4dc7-a6c9-5b712a11d2d9/fetchFileTemp9041448549370174727.tmp
16/10/30 09:53:11 INFO Executor: Adding file:/tmp/spark-0345c445-de01-4158-bb5f-16cc32b5cc0f/userFiles-ac1afae3-4e15-4dc7-a6c9-5b712a11d2d9/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:53:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 517 ms on localhost (1/1)
16/10/30 09:53:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:11 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.530 s
16/10/30 09:53:11 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.794531 s
Result: 3.1427002 in 1.072 seconds
16/10/30 09:53:11 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:11 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:11 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:11 INFO BlockManager: BlockManager stopped
16/10/30 09:53:11 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:11 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:11 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-0345c445-de01-4158-bb5f-16cc32b5cc0f
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:12 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:13 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:13 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:13 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:13 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:13 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:13 INFO Utils: Successfully started service 'sparkDriver' on port 36282.
16/10/30 09:53:13 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:13 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5d8a2784-f4e5-455c-9e50-56ab515b2dff
16/10/30 09:53:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:13 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:14 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36282/jars/jni-spark-0.1.jar with timestamp 1477817594187
16/10/30 09:53:14 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40239.
16/10/30 09:53:14 INFO NettyBlockTransferService: Server created on 192.168.0.17:40239
16/10/30 09:53:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40239)
16/10/30 09:53:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40239 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40239)
16/10/30 09:53:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40239)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:15 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817595811
16/10/30 09:53:15 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-490abf8e-ef26-4950-af41-ddbcbed5fe5e/userFiles-1993db96-1315-4c4b-9f54-1fdf8c1b5abf/SparkJNIPi.so
16/10/30 09:53:16 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:16 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:16 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:16 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:16 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40239 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:16 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817595811
16/10/30 09:53:16 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-490abf8e-ef26-4950-af41-ddbcbed5fe5e/userFiles-1993db96-1315-4c4b-9f54-1fdf8c1b5abf/SparkJNIPi.so
16/10/30 09:53:16 INFO Executor: Fetching spark://192.168.0.17:36282/jars/jni-spark-0.1.jar with timestamp 1477817594187
16/10/30 09:53:16 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36282 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:53:16 INFO Utils: Fetching spark://192.168.0.17:36282/jars/jni-spark-0.1.jar to /tmp/spark-490abf8e-ef26-4950-af41-ddbcbed5fe5e/userFiles-1993db96-1315-4c4b-9f54-1fdf8c1b5abf/fetchFileTemp8189818360922554154.tmp
16/10/30 09:53:16 INFO Executor: Adding file:/tmp/spark-490abf8e-ef26-4950-af41-ddbcbed5fe5e/userFiles-1993db96-1315-4c4b-9f54-1fdf8c1b5abf/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:53:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 511 ms on localhost (1/1)
16/10/30 09:53:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:16 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.524 s
16/10/30 09:53:16 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.774384 s
Result: 3.1391602 in 1.068 seconds
16/10/30 09:53:16 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:16 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:16 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:16 INFO BlockManager: BlockManager stopped
16/10/30 09:53:16 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:16 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:16 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-490abf8e-ef26-4950-af41-ddbcbed5fe5e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:18 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:18 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:18 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:18 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:18 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:18 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:18 INFO Utils: Successfully started service 'sparkDriver' on port 35554.
16/10/30 09:53:19 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:19 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-372ee543-0efa-484e-a789-634a40a2f14c
16/10/30 09:53:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:19 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:19 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35554/jars/jni-spark-0.1.jar with timestamp 1477817599386
16/10/30 09:53:19 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45124.
16/10/30 09:53:19 INFO NettyBlockTransferService: Server created on 192.168.0.17:45124
16/10/30 09:53:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45124)
16/10/30 09:53:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45124 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45124)
16/10/30 09:53:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45124)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:21 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817601209
16/10/30 09:53:21 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-19af9cd2-a5dd-463e-9833-3c4406d4288e/userFiles-a236449b-5702-43bb-b594-b9307e47abb2/SparkJNIPi.so
16/10/30 09:53:21 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:21 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:21 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:21 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:21 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45124 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:21 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817601209
16/10/30 09:53:21 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-19af9cd2-a5dd-463e-9833-3c4406d4288e/userFiles-a236449b-5702-43bb-b594-b9307e47abb2/SparkJNIPi.so
16/10/30 09:53:21 INFO Executor: Fetching spark://192.168.0.17:35554/jars/jni-spark-0.1.jar with timestamp 1477817599386
16/10/30 09:53:21 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35554 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:53:21 INFO Utils: Fetching spark://192.168.0.17:35554/jars/jni-spark-0.1.jar to /tmp/spark-19af9cd2-a5dd-463e-9833-3c4406d4288e/userFiles-a236449b-5702-43bb-b594-b9307e47abb2/fetchFileTemp5184278181651078733.tmp
16/10/30 09:53:21 INFO Executor: Adding file:/tmp/spark-19af9cd2-a5dd-463e-9833-3c4406d4288e/userFiles-a236449b-5702-43bb-b594-b9307e47abb2/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:53:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 553 ms on localhost (1/1)
16/10/30 09:53:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:22 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.570 s
16/10/30 09:53:22 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.841708 s
Result: 3.142456 in 1.114 seconds
16/10/30 09:53:22 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:22 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:22 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:22 INFO BlockManager: BlockManager stopped
16/10/30 09:53:22 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:22 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:22 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-19af9cd2-a5dd-463e-9833-3c4406d4288e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:23 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:24 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:24 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:24 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:24 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:24 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:24 INFO Utils: Successfully started service 'sparkDriver' on port 43223.
16/10/30 09:53:24 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:24 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d218a9f2-64f4-413b-839f-0dce52517ffa
16/10/30 09:53:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:24 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:24 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43223/jars/jni-spark-0.1.jar with timestamp 1477817604765
16/10/30 09:53:24 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35603.
16/10/30 09:53:24 INFO NettyBlockTransferService: Server created on 192.168.0.17:35603
16/10/30 09:53:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35603)
16/10/30 09:53:24 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35603 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35603)
16/10/30 09:53:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35603)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:26 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817606527
16/10/30 09:53:26 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-460fe7f0-8bc5-48bb-94e9-c1016e4cbafa/userFiles-1f1eefa1-fdb5-4c07-9d84-8c89bc35b568/SparkJNIPi.so
16/10/30 09:53:26 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:26 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:26 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:26 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:26 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35603 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:27 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817606527
16/10/30 09:53:27 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-460fe7f0-8bc5-48bb-94e9-c1016e4cbafa/userFiles-1f1eefa1-fdb5-4c07-9d84-8c89bc35b568/SparkJNIPi.so
16/10/30 09:53:27 INFO Executor: Fetching spark://192.168.0.17:43223/jars/jni-spark-0.1.jar with timestamp 1477817604765
16/10/30 09:53:27 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43223 after 22 ms (0 ms spent in bootstraps)
16/10/30 09:53:27 INFO Utils: Fetching spark://192.168.0.17:43223/jars/jni-spark-0.1.jar to /tmp/spark-460fe7f0-8bc5-48bb-94e9-c1016e4cbafa/userFiles-1f1eefa1-fdb5-4c07-9d84-8c89bc35b568/fetchFileTemp862668858338063090.tmp
16/10/30 09:53:27 INFO Executor: Adding file:/tmp/spark-460fe7f0-8bc5-48bb-94e9-c1016e4cbafa/userFiles-1f1eefa1-fdb5-4c07-9d84-8c89bc35b568/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 09:53:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 513 ms on localhost (1/1)
16/10/30 09:53:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:27 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.527 s
16/10/30 09:53:27 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.791777 s
Result: 3.1436768 in 1.059 seconds
16/10/30 09:53:27 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:27 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:27 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:27 INFO BlockManager: BlockManager stopped
16/10/30 09:53:27 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:27 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:27 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-460fe7f0-8bc5-48bb-94e9-c1016e4cbafa
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:28 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:29 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:29 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:29 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:29 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:29 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:29 INFO Utils: Successfully started service 'sparkDriver' on port 40328.
16/10/30 09:53:29 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:29 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4dc48028-3ee6-4a84-b850-97ade5ec48f0
16/10/30 09:53:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:29 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:29 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40328/jars/jni-spark-0.1.jar with timestamp 1477817609962
16/10/30 09:53:30 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34673.
16/10/30 09:53:30 INFO NettyBlockTransferService: Server created on 192.168.0.17:34673
16/10/30 09:53:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34673)
16/10/30 09:53:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34673 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34673)
16/10/30 09:53:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34673)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:31 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817611757
16/10/30 09:53:31 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-f83b00d5-af14-4216-877c-e37768fbd75d/userFiles-8dd7531b-e418-4730-9ea3-e9980c5e880f/SparkJNIPi.so
16/10/30 09:53:32 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:32 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:32 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:32 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:32 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34673 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:32 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817611757
16/10/30 09:53:32 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-f83b00d5-af14-4216-877c-e37768fbd75d/userFiles-8dd7531b-e418-4730-9ea3-e9980c5e880f/SparkJNIPi.so
16/10/30 09:53:32 INFO Executor: Fetching spark://192.168.0.17:40328/jars/jni-spark-0.1.jar with timestamp 1477817609962
16/10/30 09:53:32 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40328 after 36 ms (0 ms spent in bootstraps)
16/10/30 09:53:32 INFO Utils: Fetching spark://192.168.0.17:40328/jars/jni-spark-0.1.jar to /tmp/spark-f83b00d5-af14-4216-877c-e37768fbd75d/userFiles-8dd7531b-e418-4730-9ea3-e9980c5e880f/fetchFileTemp6495248506100608720.tmp
16/10/30 09:53:32 INFO Executor: Adding file:/tmp/spark-f83b00d5-af14-4216-877c-e37768fbd75d/userFiles-8dd7531b-e418-4730-9ea3-e9980c5e880f/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:53:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 546 ms on localhost (1/1)
16/10/30 09:53:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:32 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.558 s
16/10/30 09:53:32 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.802333 s
Result: 3.14122 in 1.108 seconds
16/10/30 09:53:32 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:32 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:32 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:32 INFO BlockManager: BlockManager stopped
16/10/30 09:53:32 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:32 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:32 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-f83b00d5-af14-4216-877c-e37768fbd75d
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:34 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:34 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:34 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:34 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:34 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:34 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:34 INFO Utils: Successfully started service 'sparkDriver' on port 43339.
16/10/30 09:53:34 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:35 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-33ecf67b-9c28-4a29-8196-3973b35b2569
16/10/30 09:53:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:35 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:35 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:35 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:35 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43339/jars/jni-spark-0.1.jar with timestamp 1477817615373
16/10/30 09:53:35 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36307.
16/10/30 09:53:35 INFO NettyBlockTransferService: Server created on 192.168.0.17:36307
16/10/30 09:53:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36307)
16/10/30 09:53:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36307 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36307)
16/10/30 09:53:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36307)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:37 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817617167
16/10/30 09:53:37 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-aa3da4dd-4e83-4c21-bbe1-fd63932e3f5a/userFiles-91150b6f-7ab0-432d-a5df-db07a9b1c20b/SparkJNIPi.so
16/10/30 09:53:37 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:37 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:37 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:37 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:37 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36307 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:37 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817617167
16/10/30 09:53:37 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-aa3da4dd-4e83-4c21-bbe1-fd63932e3f5a/userFiles-91150b6f-7ab0-432d-a5df-db07a9b1c20b/SparkJNIPi.so
16/10/30 09:53:37 INFO Executor: Fetching spark://192.168.0.17:43339/jars/jni-spark-0.1.jar with timestamp 1477817615373
16/10/30 09:53:37 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43339 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:53:37 INFO Utils: Fetching spark://192.168.0.17:43339/jars/jni-spark-0.1.jar to /tmp/spark-aa3da4dd-4e83-4c21-bbe1-fd63932e3f5a/userFiles-91150b6f-7ab0-432d-a5df-db07a9b1c20b/fetchFileTemp2137762576754818510.tmp
16/10/30 09:53:37 INFO Executor: Adding file:/tmp/spark-aa3da4dd-4e83-4c21-bbe1-fd63932e3f5a/userFiles-91150b6f-7ab0-432d-a5df-db07a9b1c20b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:53:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 563 ms on localhost (1/1)
16/10/30 09:53:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:38 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.577 s
16/10/30 09:53:38 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.866663 s
Result: 3.1378632 in 1.152 seconds
16/10/30 09:53:38 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:38 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:38 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:38 INFO BlockManager: BlockManager stopped
16/10/30 09:53:38 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:38 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:38 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-aa3da4dd-4e83-4c21-bbe1-fd63932e3f5a
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:39 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:39 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:40 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:40 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:40 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:40 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:40 INFO Utils: Successfully started service 'sparkDriver' on port 45110.
16/10/30 09:53:40 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:40 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-246df602-bb84-47a7-93d9-497bcf0f1cbb
16/10/30 09:53:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:40 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:40 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45110/jars/jni-spark-0.1.jar with timestamp 1477817620690
16/10/30 09:53:40 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38093.
16/10/30 09:53:40 INFO NettyBlockTransferService: Server created on 192.168.0.17:38093
16/10/30 09:53:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38093)
16/10/30 09:53:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38093 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38093)
16/10/30 09:53:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38093)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:42 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817622451
16/10/30 09:53:42 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-fb42cfc0-5510-4383-bb55-2cf9b4cf7274/userFiles-f0a5ee93-cdbf-44fc-b8c1-5ce8c8214d53/SparkJNIPi.so
16/10/30 09:53:42 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:42 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:42 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:42 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:42 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38093 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:43 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817622451
16/10/30 09:53:43 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-fb42cfc0-5510-4383-bb55-2cf9b4cf7274/userFiles-f0a5ee93-cdbf-44fc-b8c1-5ce8c8214d53/SparkJNIPi.so
16/10/30 09:53:43 INFO Executor: Fetching spark://192.168.0.17:45110/jars/jni-spark-0.1.jar with timestamp 1477817620690
16/10/30 09:53:43 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45110 after 33 ms (0 ms spent in bootstraps)
16/10/30 09:53:43 INFO Utils: Fetching spark://192.168.0.17:45110/jars/jni-spark-0.1.jar to /tmp/spark-fb42cfc0-5510-4383-bb55-2cf9b4cf7274/userFiles-f0a5ee93-cdbf-44fc-b8c1-5ce8c8214d53/fetchFileTemp972108486136747826.tmp
16/10/30 09:53:43 INFO Executor: Adding file:/tmp/spark-fb42cfc0-5510-4383-bb55-2cf9b4cf7274/userFiles-f0a5ee93-cdbf-44fc-b8c1-5ce8c8214d53/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:53:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 556 ms on localhost (1/1)
16/10/30 09:53:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:43 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.571 s
16/10/30 09:53:43 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.861719 s
Result: 3.1461182 in 1.11 seconds
16/10/30 09:53:43 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:43 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:43 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:43 INFO BlockManager: BlockManager stopped
16/10/30 09:53:43 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:43 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:43 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-fb42cfc0-5510-4383-bb55-2cf9b4cf7274
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:44 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:45 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:45 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:45 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:45 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:45 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:45 INFO Utils: Successfully started service 'sparkDriver' on port 46221.
16/10/30 09:53:45 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:45 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1d9722e4-06fc-4d61-8861-215d2cfd99e8
16/10/30 09:53:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:45 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:46 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46221/jars/jni-spark-0.1.jar with timestamp 1477817626033
16/10/30 09:53:46 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42583.
16/10/30 09:53:46 INFO NettyBlockTransferService: Server created on 192.168.0.17:42583
16/10/30 09:53:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42583)
16/10/30 09:53:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42583 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42583)
16/10/30 09:53:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42583)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:47 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817627751
16/10/30 09:53:47 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6618bbd8-35fc-4081-ae52-1b1f7afd416e/userFiles-c2546b89-50c9-4f74-b55d-d9d55d3893ed/SparkJNIPi.so
16/10/30 09:53:48 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:48 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:48 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:48 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:48 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42583 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:48 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817627751
16/10/30 09:53:48 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6618bbd8-35fc-4081-ae52-1b1f7afd416e/userFiles-c2546b89-50c9-4f74-b55d-d9d55d3893ed/SparkJNIPi.so
16/10/30 09:53:48 INFO Executor: Fetching spark://192.168.0.17:46221/jars/jni-spark-0.1.jar with timestamp 1477817626033
16/10/30 09:53:48 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46221 after 25 ms (0 ms spent in bootstraps)
16/10/30 09:53:48 INFO Utils: Fetching spark://192.168.0.17:46221/jars/jni-spark-0.1.jar to /tmp/spark-6618bbd8-35fc-4081-ae52-1b1f7afd416e/userFiles-c2546b89-50c9-4f74-b55d-d9d55d3893ed/fetchFileTemp4303771523371864706.tmp
16/10/30 09:53:48 INFO Executor: Adding file:/tmp/spark-6618bbd8-35fc-4081-ae52-1b1f7afd416e/userFiles-c2546b89-50c9-4f74-b55d-d9d55d3893ed/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:48 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 264513 bytes result sent to driver
16/10/30 09:53:48 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 533 ms on localhost (1/1)
16/10/30 09:53:48 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:48 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.545 s
16/10/30 09:53:48 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.833185 s
Result: 3.1415863 in 1.087 seconds
16/10/30 09:53:48 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:48 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:48 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:48 INFO BlockManager: BlockManager stopped
16/10/30 09:53:48 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:48 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:48 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-6618bbd8-35fc-4081-ae52-1b1f7afd416e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:50 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:50 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:50 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:50 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:50 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:50 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:50 INFO Utils: Successfully started service 'sparkDriver' on port 45153.
16/10/30 09:53:50 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:50 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-78f297dc-bd4e-45b9-8cd7-296ac68d9262
16/10/30 09:53:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:51 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:51 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45153/jars/jni-spark-0.1.jar with timestamp 1477817631251
16/10/30 09:53:51 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40291.
16/10/30 09:53:51 INFO NettyBlockTransferService: Server created on 192.168.0.17:40291
16/10/30 09:53:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40291)
16/10/30 09:53:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40291 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40291)
16/10/30 09:53:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40291)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:52 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817632962
16/10/30 09:53:52 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-04f84dc4-c0a9-49f7-bb35-e0218a22709b/userFiles-4ece2a44-9b1b-4be6-b62b-05e82f376147/SparkJNIPi.so
16/10/30 09:53:53 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:53 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:53 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:53 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:53 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40291 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:53 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817632962
16/10/30 09:53:53 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-04f84dc4-c0a9-49f7-bb35-e0218a22709b/userFiles-4ece2a44-9b1b-4be6-b62b-05e82f376147/SparkJNIPi.so
16/10/30 09:53:53 INFO Executor: Fetching spark://192.168.0.17:45153/jars/jni-spark-0.1.jar with timestamp 1477817631251
16/10/30 09:53:53 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45153 after 37 ms (0 ms spent in bootstraps)
16/10/30 09:53:53 INFO Utils: Fetching spark://192.168.0.17:45153/jars/jni-spark-0.1.jar to /tmp/spark-04f84dc4-c0a9-49f7-bb35-e0218a22709b/userFiles-4ece2a44-9b1b-4be6-b62b-05e82f376147/fetchFileTemp8050373755210810059.tmp
16/10/30 09:53:53 INFO Executor: Adding file:/tmp/spark-04f84dc4-c0a9-49f7-bb35-e0218a22709b/userFiles-4ece2a44-9b1b-4be6-b62b-05e82f376147/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:53:54 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:53:54 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:40291 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:53:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:53:54 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40291 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:53:54 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:40291 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:53:54 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 800 ms on localhost (1/1)
16/10/30 09:53:54 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.828 s
16/10/30 09:53:54 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:53:54 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.079695 s
Result: 3.1437626 in 1.361 seconds
16/10/30 09:53:54 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:53:54 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:53:54 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:53:54 INFO MemoryStore: MemoryStore cleared
16/10/30 09:53:54 INFO BlockManager: BlockManager stopped
16/10/30 09:53:54 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:53:54 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:53:54 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:53:54 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:53:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-04f84dc4-c0a9-49f7-bb35-e0218a22709b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:53:55 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:53:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:53:56 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:53:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:53:56 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:53:56 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:53:56 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:53:56 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:53:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:53:56 INFO Utils: Successfully started service 'sparkDriver' on port 46587.
16/10/30 09:53:56 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:53:56 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:53:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3f2121b9-d3e6-40f7-b2af-4cd89bdedea0
16/10/30 09:53:56 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:53:56 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:53:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:53:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:53:56 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46587/jars/jni-spark-0.1.jar with timestamp 1477817636785
16/10/30 09:53:56 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:53:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33879.
16/10/30 09:53:56 INFO NettyBlockTransferService: Server created on 192.168.0.17:33879
16/10/30 09:53:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33879)
16/10/30 09:53:56 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33879 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33879)
16/10/30 09:53:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33879)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:53:58 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817638667
16/10/30 09:53:58 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-fe48443d-90cc-4164-97ad-f539821e3d18/userFiles-1e65401d-8365-4faa-a25e-6512ef7cef17/SparkJNIPi.so
16/10/30 09:53:58 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:53:58 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:53:58 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:53:58 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:53:58 INFO DAGScheduler: Missing parents: List()
16/10/30 09:53:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:53:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:53:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:53:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33879 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:53:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:53:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:53:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:53:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:53:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:53:59 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817638667
16/10/30 09:53:59 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-fe48443d-90cc-4164-97ad-f539821e3d18/userFiles-1e65401d-8365-4faa-a25e-6512ef7cef17/SparkJNIPi.so
16/10/30 09:53:59 INFO Executor: Fetching spark://192.168.0.17:46587/jars/jni-spark-0.1.jar with timestamp 1477817636785
16/10/30 09:53:59 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46587 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:53:59 INFO Utils: Fetching spark://192.168.0.17:46587/jars/jni-spark-0.1.jar to /tmp/spark-fe48443d-90cc-4164-97ad-f539821e3d18/userFiles-1e65401d-8365-4faa-a25e-6512ef7cef17/fetchFileTemp19881136612160953.tmp
16/10/30 09:53:59 INFO Executor: Adding file:/tmp/spark-fe48443d-90cc-4164-97ad-f539821e3d18/userFiles-1e65401d-8365-4faa-a25e-6512ef7cef17/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:00 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:54:00 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:33879 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:54:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:54:00 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33879 after 2 ms (0 ms spent in bootstraps)
16/10/30 09:54:00 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:33879 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:54:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1015 ms on localhost (1/1)
16/10/30 09:54:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:54:00 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.051 s
16/10/30 09:54:00 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.344391 s
Result: 3.141882 in 1.609 seconds
16/10/30 09:54:00 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:00 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:00 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:00 INFO BlockManager: BlockManager stopped
16/10/30 09:54:00 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:00 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:00 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe48443d-90cc-4164-97ad-f539821e3d18
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:01 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:02 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:02 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:02 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:02 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:02 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:02 INFO Utils: Successfully started service 'sparkDriver' on port 35806.
16/10/30 09:54:02 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:02 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54b944c4-8f0b-461a-8f24-4bdd1f866052
16/10/30 09:54:02 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:02 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:02 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35806/jars/jni-spark-0.1.jar with timestamp 1477817642760
16/10/30 09:54:02 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37347.
16/10/30 09:54:02 INFO NettyBlockTransferService: Server created on 192.168.0.17:37347
16/10/30 09:54:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37347)
16/10/30 09:54:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37347 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37347)
16/10/30 09:54:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37347)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:04 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817644385
16/10/30 09:54:04 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-95e3870b-23cd-457e-91c2-17ad57a5343d/userFiles-d5f54763-ead1-4503-a521-27c765c65ab6/SparkJNIPi.so
16/10/30 09:54:04 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:04 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:04 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:04 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:04 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37347 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:04 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817644385
16/10/30 09:54:04 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-95e3870b-23cd-457e-91c2-17ad57a5343d/userFiles-d5f54763-ead1-4503-a521-27c765c65ab6/SparkJNIPi.so
16/10/30 09:54:05 INFO Executor: Fetching spark://192.168.0.17:35806/jars/jni-spark-0.1.jar with timestamp 1477817642760
16/10/30 09:54:05 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35806 after 27 ms (0 ms spent in bootstraps)
16/10/30 09:54:05 INFO Utils: Fetching spark://192.168.0.17:35806/jars/jni-spark-0.1.jar to /tmp/spark-95e3870b-23cd-457e-91c2-17ad57a5343d/userFiles-d5f54763-ead1-4503-a521-27c765c65ab6/fetchFileTemp8994176218589420644.tmp
16/10/30 09:54:05 INFO Executor: Adding file:/tmp/spark-95e3870b-23cd-457e-91c2-17ad57a5343d/userFiles-d5f54763-ead1-4503-a521-27c765c65ab6/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:05 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:54:05 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:37347 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:54:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:54:05 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37347 after 5 ms (0 ms spent in bootstraps)
16/10/30 09:54:05 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:37347 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:54:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 733 ms on localhost (1/1)
16/10/30 09:54:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:54:05 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.762 s
16/10/30 09:54:05 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.016856 s
Result: 3.142645 in 1.294 seconds
16/10/30 09:54:05 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:05 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:05 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:05 INFO BlockManager: BlockManager stopped
16/10/30 09:54:05 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:05 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:05 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-95e3870b-23cd-457e-91c2-17ad57a5343d
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:06 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:07 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:07 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:07 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:07 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:07 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:07 INFO Utils: Successfully started service 'sparkDriver' on port 38983.
16/10/30 09:54:07 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:07 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d0d58a48-de00-47d1-be99-0e9f92eba03d
16/10/30 09:54:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:07 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:08 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38983/jars/jni-spark-0.1.jar with timestamp 1477817648059
16/10/30 09:54:08 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41115.
16/10/30 09:54:08 INFO NettyBlockTransferService: Server created on 192.168.0.17:41115
16/10/30 09:54:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41115)
16/10/30 09:54:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41115 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41115)
16/10/30 09:54:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41115)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:09 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817649724
16/10/30 09:54:09 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-9177c5cb-60b6-4a55-a5f0-8d8ce34c4c39/userFiles-2c6abb8e-275a-41e3-97bd-952dd38dea6a/SparkJNIPi.so
16/10/30 09:54:09 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:10 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:10 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:10 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:10 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41115 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:10 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817649724
16/10/30 09:54:10 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-9177c5cb-60b6-4a55-a5f0-8d8ce34c4c39/userFiles-2c6abb8e-275a-41e3-97bd-952dd38dea6a/SparkJNIPi.so
16/10/30 09:54:10 INFO Executor: Fetching spark://192.168.0.17:38983/jars/jni-spark-0.1.jar with timestamp 1477817648059
16/10/30 09:54:10 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38983 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:54:10 INFO Utils: Fetching spark://192.168.0.17:38983/jars/jni-spark-0.1.jar to /tmp/spark-9177c5cb-60b6-4a55-a5f0-8d8ce34c4c39/userFiles-2c6abb8e-275a-41e3-97bd-952dd38dea6a/fetchFileTemp3670352276303176379.tmp
16/10/30 09:54:10 INFO Executor: Adding file:/tmp/spark-9177c5cb-60b6-4a55-a5f0-8d8ce34c4c39/userFiles-2c6abb8e-275a-41e3-97bd-952dd38dea6a/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:10 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:54:10 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:41115 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:54:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:54:10 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41115 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:54:11 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:41115 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:54:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 735 ms on localhost (1/1)
16/10/30 09:54:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:54:11 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.772 s
16/10/30 09:54:11 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.048777 s
Result: 3.1425343 in 1.309 seconds
16/10/30 09:54:11 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:11 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:11 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:11 INFO BlockManager: BlockManager stopped
16/10/30 09:54:11 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:11 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:11 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-9177c5cb-60b6-4a55-a5f0-8d8ce34c4c39
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:12 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:12 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:12 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:12 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:12 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:12 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:13 INFO Utils: Successfully started service 'sparkDriver' on port 44299.
16/10/30 09:54:13 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:13 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-47c8863b-670e-465c-a90d-f01ed23322bf
16/10/30 09:54:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:13 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:13 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44299/jars/jni-spark-0.1.jar with timestamp 1477817653424
16/10/30 09:54:13 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:13 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33088.
16/10/30 09:54:13 INFO NettyBlockTransferService: Server created on 192.168.0.17:33088
16/10/30 09:54:13 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33088)
16/10/30 09:54:13 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33088 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33088)
16/10/30 09:54:13 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33088)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:15 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817655171
16/10/30 09:54:15 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d2255e95-4c98-46af-80e0-6f56f5469253/userFiles-4be901bc-383b-4261-9f90-7b2bc01e47d6/SparkJNIPi.so
16/10/30 09:54:15 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:15 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:15 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:15 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:15 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33088 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:15 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817655171
16/10/30 09:54:15 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d2255e95-4c98-46af-80e0-6f56f5469253/userFiles-4be901bc-383b-4261-9f90-7b2bc01e47d6/SparkJNIPi.so
16/10/30 09:54:15 INFO Executor: Fetching spark://192.168.0.17:44299/jars/jni-spark-0.1.jar with timestamp 1477817653424
16/10/30 09:54:15 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44299 after 24 ms (0 ms spent in bootstraps)
16/10/30 09:54:15 INFO Utils: Fetching spark://192.168.0.17:44299/jars/jni-spark-0.1.jar to /tmp/spark-d2255e95-4c98-46af-80e0-6f56f5469253/userFiles-4be901bc-383b-4261-9f90-7b2bc01e47d6/fetchFileTemp2391882662134057069.tmp
16/10/30 09:54:15 INFO Executor: Adding file:/tmp/spark-d2255e95-4c98-46af-80e0-6f56f5469253/userFiles-4be901bc-383b-4261-9f90-7b2bc01e47d6/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:17 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:54:17 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:33088 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:54:17 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:54:17 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33088 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:54:17 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:33088 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:54:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1821 ms on localhost (1/1)
16/10/30 09:54:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:54:17 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.884 s
16/10/30 09:54:17 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.150717 s
Result: 3.1414444 in 2.412 seconds
16/10/30 09:54:17 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:17 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:17 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:17 INFO BlockManager: BlockManager stopped
16/10/30 09:54:17 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:17 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:17 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2255e95-4c98-46af-80e0-6f56f5469253
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:18 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:19 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:19 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:19 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:19 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:19 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:19 INFO Utils: Successfully started service 'sparkDriver' on port 34434.
16/10/30 09:54:19 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:19 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b8ffccec-c77f-4e58-99e5-38732c09d6c2
16/10/30 09:54:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:19 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:20 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34434/jars/jni-spark-0.1.jar with timestamp 1477817660050
16/10/30 09:54:20 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35944.
16/10/30 09:54:20 INFO NettyBlockTransferService: Server created on 192.168.0.17:35944
16/10/30 09:54:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35944)
16/10/30 09:54:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35944 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35944)
16/10/30 09:54:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35944)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:21 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817661716
16/10/30 09:54:21 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-49ff9c31-a530-44e2-af90-8fab2ce4e47c/userFiles-cfbcd499-2642-4581-bd04-a097432186c6/SparkJNIPi.so
16/10/30 09:54:22 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:22 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:22 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:22 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:22 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35944 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:22 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817661716
16/10/30 09:54:22 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-49ff9c31-a530-44e2-af90-8fab2ce4e47c/userFiles-cfbcd499-2642-4581-bd04-a097432186c6/SparkJNIPi.so
16/10/30 09:54:22 INFO Executor: Fetching spark://192.168.0.17:34434/jars/jni-spark-0.1.jar with timestamp 1477817660050
16/10/30 09:54:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34434 after 34 ms (0 ms spent in bootstraps)
16/10/30 09:54:22 INFO Utils: Fetching spark://192.168.0.17:34434/jars/jni-spark-0.1.jar to /tmp/spark-49ff9c31-a530-44e2-af90-8fab2ce4e47c/userFiles-cfbcd499-2642-4581-bd04-a097432186c6/fetchFileTemp5512899042534138765.tmp
16/10/30 09:54:22 INFO Executor: Adding file:/tmp/spark-49ff9c31-a530-44e2-af90-8fab2ce4e47c/userFiles-cfbcd499-2642-4581-bd04-a097432186c6/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:23 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:54:23 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:35944 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:54:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:54:24 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35944 after 20 ms (0 ms spent in bootstraps)
16/10/30 09:54:24 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:35944 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:54:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1822 ms on localhost (1/1)
16/10/30 09:54:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:54:24 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.893 s
16/10/30 09:54:24 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.157278 s
Result: 3.1419587 in 2.455 seconds
16/10/30 09:54:24 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:24 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:24 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:24 INFO BlockManager: BlockManager stopped
16/10/30 09:54:24 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:24 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:24 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-49ff9c31-a530-44e2-af90-8fab2ce4e47c
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:25 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:25 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:25 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:25 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:25 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:25 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:26 INFO Utils: Successfully started service 'sparkDriver' on port 39179.
16/10/30 09:54:26 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:26 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fe9c81f9-7baa-4f0e-9f23-26dc81414073
16/10/30 09:54:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:26 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:26 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39179/jars/jni-spark-0.1.jar with timestamp 1477817666637
16/10/30 09:54:26 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33610.
16/10/30 09:54:26 INFO NettyBlockTransferService: Server created on 192.168.0.17:33610
16/10/30 09:54:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33610)
16/10/30 09:54:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33610 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33610)
16/10/30 09:54:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33610)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:28 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817668319
16/10/30 09:54:28 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-5d041d4e-9807-4d50-b89b-3f6c30301aa7/userFiles-4fa2fec8-f8af-4170-b24d-8cdafcad86a1/SparkJNIPi.so
16/10/30 09:54:28 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:28 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:28 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:28 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33610 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:28 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817668319
16/10/30 09:54:28 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-5d041d4e-9807-4d50-b89b-3f6c30301aa7/userFiles-4fa2fec8-f8af-4170-b24d-8cdafcad86a1/SparkJNIPi.so
16/10/30 09:54:28 INFO Executor: Fetching spark://192.168.0.17:39179/jars/jni-spark-0.1.jar with timestamp 1477817666637
16/10/30 09:54:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39179 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:54:29 INFO Utils: Fetching spark://192.168.0.17:39179/jars/jni-spark-0.1.jar to /tmp/spark-5d041d4e-9807-4d50-b89b-3f6c30301aa7/userFiles-4fa2fec8-f8af-4170-b24d-8cdafcad86a1/fetchFileTemp7973422329727843347.tmp
16/10/30 09:54:29 INFO Executor: Adding file:/tmp/spark-5d041d4e-9807-4d50-b89b-3f6c30301aa7/userFiles-4fa2fec8-f8af-4170-b24d-8cdafcad86a1/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:30 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:54:30 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:33610 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:54:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:54:30 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33610 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:54:30 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:33610 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:54:30 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.886 s
16/10/30 09:54:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1813 ms on localhost (1/1)
16/10/30 09:54:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:54:30 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.157601 s
Result: 3.141649 in 2.471 seconds
16/10/30 09:54:30 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:30 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:30 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:30 INFO BlockManager: BlockManager stopped
16/10/30 09:54:30 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:30 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:30 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-5d041d4e-9807-4d50-b89b-3f6c30301aa7
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:32 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:32 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:32 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:32 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:32 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:32 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:32 INFO Utils: Successfully started service 'sparkDriver' on port 33495.
16/10/30 09:54:32 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:32 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d0737c31-a166-4478-b5b6-8cb024445f31
16/10/30 09:54:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:33 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:33 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33495/jars/jni-spark-0.1.jar with timestamp 1477817673258
16/10/30 09:54:33 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42698.
16/10/30 09:54:33 INFO NettyBlockTransferService: Server created on 192.168.0.17:42698
16/10/30 09:54:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42698)
16/10/30 09:54:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42698 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42698)
16/10/30 09:54:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42698)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:34 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817674965
16/10/30 09:54:34 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d6995325-b8b8-4df0-bf97-7bdc76737373/userFiles-5650aac0-f97e-4d8f-9395-44f2a30f3c60/SparkJNIPi.so
16/10/30 09:54:35 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:35 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:35 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:35 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:35 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42698 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:35 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817674965
16/10/30 09:54:35 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d6995325-b8b8-4df0-bf97-7bdc76737373/userFiles-5650aac0-f97e-4d8f-9395-44f2a30f3c60/SparkJNIPi.so
16/10/30 09:54:35 INFO Executor: Fetching spark://192.168.0.17:33495/jars/jni-spark-0.1.jar with timestamp 1477817673258
16/10/30 09:54:35 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33495 after 25 ms (0 ms spent in bootstraps)
16/10/30 09:54:35 INFO Utils: Fetching spark://192.168.0.17:33495/jars/jni-spark-0.1.jar to /tmp/spark-d6995325-b8b8-4df0-bf97-7bdc76737373/userFiles-5650aac0-f97e-4d8f-9395-44f2a30f3c60/fetchFileTemp3282044553026456128.tmp
16/10/30 09:54:35 INFO Executor: Adding file:/tmp/spark-d6995325-b8b8-4df0-bf97-7bdc76737373/userFiles-5650aac0-f97e-4d8f-9395-44f2a30f3c60/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:37 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:54:37 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:42698 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:54:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 09:54:37 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42698 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:54:37 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:42698 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:54:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1875 ms on localhost (1/1)
16/10/30 09:54:37 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.926 s
16/10/30 09:54:37 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.197455 s
16/10/30 09:54:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result: 3.1417992 in 2.488 seconds
16/10/30 09:54:37 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:37 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:37 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:37 INFO BlockManager: BlockManager stopped
16/10/30 09:54:37 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:37 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:37 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6995325-b8b8-4df0-bf97-7bdc76737373
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:38 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:39 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:39 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:39 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:39 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:39 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:39 INFO Utils: Successfully started service 'sparkDriver' on port 39454.
16/10/30 09:54:39 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:39 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a783734d-8e5f-4980-8fbe-de5df51e7fdb
16/10/30 09:54:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:39 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:39 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39454/jars/jni-spark-0.1.jar with timestamp 1477817679871
16/10/30 09:54:39 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32771.
16/10/30 09:54:39 INFO NettyBlockTransferService: Server created on 192.168.0.17:32771
16/10/30 09:54:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 32771)
16/10/30 09:54:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:32771 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 32771)
16/10/30 09:54:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 32771)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:41 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817681619
16/10/30 09:54:41 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-3a52eefe-8dd4-4408-afaa-bafefbc4fc37/userFiles-966db571-676f-4d5e-938b-651b8ab6647b/SparkJNIPi.so
16/10/30 09:54:41 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:41 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:41 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:41 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:41 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:32771 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:42 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817681619
16/10/30 09:54:42 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-3a52eefe-8dd4-4408-afaa-bafefbc4fc37/userFiles-966db571-676f-4d5e-938b-651b8ab6647b/SparkJNIPi.so
16/10/30 09:54:42 INFO Executor: Fetching spark://192.168.0.17:39454/jars/jni-spark-0.1.jar with timestamp 1477817679871
16/10/30 09:54:42 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39454 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:54:42 INFO Utils: Fetching spark://192.168.0.17:39454/jars/jni-spark-0.1.jar to /tmp/spark-3a52eefe-8dd4-4408-afaa-bafefbc4fc37/userFiles-966db571-676f-4d5e-938b-651b8ab6647b/fetchFileTemp5048302982643594086.tmp
16/10/30 09:54:42 INFO Executor: Adding file:/tmp/spark-3a52eefe-8dd4-4408-afaa-bafefbc4fc37/userFiles-966db571-676f-4d5e-938b-651b8ab6647b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:54:51 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:54:51 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:32771 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:54:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:54:51 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:32771 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:54:52 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:32771 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:54:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10308 ms on localhost (1/1)
16/10/30 09:54:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:54:52 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 10.473 s
16/10/30 09:54:52 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 10.764851 s
Result: 3.1414363 in 11.039 seconds
16/10/30 09:54:52 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:54:52 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:54:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:54:52 INFO MemoryStore: MemoryStore cleared
16/10/30 09:54:52 INFO BlockManager: BlockManager stopped
16/10/30 09:54:52 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:54:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:54:52 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:54:52 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:54:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-3a52eefe-8dd4-4408-afaa-bafefbc4fc37
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:54:54 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:54:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:54:54 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:54:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:54:54 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:54:54 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:54:54 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:54:54 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:54:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:54:54 INFO Utils: Successfully started service 'sparkDriver' on port 46306.
16/10/30 09:54:54 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:54:54 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:54:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a2a741ed-dd48-4fe8-a5da-12e9ff54e15d
16/10/30 09:54:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:54:54 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:54:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:54:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:54:55 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46306/jars/jni-spark-0.1.jar with timestamp 1477817695179
16/10/30 09:54:55 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:54:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37009.
16/10/30 09:54:55 INFO NettyBlockTransferService: Server created on 192.168.0.17:37009
16/10/30 09:54:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37009)
16/10/30 09:54:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37009 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37009)
16/10/30 09:54:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37009)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:54:56 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817696728
16/10/30 09:54:56 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-0bb07fbe-5037-42b4-9b25-3fadbe210492/userFiles-4f2751ef-bcf1-4655-bd25-46db6af4b2d2/SparkJNIPi.so
16/10/30 09:54:56 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:54:57 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:54:57 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:54:57 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:54:57 INFO DAGScheduler: Missing parents: List()
16/10/30 09:54:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:54:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:54:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:54:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37009 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:54:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:54:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:54:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:54:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:54:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:54:57 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817696728
16/10/30 09:54:57 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-0bb07fbe-5037-42b4-9b25-3fadbe210492/userFiles-4f2751ef-bcf1-4655-bd25-46db6af4b2d2/SparkJNIPi.so
16/10/30 09:54:57 INFO Executor: Fetching spark://192.168.0.17:46306/jars/jni-spark-0.1.jar with timestamp 1477817695179
16/10/30 09:54:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46306 after 27 ms (0 ms spent in bootstraps)
16/10/30 09:54:57 INFO Utils: Fetching spark://192.168.0.17:46306/jars/jni-spark-0.1.jar to /tmp/spark-0bb07fbe-5037-42b4-9b25-3fadbe210492/userFiles-4f2751ef-bcf1-4655-bd25-46db6af4b2d2/fetchFileTemp2415276828661154938.tmp
16/10/30 09:54:57 INFO Executor: Adding file:/tmp/spark-0bb07fbe-5037-42b4-9b25-3fadbe210492/userFiles-4f2751ef-bcf1-4655-bd25-46db6af4b2d2/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:55:07 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:55:07 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:37009 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:55:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:55:07 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37009 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:55:07 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:37009 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:55:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10405 ms on localhost (1/1)
16/10/30 09:55:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:55:07 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 10.572 s
16/10/30 09:55:07 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 10.825401 s
Result: 3.1415386 in 11.103 seconds
16/10/30 09:55:07 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:55:07 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:55:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:55:07 INFO MemoryStore: MemoryStore cleared
16/10/30 09:55:07 INFO BlockManager: BlockManager stopped
16/10/30 09:55:07 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:55:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:55:07 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:55:07 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:55:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-0bb07fbe-5037-42b4-9b25-3fadbe210492
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:55:09 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:55:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:55:09 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:55:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:55:09 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:55:09 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:55:09 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:55:09 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:55:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:55:09 INFO Utils: Successfully started service 'sparkDriver' on port 37413.
16/10/30 09:55:09 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:55:09 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:55:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-68404746-6f7e-495d-9579-1c33b53b4d4d
16/10/30 09:55:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:55:10 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:55:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:55:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:55:10 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37413/jars/jni-spark-0.1.jar with timestamp 1477817710367
16/10/30 09:55:10 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:55:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37805.
16/10/30 09:55:10 INFO NettyBlockTransferService: Server created on 192.168.0.17:37805
16/10/30 09:55:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37805)
16/10/30 09:55:10 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37805 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37805)
16/10/30 09:55:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37805)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:55:12 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817712102
16/10/30 09:55:12 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-b0ca9e57-9e68-4ac1-8c3d-ff27ee137dd0/userFiles-4463bf38-c501-4b27-a73c-f988ef3c17e7/SparkJNIPi.so
16/10/30 09:55:12 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:55:12 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:55:12 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:55:12 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:55:12 INFO DAGScheduler: Missing parents: List()
16/10/30 09:55:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:55:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:55:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:55:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37805 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:55:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:55:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:55:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:55:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:55:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:55:12 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817712102
16/10/30 09:55:12 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-b0ca9e57-9e68-4ac1-8c3d-ff27ee137dd0/userFiles-4463bf38-c501-4b27-a73c-f988ef3c17e7/SparkJNIPi.so
16/10/30 09:55:12 INFO Executor: Fetching spark://192.168.0.17:37413/jars/jni-spark-0.1.jar with timestamp 1477817710367
16/10/30 09:55:12 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37413 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:55:12 INFO Utils: Fetching spark://192.168.0.17:37413/jars/jni-spark-0.1.jar to /tmp/spark-b0ca9e57-9e68-4ac1-8c3d-ff27ee137dd0/userFiles-4463bf38-c501-4b27-a73c-f988ef3c17e7/fetchFileTemp2847698663234823531.tmp
16/10/30 09:55:12 INFO Executor: Adding file:/tmp/spark-b0ca9e57-9e68-4ac1-8c3d-ff27ee137dd0/userFiles-4463bf38-c501-4b27-a73c-f988ef3c17e7/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:55:23 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:55:23 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:37805 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:55:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:55:23 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37805 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:55:23 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:37805 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:55:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11057 ms on localhost (1/1)
16/10/30 09:55:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:55:23 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 11.223 s
16/10/30 09:55:23 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 11.474757 s
Result: 3.1415935 in 11.805 seconds
16/10/30 09:55:23 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:55:23 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:55:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:55:23 INFO MemoryStore: MemoryStore cleared
16/10/30 09:55:23 INFO BlockManager: BlockManager stopped
16/10/30 09:55:23 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:55:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:55:23 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:55:23 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:55:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0ca9e57-9e68-4ac1-8c3d-ff27ee137dd0
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:55:25 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:55:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:55:25 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:55:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:55:25 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:55:25 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:55:25 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:55:25 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:55:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:55:25 INFO Utils: Successfully started service 'sparkDriver' on port 38725.
16/10/30 09:55:25 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:55:25 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:55:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5857a147-3381-4402-a014-1cbb90977d73
16/10/30 09:55:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:55:26 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:55:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:55:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:55:26 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38725/jars/jni-spark-0.1.jar with timestamp 1477817726316
16/10/30 09:55:26 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:55:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40123.
16/10/30 09:55:26 INFO NettyBlockTransferService: Server created on 192.168.0.17:40123
16/10/30 09:55:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40123)
16/10/30 09:55:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40123 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40123)
16/10/30 09:55:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40123)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:55:27 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817727789
16/10/30 09:55:27 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-7c16b9c3-7038-46d3-9d87-5a091da59ceb/userFiles-eba14159-d193-4ae2-89bf-dca096ed8586/SparkJNIPi.so
16/10/30 09:55:28 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:55:28 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 1 output partitions
16/10/30 09:55:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:55:28 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:55:28 INFO DAGScheduler: Missing parents: List()
16/10/30 09:55:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:55:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:55:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:55:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40123 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:55:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:55:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:55:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
16/10/30 09:55:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5633 bytes)
16/10/30 09:55:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:55:28 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817727789
16/10/30 09:55:28 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-7c16b9c3-7038-46d3-9d87-5a091da59ceb/userFiles-eba14159-d193-4ae2-89bf-dca096ed8586/SparkJNIPi.so
16/10/30 09:55:28 INFO Executor: Fetching spark://192.168.0.17:38725/jars/jni-spark-0.1.jar with timestamp 1477817726316
16/10/30 09:55:28 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38725 after 36 ms (0 ms spent in bootstraps)
16/10/30 09:55:28 INFO Utils: Fetching spark://192.168.0.17:38725/jars/jni-spark-0.1.jar to /tmp/spark-7c16b9c3-7038-46d3-9d87-5a091da59ceb/userFiles-eba14159-d193-4ae2-89bf-dca096ed8586/fetchFileTemp3137050853080948069.tmp
16/10/30 09:55:28 INFO Executor: Adding file:/tmp/spark-7c16b9c3-7038-46d3-9d87-5a091da59ceb/userFiles-eba14159-d193-4ae2-89bf-dca096ed8586/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 09:55:37 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:55:37 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:40123 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:55:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 09:55:38 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40123 after 2 ms (0 ms spent in bootstraps)
16/10/30 09:55:38 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:40123 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:55:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 10311 ms on localhost (1/1)
16/10/30 09:55:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:55:38 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 10.477 s
16/10/30 09:55:38 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 10.764882 s
Result: 3.1415496 in 11.062 seconds
16/10/30 09:55:38 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:55:38 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:55:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:55:38 INFO MemoryStore: MemoryStore cleared
16/10/30 09:55:38 INFO BlockManager: BlockManager stopped
16/10/30 09:55:38 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:55:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:55:38 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:55:38 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:55:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-7c16b9c3-7038-46d3-9d87-5a091da59ceb
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:55:40 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:55:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:55:40 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:55:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:55:40 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:55:40 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:55:40 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:55:40 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:55:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:55:41 INFO Utils: Successfully started service 'sparkDriver' on port 41686.
16/10/30 09:55:41 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:55:41 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:55:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f617723f-9d79-4738-9975-f9f53332e3e4
16/10/30 09:55:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:55:41 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:55:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:55:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:55:41 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41686/jars/jni-spark-0.1.jar with timestamp 1477817741451
16/10/30 09:55:41 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:55:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41962.
16/10/30 09:55:41 INFO NettyBlockTransferService: Server created on 192.168.0.17:41962
16/10/30 09:55:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41962)
16/10/30 09:55:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41962 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41962)
16/10/30 09:55:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41962)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:55:43 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817743187
16/10/30 09:55:43 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-47d604ff-4252-4a24-a936-d2596b2f0393/userFiles-9d1e2096-c581-4ba5-8373-d71478082574/SparkJNIPi.so
16/10/30 09:55:43 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:55:43 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:55:43 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:55:43 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:55:43 INFO DAGScheduler: Missing parents: List()
16/10/30 09:55:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:55:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:55:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:55:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41962 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:55:43 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:55:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:55:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:55:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:55:43 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:55:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:55:43 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:55:43 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817743187
16/10/30 09:55:43 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-47d604ff-4252-4a24-a936-d2596b2f0393/userFiles-9d1e2096-c581-4ba5-8373-d71478082574/SparkJNIPi.so
16/10/30 09:55:43 INFO Executor: Fetching spark://192.168.0.17:41686/jars/jni-spark-0.1.jar with timestamp 1477817741451
16/10/30 09:55:43 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41686 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:55:43 INFO Utils: Fetching spark://192.168.0.17:41686/jars/jni-spark-0.1.jar to /tmp/spark-47d604ff-4252-4a24-a936-d2596b2f0393/userFiles-9d1e2096-c581-4ba5-8373-d71478082574/fetchFileTemp5355712043148764916.tmp
16/10/30 09:55:43 INFO Executor: Adding file:/tmp/spark-47d604ff-4252-4a24-a936-d2596b2f0393/userFiles-9d1e2096-c581-4ba5-8373-d71478082574/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:55:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:55:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 228 ms on localhost (1/2)
16/10/30 09:55:44 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 09:55:44 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 332 ms on localhost (2/2)
16/10/30 09:55:44 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.438 s
16/10/30 09:55:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:55:44 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.674162 s
Result: 3.1679688 in 0.976 seconds
16/10/30 09:55:44 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:55:44 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:55:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:55:44 INFO MemoryStore: MemoryStore cleared
16/10/30 09:55:44 INFO BlockManager: BlockManager stopped
16/10/30 09:55:44 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:55:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:55:44 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:55:44 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:55:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-47d604ff-4252-4a24-a936-d2596b2f0393
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:55:45 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:55:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:55:45 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:55:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:55:45 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:55:45 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:55:45 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:55:45 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:55:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:55:46 INFO Utils: Successfully started service 'sparkDriver' on port 38842.
16/10/30 09:55:46 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:55:46 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:55:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b6e66bdf-61dd-48de-9cbf-682732e0d632
16/10/30 09:55:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:55:46 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:55:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:55:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:55:46 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38842/jars/jni-spark-0.1.jar with timestamp 1477817746682
16/10/30 09:55:46 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:55:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46496.
16/10/30 09:55:46 INFO NettyBlockTransferService: Server created on 192.168.0.17:46496
16/10/30 09:55:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46496)
16/10/30 09:55:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46496 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46496)
16/10/30 09:55:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46496)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:55:48 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817748493
16/10/30 09:55:48 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-af164990-8e2f-42f8-9ba7-3eb0daab0a99/userFiles-712435bc-23b9-4434-8a6d-cbe7740b2dee/SparkJNIPi.so
16/10/30 09:55:48 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:55:48 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:55:48 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:55:48 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:55:48 INFO DAGScheduler: Missing parents: List()
16/10/30 09:55:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:55:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:55:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:55:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46496 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:55:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:55:49 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:55:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:55:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:55:49 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:55:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:55:49 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:55:49 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817748493
16/10/30 09:55:49 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-af164990-8e2f-42f8-9ba7-3eb0daab0a99/userFiles-712435bc-23b9-4434-8a6d-cbe7740b2dee/SparkJNIPi.so
16/10/30 09:55:49 INFO Executor: Fetching spark://192.168.0.17:38842/jars/jni-spark-0.1.jar with timestamp 1477817746682
16/10/30 09:55:49 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38842 after 31 ms (0 ms spent in bootstraps)
16/10/30 09:55:49 INFO Utils: Fetching spark://192.168.0.17:38842/jars/jni-spark-0.1.jar to /tmp/spark-af164990-8e2f-42f8-9ba7-3eb0daab0a99/userFiles-712435bc-23b9-4434-8a6d-cbe7740b2dee/fetchFileTemp3820906703172346482.tmp
16/10/30 09:55:49 INFO Executor: Adding file:/tmp/spark-af164990-8e2f-42f8-9ba7-3eb0daab0a99/userFiles-712435bc-23b9-4434-8a6d-cbe7740b2dee/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:55:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:55:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 334 ms on localhost (1/2)
16/10/30 09:55:49 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 09:55:49 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 416 ms on localhost (2/2)
16/10/30 09:55:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:55:49 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.476 s
16/10/30 09:55:49 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.835915 s
Result: 3.078125 in 1.158 seconds
16/10/30 09:55:49 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:55:49 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:55:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:55:49 INFO MemoryStore: MemoryStore cleared
16/10/30 09:55:49 INFO BlockManager: BlockManager stopped
16/10/30 09:55:49 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:55:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:55:49 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:55:49 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-af164990-8e2f-42f8-9ba7-3eb0daab0a99
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:55:51 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:55:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:55:51 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:55:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:55:51 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:55:51 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:55:51 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:55:51 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:55:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:55:52 INFO Utils: Successfully started service 'sparkDriver' on port 43041.
16/10/30 09:55:52 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:55:52 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:55:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b46913ac-5676-4b3f-9bde-1d9a8b02f798
16/10/30 09:55:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:55:52 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:55:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:55:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:55:52 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43041/jars/jni-spark-0.1.jar with timestamp 1477817752609
16/10/30 09:55:52 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:55:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43629.
16/10/30 09:55:52 INFO NettyBlockTransferService: Server created on 192.168.0.17:43629
16/10/30 09:55:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43629)
16/10/30 09:55:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43629 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43629)
16/10/30 09:55:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43629)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:55:54 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817754901
16/10/30 09:55:54 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-80d46d60-4e52-4bef-844f-5afc7c8888a1/userFiles-5a97604a-183d-4569-b9db-85f4b799b883/SparkJNIPi.so
16/10/30 09:55:55 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:55:55 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:55:55 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:55:55 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:55:55 INFO DAGScheduler: Missing parents: List()
16/10/30 09:55:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:55:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:55:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:55:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43629 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:55:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:55:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:55:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:55:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:55:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:55:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:55:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:55:55 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817754901
16/10/30 09:55:55 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-80d46d60-4e52-4bef-844f-5afc7c8888a1/userFiles-5a97604a-183d-4569-b9db-85f4b799b883/SparkJNIPi.so
16/10/30 09:55:55 INFO Executor: Fetching spark://192.168.0.17:43041/jars/jni-spark-0.1.jar with timestamp 1477817752609
16/10/30 09:55:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43041 after 44 ms (0 ms spent in bootstraps)
16/10/30 09:55:55 INFO Utils: Fetching spark://192.168.0.17:43041/jars/jni-spark-0.1.jar to /tmp/spark-80d46d60-4e52-4bef-844f-5afc7c8888a1/userFiles-5a97604a-183d-4569-b9db-85f4b799b883/fetchFileTemp389452742977622243.tmp
16/10/30 09:55:55 INFO Executor: Adding file:/tmp/spark-80d46d60-4e52-4bef-844f-5afc7c8888a1/userFiles-5a97604a-183d-4569-b9db-85f4b799b883/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:55:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:55:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 428 ms on localhost (1/2)
16/10/30 09:55:56 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 09:55:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 498 ms on localhost (2/2)
16/10/30 09:55:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:55:56 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.567 s
16/10/30 09:55:56 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.927813 s
Result: 3.1796875 in 1.283 seconds
16/10/30 09:55:56 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:55:56 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:55:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:55:56 INFO MemoryStore: MemoryStore cleared
16/10/30 09:55:56 INFO BlockManager: BlockManager stopped
16/10/30 09:55:56 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:55:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:55:56 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:55:56 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:55:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-80d46d60-4e52-4bef-844f-5afc7c8888a1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:55:57 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:55:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:55:58 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:55:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:55:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:55:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:55:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:55:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:55:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:55:58 INFO Utils: Successfully started service 'sparkDriver' on port 40666.
16/10/30 09:55:58 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:55:58 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:55:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c9b249f5-820f-46c2-8c1d-784c13de6dbe
16/10/30 09:55:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:55:58 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:55:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:55:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:55:59 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40666/jars/jni-spark-0.1.jar with timestamp 1477817759206
16/10/30 09:55:59 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:55:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37038.
16/10/30 09:55:59 INFO NettyBlockTransferService: Server created on 192.168.0.17:37038
16/10/30 09:55:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37038)
16/10/30 09:55:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37038 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37038)
16/10/30 09:55:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37038)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:56:01 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817761342
16/10/30 09:56:01 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-71ffdacb-bb01-488d-b484-91c8774d8160/userFiles-96cdadcb-192f-44d8-9640-ad18c71027df/SparkJNIPi.so
16/10/30 09:56:01 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:56:01 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:56:01 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:56:01 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:56:01 INFO DAGScheduler: Missing parents: List()
16/10/30 09:56:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:56:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:56:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:56:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37038 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:56:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:56:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:56:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:56:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:56:02 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:56:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:56:02 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:56:02 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817761342
16/10/30 09:56:02 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-71ffdacb-bb01-488d-b484-91c8774d8160/userFiles-96cdadcb-192f-44d8-9640-ad18c71027df/SparkJNIPi.so
16/10/30 09:56:02 INFO Executor: Fetching spark://192.168.0.17:40666/jars/jni-spark-0.1.jar with timestamp 1477817759206
16/10/30 09:56:02 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40666 after 106 ms (0 ms spent in bootstraps)
16/10/30 09:56:02 INFO Utils: Fetching spark://192.168.0.17:40666/jars/jni-spark-0.1.jar to /tmp/spark-71ffdacb-bb01-488d-b484-91c8774d8160/userFiles-96cdadcb-192f-44d8-9640-ad18c71027df/fetchFileTemp3403101954303731839.tmp
16/10/30 09:56:02 INFO Executor: Adding file:/tmp/spark-71ffdacb-bb01-488d-b484-91c8774d8160/userFiles-96cdadcb-192f-44d8-9640-ad18c71027df/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:56:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:56:02 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 547 ms on localhost (1/2)
16/10/30 09:56:02 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 09:56:02 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 650 ms on localhost (2/2)
16/10/30 09:56:02 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.705 s
16/10/30 09:56:02 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:56:02 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.055663 s
Result: 3.1484375 in 1.365 seconds
16/10/30 09:56:02 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:56:02 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:56:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:56:02 INFO MemoryStore: MemoryStore cleared
16/10/30 09:56:02 INFO BlockManager: BlockManager stopped
16/10/30 09:56:02 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:56:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:56:02 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:56:02 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:56:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-71ffdacb-bb01-488d-b484-91c8774d8160
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:56:04 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:56:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:56:05 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:56:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:56:05 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:56:05 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:56:05 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:56:05 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:56:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:56:06 INFO Utils: Successfully started service 'sparkDriver' on port 46198.
16/10/30 09:56:06 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:56:06 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:56:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5f95ee2d-8b56-42cd-839c-4d034e077a85
16/10/30 09:56:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:56:06 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:56:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:56:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:56:06 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46198/jars/jni-spark-0.1.jar with timestamp 1477817766641
16/10/30 09:56:06 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:56:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33503.
16/10/30 09:56:06 INFO NettyBlockTransferService: Server created on 192.168.0.17:33503
16/10/30 09:56:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33503)
16/10/30 09:56:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33503 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33503)
16/10/30 09:56:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33503)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:56:09 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817769723
16/10/30 09:56:09 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-897517a2-8b52-4756-b374-8261ae72c771/userFiles-5eb8ac7f-23e3-4de7-a4da-4abc46451609/SparkJNIPi.so
16/10/30 09:56:10 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:56:10 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:56:10 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:56:10 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:56:10 INFO DAGScheduler: Missing parents: List()
16/10/30 09:56:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:56:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:56:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:56:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33503 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:56:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:56:10 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:56:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:56:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:56:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:56:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:56:10 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:56:10 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817769723
16/10/30 09:56:11 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-897517a2-8b52-4756-b374-8261ae72c771/userFiles-5eb8ac7f-23e3-4de7-a4da-4abc46451609/SparkJNIPi.so
16/10/30 09:56:11 INFO Executor: Fetching spark://192.168.0.17:46198/jars/jni-spark-0.1.jar with timestamp 1477817766641
16/10/30 09:56:11 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46198 after 57 ms (0 ms spent in bootstraps)
16/10/30 09:56:11 INFO Utils: Fetching spark://192.168.0.17:46198/jars/jni-spark-0.1.jar to /tmp/spark-897517a2-8b52-4756-b374-8261ae72c771/userFiles-5eb8ac7f-23e3-4de7-a4da-4abc46451609/fetchFileTemp1214820281739556367.tmp
16/10/30 09:56:11 INFO Executor: Adding file:/tmp/spark-897517a2-8b52-4756-b374-8261ae72c771/userFiles-5eb8ac7f-23e3-4de7-a4da-4abc46451609/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:56:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:56:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 547 ms on localhost (1/2)
16/10/30 09:56:11 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 34017 bytes result sent to driver
16/10/30 09:56:11 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 650 ms on localhost (2/2)
16/10/30 09:56:11 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.745 s
16/10/30 09:56:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:56:11 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.269385 s
Result: 3.1386719 in 1.871 seconds
16/10/30 09:56:11 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:56:11 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:56:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:56:11 INFO MemoryStore: MemoryStore cleared
16/10/30 09:56:11 INFO BlockManager: BlockManager stopped
16/10/30 09:56:11 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:56:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:56:11 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:56:11 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:56:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-897517a2-8b52-4756-b374-8261ae72c771
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:56:13 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:56:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:56:14 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:56:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:56:14 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:56:14 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:56:14 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:56:14 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:56:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:56:15 INFO Utils: Successfully started service 'sparkDriver' on port 38958.
16/10/30 09:56:15 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:56:15 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:56:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e4eada0c-a2c0-452d-9f33-c463e3271c52
16/10/30 09:56:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:56:15 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:56:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:56:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:56:16 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38958/jars/jni-spark-0.1.jar with timestamp 1477817776493
16/10/30 09:56:16 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:56:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33009.
16/10/30 09:56:16 INFO NettyBlockTransferService: Server created on 192.168.0.17:33009
16/10/30 09:56:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33009)
16/10/30 09:56:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33009 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33009)
16/10/30 09:56:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33009)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:56:20 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817780621
16/10/30 09:56:20 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c9f8f489-7e41-4c25-abe2-a8a0f6bf9e8e/userFiles-f26fcf89-c3e2-425d-83cb-89ced70b4171/SparkJNIPi.so
16/10/30 09:56:21 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:56:21 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:56:21 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:56:21 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:56:21 INFO DAGScheduler: Missing parents: List()
16/10/30 09:56:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:56:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:56:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:56:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33009 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:56:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:56:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:56:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:56:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:56:22 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:56:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:56:22 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:56:22 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817780621
16/10/30 09:56:22 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c9f8f489-7e41-4c25-abe2-a8a0f6bf9e8e/userFiles-f26fcf89-c3e2-425d-83cb-89ced70b4171/SparkJNIPi.so
16/10/30 09:56:22 INFO Executor: Fetching spark://192.168.0.17:38958/jars/jni-spark-0.1.jar with timestamp 1477817776493
16/10/30 09:56:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38958 after 117 ms (0 ms spent in bootstraps)
16/10/30 09:56:22 INFO Utils: Fetching spark://192.168.0.17:38958/jars/jni-spark-0.1.jar to /tmp/spark-c9f8f489-7e41-4c25-abe2-a8a0f6bf9e8e/userFiles-f26fcf89-c3e2-425d-83cb-89ced70b4171/fetchFileTemp1808902431477053905.tmp
16/10/30 09:56:22 INFO Executor: Adding file:/tmp/spark-c9f8f489-7e41-4c25-abe2-a8a0f6bf9e8e/userFiles-f26fcf89-c3e2-425d-83cb-89ced70b4171/jni-spark-0.1.jar to class loader
16/10/30 09:56:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
16/10/30 09:56:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 686 ms on localhost (1/2)
16/10/30 09:56:23 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 34017 bytes result sent to driver
16/10/30 09:56:23 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 908 ms on localhost (2/2)
16/10/30 09:56:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:56:23 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.107 s
16/10/30 09:56:23 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.757842 s
Result: 3.1523438 in 2.522 seconds
16/10/30 09:56:23 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:56:23 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:56:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:56:23 INFO MemoryStore: MemoryStore cleared
16/10/30 09:56:23 INFO BlockManager: BlockManager stopped
16/10/30 09:56:23 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:56:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:56:23 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:56:23 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:56:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-c9f8f489-7e41-4c25-abe2-a8a0f6bf9e8e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:56:25 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:56:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:56:26 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:56:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:56:27 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:56:27 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:56:27 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:56:27 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:56:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:56:27 INFO Utils: Successfully started service 'sparkDriver' on port 46844.
16/10/30 09:56:27 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:56:27 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:56:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-75339980-baa5-4011-b88a-e333f35fcc12
16/10/30 09:56:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:56:28 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:56:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:56:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:56:28 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46844/jars/jni-spark-0.1.jar with timestamp 1477817788500
16/10/30 09:56:28 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:56:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44662.
16/10/30 09:56:28 INFO NettyBlockTransferService: Server created on 192.168.0.17:44662
16/10/30 09:56:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44662)
16/10/30 09:56:28 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44662 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44662)
16/10/30 09:56:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44662)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:56:32 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817792256
16/10/30 09:56:32 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8a284a06-e4e8-41d9-89bf-f03e5b5432fa/userFiles-63ced071-ecda-4c64-a711-392cb51488db/SparkJNIPi.so
16/10/30 09:56:32 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:56:32 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:56:32 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:56:32 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:56:32 INFO DAGScheduler: Missing parents: List()
16/10/30 09:56:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:56:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:56:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:56:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44662 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:56:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:56:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:56:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:56:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:56:33 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:56:33 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:56:33 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817792256
16/10/30 09:56:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:56:33 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8a284a06-e4e8-41d9-89bf-f03e5b5432fa/userFiles-63ced071-ecda-4c64-a711-392cb51488db/SparkJNIPi.so
16/10/30 09:56:33 INFO Executor: Fetching spark://192.168.0.17:46844/jars/jni-spark-0.1.jar with timestamp 1477817788500
16/10/30 09:56:33 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46844 after 73 ms (0 ms spent in bootstraps)
16/10/30 09:56:33 INFO Utils: Fetching spark://192.168.0.17:46844/jars/jni-spark-0.1.jar to /tmp/spark-8a284a06-e4e8-41d9-89bf-f03e5b5432fa/userFiles-63ced071-ecda-4c64-a711-392cb51488db/fetchFileTemp5509272028414611114.tmp
16/10/30 09:56:34 INFO Executor: Adding file:/tmp/spark-8a284a06-e4e8-41d9-89bf-f03e5b5432fa/userFiles-63ced071-ecda-4c64-a711-392cb51488db/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:56:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:56:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 705 ms on localhost (1/2)
16/10/30 09:56:34 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 34017 bytes result sent to driver
16/10/30 09:56:34 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 876 ms on localhost (2/2)
16/10/30 09:56:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:56:34 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.997 s
16/10/30 09:56:34 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.577841 s
Result: 3.1640625 in 2.232 seconds
16/10/30 09:56:34 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:56:34 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:56:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:56:34 INFO MemoryStore: MemoryStore cleared
16/10/30 09:56:34 INFO BlockManager: BlockManager stopped
16/10/30 09:56:34 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:56:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:56:34 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:56:34 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:56:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a284a06-e4e8-41d9-89bf-f03e5b5432fa
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:56:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:56:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:56:37 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:56:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:56:37 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:56:37 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:56:37 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:56:37 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:56:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:56:37 INFO Utils: Successfully started service 'sparkDriver' on port 44299.
16/10/30 09:56:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:56:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:56:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2332260a-bbd3-4bfd-8bdc-837b153e8feb
16/10/30 09:56:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:56:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:56:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:56:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:56:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44299/jars/jni-spark-0.1.jar with timestamp 1477817797861
16/10/30 09:56:37 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:56:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46673.
16/10/30 09:56:37 INFO NettyBlockTransferService: Server created on 192.168.0.17:46673
16/10/30 09:56:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46673)
16/10/30 09:56:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46673 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46673)
16/10/30 09:56:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46673)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:56:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817799352
16/10/30 09:56:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2ae12f9e-bef0-4eef-b7be-42d3f6599ae3/userFiles-6a30b831-d46c-453f-be25-6510db0d239e/SparkJNIPi.so
16/10/30 09:56:39 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:56:39 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:56:39 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:56:39 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:56:39 INFO DAGScheduler: Missing parents: List()
16/10/30 09:56:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:56:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:56:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:56:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46673 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:56:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:56:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:56:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:56:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:56:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:56:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:56:39 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:56:39 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817799352
16/10/30 09:56:39 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2ae12f9e-bef0-4eef-b7be-42d3f6599ae3/userFiles-6a30b831-d46c-453f-be25-6510db0d239e/SparkJNIPi.so
16/10/30 09:56:40 INFO Executor: Fetching spark://192.168.0.17:44299/jars/jni-spark-0.1.jar with timestamp 1477817797861
16/10/30 09:56:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44299 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:56:40 INFO Utils: Fetching spark://192.168.0.17:44299/jars/jni-spark-0.1.jar to /tmp/spark-2ae12f9e-bef0-4eef-b7be-42d3f6599ae3/userFiles-6a30b831-d46c-453f-be25-6510db0d239e/fetchFileTemp2618667302865631526.tmp
16/10/30 09:56:40 INFO Executor: Adding file:/tmp/spark-2ae12f9e-bef0-4eef-b7be-42d3f6599ae3/userFiles-6a30b831-d46c-453f-be25-6510db0d239e/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:56:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:56:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 234 ms on localhost (1/2)
16/10/30 09:56:40 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 34017 bytes result sent to driver
16/10/30 09:56:40 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 327 ms on localhost (2/2)
16/10/30 09:56:40 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.374 s
16/10/30 09:56:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:56:40 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.650408 s
Result: 3.1464844 in 0.916 seconds
16/10/30 09:56:40 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:56:40 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:56:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:56:40 INFO MemoryStore: MemoryStore cleared
16/10/30 09:56:40 INFO BlockManager: BlockManager stopped
16/10/30 09:56:40 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:56:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:56:40 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:56:40 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:56:40 INFO ShutdownHookManager: Deleting directory /tmp/spark-2ae12f9e-bef0-4eef-b7be-42d3f6599ae3
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:56:41 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:56:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:56:42 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:56:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:56:42 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:56:42 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:56:42 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:56:42 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:56:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:56:42 INFO Utils: Successfully started service 'sparkDriver' on port 34693.
16/10/30 09:56:42 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:56:42 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:56:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-57592603-5486-4bef-aabf-2e6e7a213a44
16/10/30 09:56:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:56:42 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:56:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:56:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:56:43 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34693/jars/jni-spark-0.1.jar with timestamp 1477817803054
16/10/30 09:56:43 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:56:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35879.
16/10/30 09:56:43 INFO NettyBlockTransferService: Server created on 192.168.0.17:35879
16/10/30 09:56:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35879)
16/10/30 09:56:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35879 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35879)
16/10/30 09:56:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35879)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:56:44 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817804895
16/10/30 09:56:44 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-3dae8211-1d06-40ce-b3b4-129dd601eed9/userFiles-01924477-add9-404d-836d-6487d4e0e6e0/SparkJNIPi.so
16/10/30 09:56:45 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:56:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:56:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:56:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:56:45 INFO DAGScheduler: Missing parents: List()
16/10/30 09:56:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:56:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:56:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:56:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35879 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:56:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:56:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:56:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:56:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:56:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:56:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:56:45 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:56:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817804895
16/10/30 09:56:45 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-3dae8211-1d06-40ce-b3b4-129dd601eed9/userFiles-01924477-add9-404d-836d-6487d4e0e6e0/SparkJNIPi.so
16/10/30 09:56:45 INFO Executor: Fetching spark://192.168.0.17:34693/jars/jni-spark-0.1.jar with timestamp 1477817803054
16/10/30 09:56:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34693 after 40 ms (0 ms spent in bootstraps)
16/10/30 09:56:45 INFO Utils: Fetching spark://192.168.0.17:34693/jars/jni-spark-0.1.jar to /tmp/spark-3dae8211-1d06-40ce-b3b4-129dd601eed9/userFiles-01924477-add9-404d-836d-6487d4e0e6e0/fetchFileTemp5445069181458261639.tmp
16/10/30 09:56:45 INFO Executor: Adding file:/tmp/spark-3dae8211-1d06-40ce-b3b4-129dd601eed9/userFiles-01924477-add9-404d-836d-6487d4e0e6e0/jni-spark-0.1.jar to class loader
16/10/30 09:56:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
16/10/30 09:56:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 377 ms on localhost (1/2)
16/10/30 09:56:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 264513 bytes result sent to driver
16/10/30 09:56:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 523 ms on localhost (2/2)
16/10/30 09:56:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:56:46 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.591 s
16/10/30 09:56:46 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.936389 s
Result: 3.138733 in 1.209 seconds
16/10/30 09:56:46 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:56:46 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:56:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:56:46 INFO MemoryStore: MemoryStore cleared
16/10/30 09:56:46 INFO BlockManager: BlockManager stopped
16/10/30 09:56:46 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:56:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:56:46 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:56:46 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:56:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-3dae8211-1d06-40ce-b3b4-129dd601eed9
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:56:48 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:56:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:56:49 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:56:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:56:49 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:56:49 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:56:49 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:56:49 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:56:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:56:49 INFO Utils: Successfully started service 'sparkDriver' on port 44422.
16/10/30 09:56:49 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:56:49 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:56:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-678868fd-e4f8-4ca9-a96a-92d9d77b4904
16/10/30 09:56:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:56:50 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:56:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:56:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:56:50 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44422/jars/jni-spark-0.1.jar with timestamp 1477817810647
16/10/30 09:56:50 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:56:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46183.
16/10/30 09:56:50 INFO NettyBlockTransferService: Server created on 192.168.0.17:46183
16/10/30 09:56:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46183)
16/10/30 09:56:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46183 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46183)
16/10/30 09:56:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46183)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:56:53 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817813185
16/10/30 09:56:53 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c7002a93-e170-4427-92b0-1cef2f71a5ef/userFiles-b13f7190-d356-4d33-b4be-3886bf1f8f94/SparkJNIPi.so
16/10/30 09:56:53 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:56:53 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:56:53 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:56:53 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:56:53 INFO DAGScheduler: Missing parents: List()
16/10/30 09:56:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:56:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:56:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:56:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46183 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:56:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:56:54 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:56:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:56:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:56:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:56:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:56:54 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:56:54 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817813185
16/10/30 09:56:54 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c7002a93-e170-4427-92b0-1cef2f71a5ef/userFiles-b13f7190-d356-4d33-b4be-3886bf1f8f94/SparkJNIPi.so
16/10/30 09:56:54 INFO Executor: Fetching spark://192.168.0.17:44422/jars/jni-spark-0.1.jar with timestamp 1477817810647
16/10/30 09:56:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44422 after 61 ms (0 ms spent in bootstraps)
16/10/30 09:56:55 INFO Utils: Fetching spark://192.168.0.17:44422/jars/jni-spark-0.1.jar to /tmp/spark-c7002a93-e170-4427-92b0-1cef2f71a5ef/userFiles-b13f7190-d356-4d33-b4be-3886bf1f8f94/fetchFileTemp1775144883782364360.tmp
16/10/30 09:56:55 INFO Executor: Adding file:/tmp/spark-c7002a93-e170-4427-92b0-1cef2f71a5ef/userFiles-b13f7190-d356-4d33-b4be-3886bf1f8f94/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:56:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:56:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 726 ms on localhost (1/2)
16/10/30 09:56:55 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 264513 bytes result sent to driver
16/10/30 09:56:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 825 ms on localhost (2/2)
16/10/30 09:56:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:56:55 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.982 s
16/10/30 09:56:55 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.838037 s
Result: 3.1325073 in 2.4 seconds
16/10/30 09:56:55 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:56:55 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:56:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:56:55 INFO MemoryStore: MemoryStore cleared
16/10/30 09:56:55 INFO BlockManager: BlockManager stopped
16/10/30 09:56:55 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:56:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:56:55 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:56:55 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:56:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7002a93-e170-4427-92b0-1cef2f71a5ef
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:56:57 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:56:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:56:57 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:56:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:56:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:56:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:56:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:56:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:56:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:56:58 INFO Utils: Successfully started service 'sparkDriver' on port 41780.
16/10/30 09:56:58 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:56:58 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:56:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-59ad17c6-ca8b-430e-af80-48b75538a450
16/10/30 09:56:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:56:58 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:56:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:56:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:56:59 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41780/jars/jni-spark-0.1.jar with timestamp 1477817819334
16/10/30 09:56:59 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:56:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43994.
16/10/30 09:56:59 INFO NettyBlockTransferService: Server created on 192.168.0.17:43994
16/10/30 09:56:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43994)
16/10/30 09:56:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43994 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43994)
16/10/30 09:56:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43994)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:02 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817822024
16/10/30 09:57:02 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-61fb3e6d-0efd-4ab1-b9e8-ca3074bf78a0/userFiles-dc28f0bb-ddd6-4583-a1cc-2e7a21a2db1b/SparkJNIPi.so
16/10/30 09:57:02 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:02 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:02 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:02 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:02 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43994 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:03 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817822024
16/10/30 09:57:03 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-61fb3e6d-0efd-4ab1-b9e8-ca3074bf78a0/userFiles-dc28f0bb-ddd6-4583-a1cc-2e7a21a2db1b/SparkJNIPi.so
16/10/30 09:57:03 INFO Executor: Fetching spark://192.168.0.17:41780/jars/jni-spark-0.1.jar with timestamp 1477817819334
16/10/30 09:57:03 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41780 after 65 ms (0 ms spent in bootstraps)
16/10/30 09:57:03 INFO Utils: Fetching spark://192.168.0.17:41780/jars/jni-spark-0.1.jar to /tmp/spark-61fb3e6d-0efd-4ab1-b9e8-ca3074bf78a0/userFiles-dc28f0bb-ddd6-4583-a1cc-2e7a21a2db1b/fetchFileTemp1256596479909566454.tmp
16/10/30 09:57:03 INFO Executor: Adding file:/tmp/spark-61fb3e6d-0efd-4ab1-b9e8-ca3074bf78a0/userFiles-dc28f0bb-ddd6-4583-a1cc-2e7a21a2db1b/jni-spark-0.1.jar to class loader
16/10/30 09:57:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
16/10/30 09:57:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 460 ms on localhost (1/2)
16/10/30 09:57:03 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 264513 bytes result sent to driver
16/10/30 09:57:03 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 601 ms on localhost (2/2)
16/10/30 09:57:03 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.686 s
16/10/30 09:57:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:03 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.182974 s
Result: 3.1271973 in 1.725 seconds
16/10/30 09:57:03 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:03 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:03 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:03 INFO BlockManager: BlockManager stopped
16/10/30 09:57:03 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:03 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:03 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-61fb3e6d-0efd-4ab1-b9e8-ca3074bf78a0
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:05 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:05 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:05 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:05 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:05 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:05 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:05 INFO Utils: Successfully started service 'sparkDriver' on port 35509.
16/10/30 09:57:06 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:06 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-34b846e1-c3b8-43cd-9ad0-7e65c3a3be07
16/10/30 09:57:06 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:06 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:06 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35509/jars/jni-spark-0.1.jar with timestamp 1477817826635
16/10/30 09:57:06 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32928.
16/10/30 09:57:06 INFO NettyBlockTransferService: Server created on 192.168.0.17:32928
16/10/30 09:57:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 32928)
16/10/30 09:57:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:32928 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 32928)
16/10/30 09:57:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 32928)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:09 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817829006
16/10/30 09:57:09 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-e17a169e-b6b2-4466-8978-cd6baff049a5/userFiles-8b067a93-9185-4d83-8572-687116b96e2d/SparkJNIPi.so
16/10/30 09:57:09 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:09 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:09 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:09 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:09 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:32928 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:09 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:09 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:09 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817829006
16/10/30 09:57:09 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-e17a169e-b6b2-4466-8978-cd6baff049a5/userFiles-8b067a93-9185-4d83-8572-687116b96e2d/SparkJNIPi.so
16/10/30 09:57:09 INFO Executor: Fetching spark://192.168.0.17:35509/jars/jni-spark-0.1.jar with timestamp 1477817826635
16/10/30 09:57:09 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35509 after 24 ms (0 ms spent in bootstraps)
16/10/30 09:57:09 INFO Utils: Fetching spark://192.168.0.17:35509/jars/jni-spark-0.1.jar to /tmp/spark-e17a169e-b6b2-4466-8978-cd6baff049a5/userFiles-8b067a93-9185-4d83-8572-687116b96e2d/fetchFileTemp1009694764184301110.tmp
16/10/30 09:57:09 INFO Executor: Adding file:/tmp/spark-e17a169e-b6b2-4466-8978-cd6baff049a5/userFiles-8b067a93-9185-4d83-8572-687116b96e2d/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 09:57:09 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:57:09 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 251 ms on localhost (1/2)
16/10/30 09:57:09 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 264513 bytes result sent to driver
16/10/30 09:57:09 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 361 ms on localhost (2/2)
16/10/30 09:57:09 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.406 s
16/10/30 09:57:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:09 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.664568 s
Result: 3.1292725 in 0.968 seconds
16/10/30 09:57:09 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:09 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:10 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:10 INFO BlockManager: BlockManager stopped
16/10/30 09:57:10 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:10 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:10 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-e17a169e-b6b2-4466-8978-cd6baff049a5
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:11 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:11 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:11 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:11 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:11 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:11 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:11 INFO Utils: Successfully started service 'sparkDriver' on port 35671.
16/10/30 09:57:11 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:11 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-83a5fb44-06db-49ee-b32e-268bfb88c965
16/10/30 09:57:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:12 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:12 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35671/jars/jni-spark-0.1.jar with timestamp 1477817832322
16/10/30 09:57:12 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46717.
16/10/30 09:57:12 INFO NettyBlockTransferService: Server created on 192.168.0.17:46717
16/10/30 09:57:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46717)
16/10/30 09:57:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46717 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46717)
16/10/30 09:57:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46717)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:14 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817834215
16/10/30 09:57:14 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2d609131-cbbd-46ae-9e7a-25b21dc9c19e/userFiles-3536ceb1-ba47-4128-a2f9-3591d1548470/SparkJNIPi.so
16/10/30 09:57:14 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:14 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:14 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:14 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:14 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46717 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:14 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:14 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817834215
16/10/30 09:57:14 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2d609131-cbbd-46ae-9e7a-25b21dc9c19e/userFiles-3536ceb1-ba47-4128-a2f9-3591d1548470/SparkJNIPi.so
16/10/30 09:57:14 INFO Executor: Fetching spark://192.168.0.17:35671/jars/jni-spark-0.1.jar with timestamp 1477817832322
16/10/30 09:57:14 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35671 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:57:14 INFO Utils: Fetching spark://192.168.0.17:35671/jars/jni-spark-0.1.jar to /tmp/spark-2d609131-cbbd-46ae-9e7a-25b21dc9c19e/userFiles-3536ceb1-ba47-4128-a2f9-3591d1548470/fetchFileTemp6354112097150236058.tmp
16/10/30 09:57:14 INFO Executor: Adding file:/tmp/spark-2d609131-cbbd-46ae-9e7a-25b21dc9c19e/userFiles-3536ceb1-ba47-4128-a2f9-3591d1548470/jni-spark-0.1.jar to class loader
16/10/30 09:57:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:57:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 242 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:57:15 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:57:15 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:46717 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:57:15 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 09:57:15 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46717 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:57:15 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:46717 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:57:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 546 ms on localhost (2/2)
16/10/30 09:57:15 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.600 s
16/10/30 09:57:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:15 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.878848 s
Result: 3.1434937 in 1.115 seconds
16/10/30 09:57:15 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:15 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:15 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:15 INFO BlockManager: BlockManager stopped
16/10/30 09:57:15 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:15 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:15 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d609131-cbbd-46ae-9e7a-25b21dc9c19e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:16 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:17 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:17 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:17 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:17 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:17 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:17 INFO Utils: Successfully started service 'sparkDriver' on port 39674.
16/10/30 09:57:17 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:17 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e2b4697b-2857-4591-8540-31a135de7008
16/10/30 09:57:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:17 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:17 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39674/jars/jni-spark-0.1.jar with timestamp 1477817837772
16/10/30 09:57:17 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44920.
16/10/30 09:57:17 INFO NettyBlockTransferService: Server created on 192.168.0.17:44920
16/10/30 09:57:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44920)
16/10/30 09:57:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44920 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44920)
16/10/30 09:57:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44920)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:19 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817839776
16/10/30 09:57:19 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-f1b09955-3258-4f57-8347-2f99067653e6/userFiles-b5606ad3-166c-46c4-ac80-3996d402247c/SparkJNIPi.so
16/10/30 09:57:20 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:20 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:20 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:20 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:20 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44920 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:20 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:20 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:20 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817839776
16/10/30 09:57:20 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-f1b09955-3258-4f57-8347-2f99067653e6/userFiles-b5606ad3-166c-46c4-ac80-3996d402247c/SparkJNIPi.so
16/10/30 09:57:20 INFO Executor: Fetching spark://192.168.0.17:39674/jars/jni-spark-0.1.jar with timestamp 1477817837772
16/10/30 09:57:20 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39674 after 32 ms (0 ms spent in bootstraps)
16/10/30 09:57:20 INFO Utils: Fetching spark://192.168.0.17:39674/jars/jni-spark-0.1.jar to /tmp/spark-f1b09955-3258-4f57-8347-2f99067653e6/userFiles-b5606ad3-166c-46c4-ac80-3996d402247c/fetchFileTemp1298612535018955811.tmp
16/10/30 09:57:20 INFO Executor: Adding file:/tmp/spark-f1b09955-3258-4f57-8347-2f99067653e6/userFiles-b5606ad3-166c-46c4-ac80-3996d402247c/jni-spark-0.1.jar to class loader
16/10/30 09:57:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
16/10/30 09:57:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 330 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:57:20 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:57:20 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:44920 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:57:20 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 09:57:20 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44920 after 2 ms (0 ms spent in bootstraps)
16/10/30 09:57:21 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:44920 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:57:21 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 508 ms on localhost (2/2)
16/10/30 09:57:21 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.703 s
16/10/30 09:57:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:21 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.010686 s
Result: 3.1441574 in 1.307 seconds
16/10/30 09:57:21 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:21 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:21 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:21 INFO BlockManager: BlockManager stopped
16/10/30 09:57:21 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:21 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:21 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-f1b09955-3258-4f57-8347-2f99067653e6
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:22 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:22 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:22 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:22 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:22 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:22 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:23 INFO Utils: Successfully started service 'sparkDriver' on port 37661.
16/10/30 09:57:23 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:23 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-65c3880f-9148-4f1f-8ec4-48b4a422870d
16/10/30 09:57:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:23 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:23 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37661/jars/jni-spark-0.1.jar with timestamp 1477817843486
16/10/30 09:57:23 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44429.
16/10/30 09:57:23 INFO NettyBlockTransferService: Server created on 192.168.0.17:44429
16/10/30 09:57:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44429)
16/10/30 09:57:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44429 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44429)
16/10/30 09:57:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44429)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:25 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817845487
16/10/30 09:57:25 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-cbdaab1d-81f5-4925-8374-a83aec381e78/userFiles-f2cf2523-ac15-4c57-9544-3fbad6754e63/SparkJNIPi.so
16/10/30 09:57:25 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:25 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:25 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:25 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:25 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44429 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:26 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:26 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:26 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817845487
16/10/30 09:57:26 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-cbdaab1d-81f5-4925-8374-a83aec381e78/userFiles-f2cf2523-ac15-4c57-9544-3fbad6754e63/SparkJNIPi.so
16/10/30 09:57:26 INFO Executor: Fetching spark://192.168.0.17:37661/jars/jni-spark-0.1.jar with timestamp 1477817843486
16/10/30 09:57:26 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37661 after 33 ms (0 ms spent in bootstraps)
16/10/30 09:57:26 INFO Utils: Fetching spark://192.168.0.17:37661/jars/jni-spark-0.1.jar to /tmp/spark-cbdaab1d-81f5-4925-8374-a83aec381e78/userFiles-f2cf2523-ac15-4c57-9544-3fbad6754e63/fetchFileTemp3285703359587947396.tmp
16/10/30 09:57:26 INFO Executor: Adding file:/tmp/spark-cbdaab1d-81f5-4925-8374-a83aec381e78/userFiles-f2cf2523-ac15-4c57-9544-3fbad6754e63/jni-spark-0.1.jar to class loader
16/10/30 09:57:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
16/10/30 09:57:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 282 ms on localhost (1/2)
16/10/30 09:57:26 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:57:26 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:44429 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:57:26 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 09:57:26 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44429 after 6 ms (0 ms spent in bootstraps)
16/10/30 09:57:26 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:44429 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:57:26 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.610 s
16/10/30 09:57:26 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 544 ms on localhost (2/2)
16/10/30 09:57:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:26 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.869431 s
Result: 3.1463318 in 1.311 seconds
16/10/30 09:57:26 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:26 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:26 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:26 INFO BlockManager: BlockManager stopped
16/10/30 09:57:26 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:26 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:26 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-cbdaab1d-81f5-4925-8374-a83aec381e78
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:28 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:28 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:28 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:28 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:28 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:28 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:28 INFO Utils: Successfully started service 'sparkDriver' on port 43225.
16/10/30 09:57:28 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:28 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7dae736a-8d27-42f3-a442-b7717e479bf4
16/10/30 09:57:28 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:28 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:29 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43225/jars/jni-spark-0.1.jar with timestamp 1477817849211
16/10/30 09:57:29 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42635.
16/10/30 09:57:29 INFO NettyBlockTransferService: Server created on 192.168.0.17:42635
16/10/30 09:57:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42635)
16/10/30 09:57:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42635 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42635)
16/10/30 09:57:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42635)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:30 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817850768
16/10/30 09:57:30 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-27c5bc76-df9e-4133-b271-fb55160002cb/userFiles-b9b25ece-bd9e-4e92-88f9-7056fce16765/SparkJNIPi.so
16/10/30 09:57:31 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:31 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:31 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:31 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:31 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42635 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:31 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:31 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:31 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817850768
16/10/30 09:57:31 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-27c5bc76-df9e-4133-b271-fb55160002cb/userFiles-b9b25ece-bd9e-4e92-88f9-7056fce16765/SparkJNIPi.so
16/10/30 09:57:31 INFO Executor: Fetching spark://192.168.0.17:43225/jars/jni-spark-0.1.jar with timestamp 1477817849211
16/10/30 09:57:31 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43225 after 33 ms (0 ms spent in bootstraps)
16/10/30 09:57:31 INFO Utils: Fetching spark://192.168.0.17:43225/jars/jni-spark-0.1.jar to /tmp/spark-27c5bc76-df9e-4133-b271-fb55160002cb/userFiles-b9b25ece-bd9e-4e92-88f9-7056fce16765/fetchFileTemp2512032074214385971.tmp
16/10/30 09:57:31 INFO Executor: Adding file:/tmp/spark-27c5bc76-df9e-4133-b271-fb55160002cb/userFiles-b9b25ece-bd9e-4e92-88f9-7056fce16765/jni-spark-0.1.jar to class loader
16/10/30 09:57:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
Calling method randToSum
16/10/30 09:57:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 253 ms on localhost (1/2)
16/10/30 09:57:31 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:57:31 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:42635 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:57:31 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 09:57:31 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42635 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:57:31 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:42635 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:57:31 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 427 ms on localhost (2/2)
16/10/30 09:57:31 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.579 s
16/10/30 09:57:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:31 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.830807 s
Result: 3.1438217 in 1.1 seconds
16/10/30 09:57:31 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:31 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:31 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:31 INFO BlockManager: BlockManager stopped
16/10/30 09:57:31 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:31 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:31 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-27c5bc76-df9e-4133-b271-fb55160002cb
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:33 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:33 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:33 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:33 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:33 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:33 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:34 INFO Utils: Successfully started service 'sparkDriver' on port 45358.
16/10/30 09:57:34 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:34 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8ba903c8-b484-4b47-ab4b-da5ae750603e
16/10/30 09:57:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:34 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:34 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45358/jars/jni-spark-0.1.jar with timestamp 1477817854596
16/10/30 09:57:34 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44752.
16/10/30 09:57:34 INFO NettyBlockTransferService: Server created on 192.168.0.17:44752
16/10/30 09:57:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44752)
16/10/30 09:57:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44752 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44752)
16/10/30 09:57:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44752)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:36 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817856429
16/10/30 09:57:36 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-9c30cb3d-7eb3-454b-bb0e-6b9768a87ff3/userFiles-df6bc232-4867-43c5-bb86-5d8ff9dcf904/SparkJNIPi.so
16/10/30 09:57:36 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:36 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:36 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:36 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:36 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44752 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:37 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:37 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:37 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817856429
16/10/30 09:57:37 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-9c30cb3d-7eb3-454b-bb0e-6b9768a87ff3/userFiles-df6bc232-4867-43c5-bb86-5d8ff9dcf904/SparkJNIPi.so
16/10/30 09:57:37 INFO Executor: Fetching spark://192.168.0.17:45358/jars/jni-spark-0.1.jar with timestamp 1477817854596
16/10/30 09:57:37 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45358 after 23 ms (0 ms spent in bootstraps)
16/10/30 09:57:37 INFO Utils: Fetching spark://192.168.0.17:45358/jars/jni-spark-0.1.jar to /tmp/spark-9c30cb3d-7eb3-454b-bb0e-6b9768a87ff3/userFiles-df6bc232-4867-43c5-bb86-5d8ff9dcf904/fetchFileTemp1496436819511853829.tmp
16/10/30 09:57:37 INFO Executor: Adding file:/tmp/spark-9c30cb3d-7eb3-454b-bb0e-6b9768a87ff3/userFiles-df6bc232-4867-43c5-bb86-5d8ff9dcf904/jni-spark-0.1.jar to class loader
16/10/30 09:57:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
16/10/30 09:57:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 469 ms on localhost (1/2)
16/10/30 09:57:37 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:57:37 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:44752 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:57:37 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860225 bytes result sent via BlockManager)
16/10/30 09:57:37 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44752 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:57:37 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:44752 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:57:37 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 860 ms on localhost (2/2)
16/10/30 09:57:37 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.948 s
16/10/30 09:57:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:37 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.203601 s
Result: 3.1414747 in 1.525 seconds
16/10/30 09:57:37 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:37 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:37 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:37 INFO BlockManager: BlockManager stopped
16/10/30 09:57:37 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:37 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:37 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-9c30cb3d-7eb3-454b-bb0e-6b9768a87ff3
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:39 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:39 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:39 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:39 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:39 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:39 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:40 INFO Utils: Successfully started service 'sparkDriver' on port 41741.
16/10/30 09:57:40 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:40 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-47b124f6-01cd-4227-a643-42c5b77b2a04
16/10/30 09:57:40 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:40 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:40 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41741/jars/jni-spark-0.1.jar with timestamp 1477817860525
16/10/30 09:57:40 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39982.
16/10/30 09:57:40 INFO NettyBlockTransferService: Server created on 192.168.0.17:39982
16/10/30 09:57:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39982)
16/10/30 09:57:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39982 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39982)
16/10/30 09:57:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39982)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:42 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817862575
16/10/30 09:57:42 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-adef63b2-acb4-4aa4-bc19-a17a3d33e68e/userFiles-8610ee0f-20c4-4ca0-b4ca-f24b2ff7eb7e/SparkJNIPi.so
16/10/30 09:57:43 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:43 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:43 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:43 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:43 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39982 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:43 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:43 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:43 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:43 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817862575
16/10/30 09:57:43 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-adef63b2-acb4-4aa4-bc19-a17a3d33e68e/userFiles-8610ee0f-20c4-4ca0-b4ca-f24b2ff7eb7e/SparkJNIPi.so
16/10/30 09:57:43 INFO Executor: Fetching spark://192.168.0.17:41741/jars/jni-spark-0.1.jar with timestamp 1477817860525
16/10/30 09:57:43 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41741 after 35 ms (0 ms spent in bootstraps)
16/10/30 09:57:43 INFO Utils: Fetching spark://192.168.0.17:41741/jars/jni-spark-0.1.jar to /tmp/spark-adef63b2-acb4-4aa4-bc19-a17a3d33e68e/userFiles-8610ee0f-20c4-4ca0-b4ca-f24b2ff7eb7e/fetchFileTemp5335203833631142120.tmp
16/10/30 09:57:43 INFO Executor: Adding file:/tmp/spark-adef63b2-acb4-4aa4-bc19-a17a3d33e68e/userFiles-8610ee0f-20c4-4ca0-b4ca-f24b2ff7eb7e/jni-spark-0.1.jar to class loader
16/10/30 09:57:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:57:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 258 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:57:43 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:57:43 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:39982 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:57:43 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 09:57:43 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39982 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:57:44 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:39982 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:57:44 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 832 ms on localhost (2/2)
16/10/30 09:57:44 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.922 s
16/10/30 09:57:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:44 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.191862 s
Result: 3.1423988 in 1.631 seconds
16/10/30 09:57:44 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:44 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:44 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:44 INFO BlockManager: BlockManager stopped
16/10/30 09:57:44 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:44 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:44 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-adef63b2-acb4-4aa4-bc19-a17a3d33e68e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:45 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:45 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:45 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:45 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:45 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:45 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:46 INFO Utils: Successfully started service 'sparkDriver' on port 35781.
16/10/30 09:57:46 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:46 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-72e9b4c5-f9b6-4e78-9da3-2c97796d885f
16/10/30 09:57:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:46 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:46 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35781/jars/jni-spark-0.1.jar with timestamp 1477817866625
16/10/30 09:57:46 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37539.
16/10/30 09:57:46 INFO NettyBlockTransferService: Server created on 192.168.0.17:37539
16/10/30 09:57:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37539)
16/10/30 09:57:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37539 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37539)
16/10/30 09:57:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37539)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:48 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817868293
16/10/30 09:57:48 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-81391c6e-4ac3-4f6d-b66d-b825693456a6/userFiles-555729b7-0e3d-410d-8f1c-801081e3491e/SparkJNIPi.so
16/10/30 09:57:48 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:48 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:48 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:48 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:48 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37539 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:48 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:48 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817868293
16/10/30 09:57:48 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-81391c6e-4ac3-4f6d-b66d-b825693456a6/userFiles-555729b7-0e3d-410d-8f1c-801081e3491e/SparkJNIPi.so
16/10/30 09:57:48 INFO Executor: Fetching spark://192.168.0.17:35781/jars/jni-spark-0.1.jar with timestamp 1477817866625
16/10/30 09:57:48 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35781 after 27 ms (0 ms spent in bootstraps)
16/10/30 09:57:48 INFO Utils: Fetching spark://192.168.0.17:35781/jars/jni-spark-0.1.jar to /tmp/spark-81391c6e-4ac3-4f6d-b66d-b825693456a6/userFiles-555729b7-0e3d-410d-8f1c-801081e3491e/fetchFileTemp6696545861460024867.tmp
16/10/30 09:57:49 INFO Executor: Adding file:/tmp/spark-81391c6e-4ac3-4f6d-b66d-b825693456a6/userFiles-555729b7-0e3d-410d-8f1c-801081e3491e/jni-spark-0.1.jar to class loader
16/10/30 09:57:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:57:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 242 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:57:49 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:57:49 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:37539 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:57:49 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 09:57:49 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37539 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:57:49 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:37539 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:57:49 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 873 ms on localhost (2/2)
16/10/30 09:57:49 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.959 s
16/10/30 09:57:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:49 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.210376 s
Result: 3.142353 in 1.484 seconds
16/10/30 09:57:49 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:49 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:49 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:49 INFO BlockManager: BlockManager stopped
16/10/30 09:57:49 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:49 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:49 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-81391c6e-4ac3-4f6d-b66d-b825693456a6
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:51 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:57:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:57:51 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:57:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:57:51 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:57:51 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:57:51 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:57:51 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:57:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:57:52 INFO Utils: Successfully started service 'sparkDriver' on port 45247.
16/10/30 09:57:52 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:57:52 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:57:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d9dbb96e-f8d9-4b6b-9d38-f88d619689fe
16/10/30 09:57:52 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:57:52 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:57:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:57:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:57:52 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45247/jars/jni-spark-0.1.jar with timestamp 1477817872533
16/10/30 09:57:52 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:57:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43778.
16/10/30 09:57:52 INFO NettyBlockTransferService: Server created on 192.168.0.17:43778
16/10/30 09:57:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43778)
16/10/30 09:57:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43778 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43778)
16/10/30 09:57:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43778)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:57:54 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817874785
16/10/30 09:57:54 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-67078705-abbb-4328-a00a-635a5c1625e1/userFiles-e60930f4-cfc4-4490-acf3-57c2f31c9c74/SparkJNIPi.so
16/10/30 09:57:55 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:57:55 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:57:55 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:57:55 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:57:55 INFO DAGScheduler: Missing parents: List()
16/10/30 09:57:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:57:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:57:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:57:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43778 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:57:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:57:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:57:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:57:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:57:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:57:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:57:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:57:55 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817874785
16/10/30 09:57:55 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-67078705-abbb-4328-a00a-635a5c1625e1/userFiles-e60930f4-cfc4-4490-acf3-57c2f31c9c74/SparkJNIPi.so
16/10/30 09:57:55 INFO Executor: Fetching spark://192.168.0.17:45247/jars/jni-spark-0.1.jar with timestamp 1477817872533
16/10/30 09:57:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45247 after 53 ms (0 ms spent in bootstraps)
16/10/30 09:57:55 INFO Utils: Fetching spark://192.168.0.17:45247/jars/jni-spark-0.1.jar to /tmp/spark-67078705-abbb-4328-a00a-635a5c1625e1/userFiles-e60930f4-cfc4-4490-acf3-57c2f31c9c74/fetchFileTemp6103155285812032733.tmp
16/10/30 09:57:56 INFO Executor: Adding file:/tmp/spark-67078705-abbb-4328-a00a-635a5c1625e1/userFiles-e60930f4-cfc4-4490-acf3-57c2f31c9c74/jni-spark-0.1.jar to class loader
16/10/30 09:57:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:57:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 565 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:57:57 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 09:57:57 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:43778 (size: 16.1 MB, free: 350.2 MB)
16/10/30 09:57:57 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 09:57:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43778 after 6 ms (0 ms spent in bootstraps)
16/10/30 09:57:57 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:43778 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 09:57:57 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1652 ms on localhost (2/2)
16/10/30 09:57:57 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.860 s
16/10/30 09:57:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:57:57 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.172305 s
Result: 3.1403494 in 2.658 seconds
16/10/30 09:57:57 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:57:57 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:57:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:57:57 INFO MemoryStore: MemoryStore cleared
16/10/30 09:57:57 INFO BlockManager: BlockManager stopped
16/10/30 09:57:57 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:57:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:57:57 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:57:57 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:57:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-67078705-abbb-4328-a00a-635a5c1625e1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:57:59 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:00 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:00 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:00 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:00 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:00 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:01 INFO Utils: Successfully started service 'sparkDriver' on port 34703.
16/10/30 09:58:01 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:01 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ace3545c-f7c0-4c57-9925-a988368d3976
16/10/30 09:58:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:01 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:02 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34703/jars/jni-spark-0.1.jar with timestamp 1477817882164
16/10/30 09:58:02 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33714.
16/10/30 09:58:02 INFO NettyBlockTransferService: Server created on 192.168.0.17:33714
16/10/30 09:58:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33714)
16/10/30 09:58:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33714 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33714)
16/10/30 09:58:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33714)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:04 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817884349
16/10/30 09:58:04 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-fec5baa0-d687-416a-b4fd-f277678f31bc/userFiles-98065f1c-cbe9-443c-b9f1-a8130c16f058/SparkJNIPi.so
16/10/30 09:58:04 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:04 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:04 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:04 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:04 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33714 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:58:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:04 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:04 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817884349
16/10/30 09:58:05 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-fec5baa0-d687-416a-b4fd-f277678f31bc/userFiles-98065f1c-cbe9-443c-b9f1-a8130c16f058/SparkJNIPi.so
16/10/30 09:58:05 INFO Executor: Fetching spark://192.168.0.17:34703/jars/jni-spark-0.1.jar with timestamp 1477817882164
16/10/30 09:58:05 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34703 after 25 ms (0 ms spent in bootstraps)
16/10/30 09:58:05 INFO Utils: Fetching spark://192.168.0.17:34703/jars/jni-spark-0.1.jar to /tmp/spark-fec5baa0-d687-416a-b4fd-f277678f31bc/userFiles-98065f1c-cbe9-443c-b9f1-a8130c16f058/fetchFileTemp7405185106366081148.tmp
16/10/30 09:58:05 INFO Executor: Adding file:/tmp/spark-fec5baa0-d687-416a-b4fd-f277678f31bc/userFiles-98065f1c-cbe9-443c-b9f1-a8130c16f058/jni-spark-0.1.jar to class loader
16/10/30 09:58:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:58:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 322 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:58:08 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:58:08 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:33714 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:58:08 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 09:58:08 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33714 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:58:08 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:33714 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:58:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3574 ms on localhost (2/2)
16/10/30 09:58:08 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.769 s
16/10/30 09:58:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:58:08 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 4.024215 s
Result: 3.1411226 in 4.341 seconds
16/10/30 09:58:08 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:58:08 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:58:08 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:58:08 INFO MemoryStore: MemoryStore cleared
16/10/30 09:58:08 INFO BlockManager: BlockManager stopped
16/10/30 09:58:08 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:58:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:58:08 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:58:08 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:58:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-fec5baa0-d687-416a-b4fd-f277678f31bc
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:09 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:10 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:10 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:10 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:10 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:10 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:10 INFO Utils: Successfully started service 'sparkDriver' on port 39905.
16/10/30 09:58:10 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:10 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3a3e554c-a975-4fee-ae45-08e2d812e265
16/10/30 09:58:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:10 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:11 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39905/jars/jni-spark-0.1.jar with timestamp 1477817891062
16/10/30 09:58:11 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42616.
16/10/30 09:58:11 INFO NettyBlockTransferService: Server created on 192.168.0.17:42616
16/10/30 09:58:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42616)
16/10/30 09:58:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42616 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42616)
16/10/30 09:58:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42616)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:12 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817892849
16/10/30 09:58:12 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-b3fc8841-2d33-4de4-910c-632d25f0392c/userFiles-08546596-9249-4ab1-9774-a712556b924f/SparkJNIPi.so
16/10/30 09:58:13 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:13 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:13 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:13 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:13 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42616 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:58:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:13 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:13 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817892849
16/10/30 09:58:13 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-b3fc8841-2d33-4de4-910c-632d25f0392c/userFiles-08546596-9249-4ab1-9774-a712556b924f/SparkJNIPi.so
16/10/30 09:58:13 INFO Executor: Fetching spark://192.168.0.17:39905/jars/jni-spark-0.1.jar with timestamp 1477817891062
16/10/30 09:58:13 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39905 after 22 ms (0 ms spent in bootstraps)
16/10/30 09:58:13 INFO Utils: Fetching spark://192.168.0.17:39905/jars/jni-spark-0.1.jar to /tmp/spark-b3fc8841-2d33-4de4-910c-632d25f0392c/userFiles-08546596-9249-4ab1-9774-a712556b924f/fetchFileTemp3411409044797343739.tmp
16/10/30 09:58:13 INFO Executor: Adding file:/tmp/spark-b3fc8841-2d33-4de4-910c-632d25f0392c/userFiles-08546596-9249-4ab1-9774-a712556b924f/jni-spark-0.1.jar to class loader
16/10/30 09:58:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:58:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 337 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:58:16 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:58:16 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:42616 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:58:16 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 09:58:16 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42616 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:58:16 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:42616 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:58:17 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3524 ms on localhost (2/2)
16/10/30 09:58:17 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.711 s
16/10/30 09:58:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:58:17 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.998518 s
Result: 3.141424 in 4.257 seconds
16/10/30 09:58:17 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:58:17 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:58:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:58:17 INFO MemoryStore: MemoryStore cleared
16/10/30 09:58:17 INFO BlockManager: BlockManager stopped
16/10/30 09:58:17 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:58:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:58:17 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:58:17 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:58:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-b3fc8841-2d33-4de4-910c-632d25f0392c
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:18 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:18 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:18 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:18 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:18 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:18 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:19 INFO Utils: Successfully started service 'sparkDriver' on port 45328.
16/10/30 09:58:19 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:19 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c87d941d-637a-4ef2-a765-bf34b28be4bf
16/10/30 09:58:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:19 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:19 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45328/jars/jni-spark-0.1.jar with timestamp 1477817899630
16/10/30 09:58:19 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34161.
16/10/30 09:58:19 INFO NettyBlockTransferService: Server created on 192.168.0.17:34161
16/10/30 09:58:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34161)
16/10/30 09:58:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34161 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34161)
16/10/30 09:58:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34161)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:21 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817901087
16/10/30 09:58:21 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-b0cdb85f-0b1f-4965-b80f-10c2debf41b7/userFiles-d314d0df-4833-4c8f-8ef3-4a82bdd3a6f8/SparkJNIPi.so
16/10/30 09:58:21 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:21 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:21 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:21 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:21 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34161 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:58:21 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:21 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:21 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817901087
16/10/30 09:58:21 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-b0cdb85f-0b1f-4965-b80f-10c2debf41b7/userFiles-d314d0df-4833-4c8f-8ef3-4a82bdd3a6f8/SparkJNIPi.so
16/10/30 09:58:21 INFO Executor: Fetching spark://192.168.0.17:45328/jars/jni-spark-0.1.jar with timestamp 1477817899630
16/10/30 09:58:21 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45328 after 22 ms (0 ms spent in bootstraps)
16/10/30 09:58:21 INFO Utils: Fetching spark://192.168.0.17:45328/jars/jni-spark-0.1.jar to /tmp/spark-b0cdb85f-0b1f-4965-b80f-10c2debf41b7/userFiles-d314d0df-4833-4c8f-8ef3-4a82bdd3a6f8/fetchFileTemp6253228794697204015.tmp
16/10/30 09:58:21 INFO Executor: Adding file:/tmp/spark-b0cdb85f-0b1f-4965-b80f-10c2debf41b7/userFiles-d314d0df-4833-4c8f-8ef3-4a82bdd3a6f8/jni-spark-0.1.jar to class loader
16/10/30 09:58:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 09:58:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 256 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:58:24 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:58:24 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:34161 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:58:24 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 09:58:24 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34161 after 7 ms (0 ms spent in bootstraps)
16/10/30 09:58:24 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:34161 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:58:25 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3197 ms on localhost (2/2)
16/10/30 09:58:25 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.466 s
16/10/30 09:58:25 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:58:25 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.720377 s
16/10/30 09:58:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.17:34161 in memory (size: 1698.0 B, free: 366.3 MB)
Result: 3.1413476 in 4.207 seconds
16/10/30 09:58:25 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:58:25 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:58:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:58:25 INFO MemoryStore: MemoryStore cleared
16/10/30 09:58:25 INFO BlockManager: BlockManager stopped
16/10/30 09:58:25 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:58:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:58:25 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:58:25 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:58:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-b0cdb85f-0b1f-4965-b80f-10c2debf41b7
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:26 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:26 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:27 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:27 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:27 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:27 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:27 INFO Utils: Successfully started service 'sparkDriver' on port 33968.
16/10/30 09:58:27 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:27 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1072adc5-3b0c-44a0-bb4b-0f65d4efc8e5
16/10/30 09:58:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:27 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:27 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33968/jars/jni-spark-0.1.jar with timestamp 1477817907676
16/10/30 09:58:27 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34610.
16/10/30 09:58:27 INFO NettyBlockTransferService: Server created on 192.168.0.17:34610
16/10/30 09:58:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34610)
16/10/30 09:58:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34610 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34610)
16/10/30 09:58:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34610)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:29 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817909465
16/10/30 09:58:29 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6c4a6e6c-ae97-4203-b102-fb0524156e79/userFiles-7bf50532-19c4-4e5e-8fbf-dcfa95b3ebec/SparkJNIPi.so
16/10/30 09:58:29 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:29 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:29 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:29 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:29 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34610 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:30 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 09:58:30 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:30 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:30 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817909465
16/10/30 09:58:30 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6c4a6e6c-ae97-4203-b102-fb0524156e79/userFiles-7bf50532-19c4-4e5e-8fbf-dcfa95b3ebec/SparkJNIPi.so
16/10/30 09:58:30 INFO Executor: Fetching spark://192.168.0.17:33968/jars/jni-spark-0.1.jar with timestamp 1477817907676
16/10/30 09:58:30 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33968 after 62 ms (0 ms spent in bootstraps)
16/10/30 09:58:30 INFO Utils: Fetching spark://192.168.0.17:33968/jars/jni-spark-0.1.jar to /tmp/spark-6c4a6e6c-ae97-4203-b102-fb0524156e79/userFiles-7bf50532-19c4-4e5e-8fbf-dcfa95b3ebec/fetchFileTemp3185349675274030734.tmp
16/10/30 09:58:30 INFO Executor: Adding file:/tmp/spark-6c4a6e6c-ae97-4203-b102-fb0524156e79/userFiles-7bf50532-19c4-4e5e-8fbf-dcfa95b3ebec/jni-spark-0.1.jar to class loader
16/10/30 09:58:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
16/10/30 09:58:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 443 ms on localhost (1/2)
Calling method randToSum
16/10/30 09:58:33 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 09:58:33 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:34610 (size: 128.6 MB, free: 237.7 MB)
16/10/30 09:58:33 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 09:58:33 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34610 after 1 ms (0 ms spent in bootstraps)
16/10/30 09:58:33 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:34610 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 09:58:33 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3615 ms on localhost (2/2)
16/10/30 09:58:33 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.862 s
16/10/30 09:58:33 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:58:33 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 4.200023 s
Result: 3.1415753 in 4.496 seconds
16/10/30 09:58:33 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:58:33 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:58:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:58:34 INFO MemoryStore: MemoryStore cleared
16/10/30 09:58:34 INFO BlockManager: BlockManager stopped
16/10/30 09:58:34 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:58:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:58:34 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:58:34 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:58:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c4a6e6c-ae97-4203-b102-fb0524156e79
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:35 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:35 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:35 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:35 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:35 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:35 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:35 INFO Utils: Successfully started service 'sparkDriver' on port 41719.
16/10/30 09:58:35 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:35 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:35 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8a231e64-8367-4e4f-9e88-002a489bb719
16/10/30 09:58:35 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:36 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:36 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41719/jars/jni-spark-0.1.jar with timestamp 1477817916294
16/10/30 09:58:36 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44907.
16/10/30 09:58:36 INFO NettyBlockTransferService: Server created on 192.168.0.17:44907
16/10/30 09:58:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44907)
16/10/30 09:58:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44907 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44907)
16/10/30 09:58:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44907)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:38 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817918119
16/10/30 09:58:38 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-9e2c3c05-c965-4978-a0c9-98f025694e1f/userFiles-c4cf0ca1-df3e-4f44-b6cf-2eda5c2ebd76/SparkJNIPi.so
16/10/30 09:58:38 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:38 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:38 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:38 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:38 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:38 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44907 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:38 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:38 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:38 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:38 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:38 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:38 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:38 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817918119
16/10/30 09:58:38 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-9e2c3c05-c965-4978-a0c9-98f025694e1f/userFiles-c4cf0ca1-df3e-4f44-b6cf-2eda5c2ebd76/SparkJNIPi.so
16/10/30 09:58:38 INFO Executor: Fetching spark://192.168.0.17:41719/jars/jni-spark-0.1.jar with timestamp 1477817916294
16/10/30 09:58:38 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41719 after 44 ms (0 ms spent in bootstraps)
16/10/30 09:58:38 INFO Utils: Fetching spark://192.168.0.17:41719/jars/jni-spark-0.1.jar to /tmp/spark-9e2c3c05-c965-4978-a0c9-98f025694e1f/userFiles-c4cf0ca1-df3e-4f44-b6cf-2eda5c2ebd76/fetchFileTemp3851960144882106456.tmp
16/10/30 09:58:39 INFO Executor: Adding file:/tmp/spark-9e2c3c05-c965-4978-a0c9-98f025694e1f/userFiles-c4cf0ca1-df3e-4f44-b6cf-2eda5c2ebd76/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f0ad08dcf30, pid=22160, tid=0x00007f0b412ba700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22160.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22160 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:40 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:40 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:40 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:40 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:40 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:40 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:41 INFO Utils: Successfully started service 'sparkDriver' on port 41059.
16/10/30 09:58:41 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:41 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:41 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-73445af6-47b4-4325-95ab-d9ad250d3bae
16/10/30 09:58:41 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:41 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:41 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:41 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:41 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41059/jars/jni-spark-0.1.jar with timestamp 1477817921590
16/10/30 09:58:41 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36355.
16/10/30 09:58:41 INFO NettyBlockTransferService: Server created on 192.168.0.17:36355
16/10/30 09:58:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36355)
16/10/30 09:58:41 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36355 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36355)
16/10/30 09:58:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36355)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:43 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817923346
16/10/30 09:58:43 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-4e3d3213-e4ff-44d3-97bd-8b3dff0180bd/userFiles-1988125c-595f-4d4c-bf7f-de9486247c74/SparkJNIPi.so
16/10/30 09:58:43 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:43 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:43 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:43 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:43 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36355 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:43 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:43 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:43 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:43 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:43 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817923346
16/10/30 09:58:43 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-4e3d3213-e4ff-44d3-97bd-8b3dff0180bd/userFiles-1988125c-595f-4d4c-bf7f-de9486247c74/SparkJNIPi.so
16/10/30 09:58:43 INFO Executor: Fetching spark://192.168.0.17:41059/jars/jni-spark-0.1.jar with timestamp 1477817921590
16/10/30 09:58:44 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41059 after 28 ms (0 ms spent in bootstraps)
16/10/30 09:58:44 INFO Utils: Fetching spark://192.168.0.17:41059/jars/jni-spark-0.1.jar to /tmp/spark-4e3d3213-e4ff-44d3-97bd-8b3dff0180bd/userFiles-1988125c-595f-4d4c-bf7f-de9486247c74/fetchFileTemp9150912644658308783.tmp
16/10/30 09:58:44 INFO Executor: Adding file:/tmp/spark-4e3d3213-e4ff-44d3-97bd-8b3dff0180bd/userFiles-1988125c-595f-4d4c-bf7f-de9486247c74/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f92506cfb21, pid=22258, tid=0x00007f92c11b9700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x2b21]  pci_system_cleanup+0x71
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22258.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22258 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:45 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:45 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:45 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:45 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:45 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:45 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:45 INFO Utils: Successfully started service 'sparkDriver' on port 32940.
16/10/30 09:58:45 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:45 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0635b0db-1479-4844-b606-bd5cc01b0712
16/10/30 09:58:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:46 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:46 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:32940/jars/jni-spark-0.1.jar with timestamp 1477817926247
16/10/30 09:58:46 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45671.
16/10/30 09:58:46 INFO NettyBlockTransferService: Server created on 192.168.0.17:45671
16/10/30 09:58:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45671)
16/10/30 09:58:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45671 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45671)
16/10/30 09:58:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45671)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:48 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817928035
16/10/30 09:58:48 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-19124db2-e16a-434a-a0be-c3914c7bc603/userFiles-ea5ed8d8-1954-45da-bf43-b34b4db8ea04/SparkJNIPi.so
16/10/30 09:58:48 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:48 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:48 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:48 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:48 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:48 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:48 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:48 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:48 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45671 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:48 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:48 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:48 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:48 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:48 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:48 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:48 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:48 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817928035
16/10/30 09:58:48 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-19124db2-e16a-434a-a0be-c3914c7bc603/userFiles-ea5ed8d8-1954-45da-bf43-b34b4db8ea04/SparkJNIPi.so
16/10/30 09:58:48 INFO Executor: Fetching spark://192.168.0.17:32940/jars/jni-spark-0.1.jar with timestamp 1477817926247
16/10/30 09:58:48 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:32940 after 28 ms (0 ms spent in bootstraps)
16/10/30 09:58:48 INFO Utils: Fetching spark://192.168.0.17:32940/jars/jni-spark-0.1.jar to /tmp/spark-19124db2-e16a-434a-a0be-c3914c7bc603/userFiles-ea5ed8d8-1954-45da-bf43-b34b4db8ea04/fetchFileTemp1197213683417854023.tmp
16/10/30 09:58:48 INFO Executor: Adding file:/tmp/spark-19124db2-e16a-434a-a0be-c3914c7bc603/userFiles-ea5ed8d8-1954-45da-bf43-b34b4db8ea04/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f4077bb79dd, pid=22359, tid=0x00007f407bbf2700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22359.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22359 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:49 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:50 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:50 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:50 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:50 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:50 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:50 INFO Utils: Successfully started service 'sparkDriver' on port 46252.
16/10/30 09:58:50 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:50 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c553681a-6f89-4104-9910-3190f849bc2f
16/10/30 09:58:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:50 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:51 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46252/jars/jni-spark-0.1.jar with timestamp 1477817931002
16/10/30 09:58:51 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:51 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45321.
16/10/30 09:58:51 INFO NettyBlockTransferService: Server created on 192.168.0.17:45321
16/10/30 09:58:51 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45321)
16/10/30 09:58:51 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45321 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45321)
16/10/30 09:58:51 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45321)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:52 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817932787
16/10/30 09:58:52 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-9dd48ae7-36a1-4977-90a7-3652b3007075/userFiles-7ca591de-dd86-442b-a93b-9d36f471b891/SparkJNIPi.so
16/10/30 09:58:53 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:53 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:53 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:53 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:53 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:53 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45321 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:53 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:53 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817932787
16/10/30 09:58:53 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-9dd48ae7-36a1-4977-90a7-3652b3007075/userFiles-7ca591de-dd86-442b-a93b-9d36f471b891/SparkJNIPi.so
16/10/30 09:58:53 INFO Executor: Fetching spark://192.168.0.17:46252/jars/jni-spark-0.1.jar with timestamp 1477817931002
16/10/30 09:58:53 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46252 after 28 ms (0 ms spent in bootstraps)
16/10/30 09:58:53 INFO Utils: Fetching spark://192.168.0.17:46252/jars/jni-spark-0.1.jar to /tmp/spark-9dd48ae7-36a1-4977-90a7-3652b3007075/userFiles-7ca591de-dd86-442b-a93b-9d36f471b891/fetchFileTemp1482328001894433630.tmp
16/10/30 09:58:53 INFO Executor: Adding file:/tmp/spark-9dd48ae7-36a1-4977-90a7-3652b3007075/userFiles-7ca591de-dd86-442b-a93b-9d36f471b891/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7e318ddf30, pid=22456, tid=0x00007f7ea23eb700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22456.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22456 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:54 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:55 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:55 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:55 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:55 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:55 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:58:55 INFO Utils: Successfully started service 'sparkDriver' on port 43114.
16/10/30 09:58:55 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:58:55 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:58:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9bbcaaba-e413-43f3-b14b-a43ad15e5f60
16/10/30 09:58:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:58:55 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:58:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:58:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:58:55 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43114/jars/jni-spark-0.1.jar with timestamp 1477817935878
16/10/30 09:58:55 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:58:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45771.
16/10/30 09:58:55 INFO NettyBlockTransferService: Server created on 192.168.0.17:45771
16/10/30 09:58:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45771)
16/10/30 09:58:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45771 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45771)
16/10/30 09:58:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45771)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:58:57 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817937628
16/10/30 09:58:57 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8b57f6ea-7b8b-4bb9-bb23-cd07f4caf904/userFiles-b61dace5-4ec4-40ea-9a48-a63b784ff1b9/SparkJNIPi.so
16/10/30 09:58:57 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:58:57 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:58:57 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:58:57 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:58:57 INFO DAGScheduler: Missing parents: List()
16/10/30 09:58:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:58:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:58:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:58:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45771 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:58:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:58:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:58:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:58:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:58 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:58:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:58:58 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:58:58 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817937628
16/10/30 09:58:58 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8b57f6ea-7b8b-4bb9-bb23-cd07f4caf904/userFiles-b61dace5-4ec4-40ea-9a48-a63b784ff1b9/SparkJNIPi.so
16/10/30 09:58:58 INFO Executor: Fetching spark://192.168.0.17:43114/jars/jni-spark-0.1.jar with timestamp 1477817935878
16/10/30 09:58:58 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43114 after 29 ms (0 ms spent in bootstraps)
16/10/30 09:58:58 INFO Utils: Fetching spark://192.168.0.17:43114/jars/jni-spark-0.1.jar to /tmp/spark-8b57f6ea-7b8b-4bb9-bb23-cd07f4caf904/userFiles-b61dace5-4ec4-40ea-9a48-a63b784ff1b9/fetchFileTemp4787835205346736568.tmp
16/10/30 09:58:58 INFO Executor: Adding file:/tmp/spark-8b57f6ea-7b8b-4bb9-bb23-cd07f4caf904/userFiles-b61dace5-4ec4-40ea-9a48-a63b784ff1b9/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fed6105d013, pid=22554, tid=0x00007fed691d6700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x3013]  pci_device_unmap_region+0x23
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22554.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22554 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:58:59 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:58:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:58:59 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:58:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:58:59 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:58:59 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:58:59 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:58:59 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:58:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:00 INFO Utils: Successfully started service 'sparkDriver' on port 45010.
16/10/30 09:59:00 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:00 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9973f896-8fde-4999-b183-cba83884f8a0
16/10/30 09:59:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:00 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:00 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45010/jars/jni-spark-0.1.jar with timestamp 1477817940586
16/10/30 09:59:00 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38704.
16/10/30 09:59:00 INFO NettyBlockTransferService: Server created on 192.168.0.17:38704
16/10/30 09:59:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38704)
16/10/30 09:59:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38704 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38704)
16/10/30 09:59:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38704)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:02 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817942662
16/10/30 09:59:02 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-92419455-4b9f-4e52-b72a-b0e3bca0fd78/userFiles-3ac9886d-6ba8-4b8d-bd58-2116350c7ec5/SparkJNIPi.so
16/10/30 09:59:02 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:02 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:02 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:02 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:02 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38704 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:03 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817942662
16/10/30 09:59:03 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-92419455-4b9f-4e52-b72a-b0e3bca0fd78/userFiles-3ac9886d-6ba8-4b8d-bd58-2116350c7ec5/SparkJNIPi.so
16/10/30 09:59:03 INFO Executor: Fetching spark://192.168.0.17:45010/jars/jni-spark-0.1.jar with timestamp 1477817940586
16/10/30 09:59:03 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45010 after 35 ms (0 ms spent in bootstraps)
16/10/30 09:59:03 INFO Utils: Fetching spark://192.168.0.17:45010/jars/jni-spark-0.1.jar to /tmp/spark-92419455-4b9f-4e52-b72a-b0e3bca0fd78/userFiles-3ac9886d-6ba8-4b8d-bd58-2116350c7ec5/fetchFileTemp8506995615237996168.tmp
16/10/30 09:59:03 INFO Executor: Adding file:/tmp/spark-92419455-4b9f-4e52-b72a-b0e3bca0fd78/userFiles-3ac9886d-6ba8-4b8d-bd58-2116350c7ec5/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f96e1af2b5a, pid=22652, tid=0x00007f96f1bde700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x4b5a]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22652.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22652 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:04 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:05 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:05 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:05 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:05 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:05 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:05 INFO Utils: Successfully started service 'sparkDriver' on port 43662.
16/10/30 09:59:05 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:05 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-17962bb2-32b1-4f8c-9e32-09c29457bdcf
16/10/30 09:59:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:05 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:06 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43662/jars/jni-spark-0.1.jar with timestamp 1477817946066
16/10/30 09:59:06 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37884.
16/10/30 09:59:06 INFO NettyBlockTransferService: Server created on 192.168.0.17:37884
16/10/30 09:59:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37884)
16/10/30 09:59:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37884 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37884)
16/10/30 09:59:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37884)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:07 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817947783
16/10/30 09:59:07 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-0e881723-daff-4592-b44f-7604dca65a92/userFiles-1e7e5ba6-e991-4539-a09a-e4b5f4d4734b/SparkJNIPi.so
16/10/30 09:59:08 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:08 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:08 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:08 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:08 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37884 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:08 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:08 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:08 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817947783
16/10/30 09:59:08 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-0e881723-daff-4592-b44f-7604dca65a92/userFiles-1e7e5ba6-e991-4539-a09a-e4b5f4d4734b/SparkJNIPi.so
16/10/30 09:59:08 INFO Executor: Fetching spark://192.168.0.17:43662/jars/jni-spark-0.1.jar with timestamp 1477817946066
16/10/30 09:59:08 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43662 after 27 ms (0 ms spent in bootstraps)
16/10/30 09:59:08 INFO Utils: Fetching spark://192.168.0.17:43662/jars/jni-spark-0.1.jar to /tmp/spark-0e881723-daff-4592-b44f-7604dca65a92/userFiles-1e7e5ba6-e991-4539-a09a-e4b5f4d4734b/fetchFileTemp8849583187907899091.tmp
16/10/30 09:59:08 INFO Executor: Adding file:/tmp/spark-0e881723-daff-4592-b44f-7604dca65a92/userFiles-1e7e5ba6-e991-4539-a09a-e4b5f4d4734b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f26cc2c7b39, pid=22750, tid=0x00007f2744cb7700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x2b39]  pci_system_cleanup+0x89
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22750.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22750 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:09 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:10 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:10 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:10 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:10 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:10 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:10 INFO Utils: Successfully started service 'sparkDriver' on port 33400.
16/10/30 09:59:10 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:10 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2d37ece9-45da-46b1-8056-12cbf5fb2179
16/10/30 09:59:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:10 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:10 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33400/jars/jni-spark-0.1.jar with timestamp 1477817950905
16/10/30 09:59:10 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45646.
16/10/30 09:59:11 INFO NettyBlockTransferService: Server created on 192.168.0.17:45646
16/10/30 09:59:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45646)
16/10/30 09:59:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45646 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45646)
16/10/30 09:59:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45646)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:12 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817952506
16/10/30 09:59:12 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-347a9cd6-8538-4ae2-822b-1d351be0913b/userFiles-bf3a5efb-2c1b-432a-a8b6-c89a41676fb4/SparkJNIPi.so
16/10/30 09:59:12 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:12 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:12 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:12 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:12 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45646 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:12 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:13 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:13 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817952506
16/10/30 09:59:13 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-347a9cd6-8538-4ae2-822b-1d351be0913b/userFiles-bf3a5efb-2c1b-432a-a8b6-c89a41676fb4/SparkJNIPi.so
16/10/30 09:59:13 INFO Executor: Fetching spark://192.168.0.17:33400/jars/jni-spark-0.1.jar with timestamp 1477817950905
16/10/30 09:59:13 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33400 after 22 ms (0 ms spent in bootstraps)
16/10/30 09:59:13 INFO Utils: Fetching spark://192.168.0.17:33400/jars/jni-spark-0.1.jar to /tmp/spark-347a9cd6-8538-4ae2-822b-1d351be0913b/userFiles-bf3a5efb-2c1b-432a-a8b6-c89a41676fb4/fetchFileTemp1290501982982666900.tmp
16/10/30 09:59:13 INFO Executor: Adding file:/tmp/spark-347a9cd6-8538-4ae2-822b-1d351be0913b/userFiles-bf3a5efb-2c1b-432a-a8b6-c89a41676fb4/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f67144ccf30, pid=22856, tid=0x00007f67545a9700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid22856.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22856 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:14 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:14 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:14 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:14 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:14 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:14 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:15 INFO Utils: Successfully started service 'sparkDriver' on port 35840.
16/10/30 09:59:15 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:15 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-30a8bdb4-fd72-492a-b3ad-e882f8e06ada
16/10/30 09:59:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:15 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:15 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35840/jars/jni-spark-0.1.jar with timestamp 1477817955519
16/10/30 09:59:15 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45979.
16/10/30 09:59:15 INFO NettyBlockTransferService: Server created on 192.168.0.17:45979
16/10/30 09:59:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45979)
16/10/30 09:59:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45979 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45979)
16/10/30 09:59:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45979)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:17 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817957265
16/10/30 09:59:17 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-764d908a-072d-47e6-ab07-8fb1f0469d51/userFiles-e73b8366-b796-47de-ac7c-e3b7f6d63944/SparkJNIPi.so
16/10/30 09:59:17 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:17 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:17 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:17 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:17 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45979 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:17 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817957265
16/10/30 09:59:17 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-764d908a-072d-47e6-ab07-8fb1f0469d51/userFiles-e73b8366-b796-47de-ac7c-e3b7f6d63944/SparkJNIPi.so
16/10/30 09:59:17 INFO Executor: Fetching spark://192.168.0.17:35840/jars/jni-spark-0.1.jar with timestamp 1477817955519
16/10/30 09:59:17 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35840 after 24 ms (0 ms spent in bootstraps)
16/10/30 09:59:17 INFO Utils: Fetching spark://192.168.0.17:35840/jars/jni-spark-0.1.jar to /tmp/spark-764d908a-072d-47e6-ab07-8fb1f0469d51/userFiles-e73b8366-b796-47de-ac7c-e3b7f6d63944/fetchFileTemp2019907056475934600.tmp
16/10/30 09:59:17 INFO Executor: Adding file:/tmp/spark-764d908a-072d-47e6-ab07-8fb1f0469d51/userFiles-e73b8366-b796-47de-ac7c-e3b7f6d63944/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 22955 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:19 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:19 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:19 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:19 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:19 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:19 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:19 INFO Utils: Successfully started service 'sparkDriver' on port 38042.
16/10/30 09:59:19 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:19 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-601be57d-1e6d-4bb3-aa4d-06051ee8ec95
16/10/30 09:59:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:19 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:20 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:20 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:20 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38042/jars/jni-spark-0.1.jar with timestamp 1477817960192
16/10/30 09:59:20 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36680.
16/10/30 09:59:20 INFO NettyBlockTransferService: Server created on 192.168.0.17:36680
16/10/30 09:59:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36680)
16/10/30 09:59:20 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36680 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36680)
16/10/30 09:59:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36680)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:22 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817962054
16/10/30 09:59:22 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-7f2ab530-b87b-4170-ace6-aeee260e573a/userFiles-668656a8-71e9-41f5-84f5-4ee994b4aa1a/SparkJNIPi.so
16/10/30 09:59:22 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:22 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:22 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:22 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:22 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36680 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:22 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:22 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:22 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:22 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817962054
16/10/30 09:59:22 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-7f2ab530-b87b-4170-ace6-aeee260e573a/userFiles-668656a8-71e9-41f5-84f5-4ee994b4aa1a/SparkJNIPi.so
16/10/30 09:59:22 INFO Executor: Fetching spark://192.168.0.17:38042/jars/jni-spark-0.1.jar with timestamp 1477817960192
16/10/30 09:59:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38042 after 30 ms (0 ms spent in bootstraps)
16/10/30 09:59:22 INFO Utils: Fetching spark://192.168.0.17:38042/jars/jni-spark-0.1.jar to /tmp/spark-7f2ab530-b87b-4170-ace6-aeee260e573a/userFiles-668656a8-71e9-41f5-84f5-4ee994b4aa1a/fetchFileTemp7762891226166246460.tmp
16/10/30 09:59:22 INFO Executor: Adding file:/tmp/spark-7f2ab530-b87b-4170-ace6-aeee260e573a/userFiles-668656a8-71e9-41f5-84f5-4ee994b4aa1a/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f4f488dcf30, pid=23052, tid=0x00007f4fb93d0700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid23052.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 23052 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:24 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:24 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:24 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:24 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:24 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:24 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:25 INFO Utils: Successfully started service 'sparkDriver' on port 42647.
16/10/30 09:59:25 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:25 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f2036e0b-f79a-4684-af23-2163eddb0bcd
16/10/30 09:59:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:25 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:25 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42647/jars/jni-spark-0.1.jar with timestamp 1477817965755
16/10/30 09:59:25 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44990.
16/10/30 09:59:25 INFO NettyBlockTransferService: Server created on 192.168.0.17:44990
16/10/30 09:59:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44990)
16/10/30 09:59:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44990 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44990)
16/10/30 09:59:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44990)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:27 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817967484
16/10/30 09:59:27 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-7dc9b354-4bfa-4375-a901-e72a0153d9f9/userFiles-bf4d7ca2-889a-4091-a89a-1876c68721b4/SparkJNIPi.so
16/10/30 09:59:27 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:27 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:27 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:27 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:27 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44990 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:28 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:28 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:28 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:28 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817967484
16/10/30 09:59:28 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-7dc9b354-4bfa-4375-a901-e72a0153d9f9/userFiles-bf4d7ca2-889a-4091-a89a-1876c68721b4/SparkJNIPi.so
16/10/30 09:59:28 INFO Executor: Fetching spark://192.168.0.17:42647/jars/jni-spark-0.1.jar with timestamp 1477817965755
16/10/30 09:59:28 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42647 after 40 ms (0 ms spent in bootstraps)
16/10/30 09:59:28 INFO Utils: Fetching spark://192.168.0.17:42647/jars/jni-spark-0.1.jar to /tmp/spark-7dc9b354-4bfa-4375-a901-e72a0153d9f9/userFiles-bf4d7ca2-889a-4091-a89a-1876c68721b4/fetchFileTemp8671153936122939160.tmp
16/10/30 09:59:28 INFO Executor: Adding file:/tmp/spark-7dc9b354-4bfa-4375-a901-e72a0153d9f9/userFiles-bf4d7ca2-889a-4091-a89a-1876c68721b4/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9: 23150 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:29 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:30 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:30 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:30 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:30 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:30 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:30 INFO Utils: Successfully started service 'sparkDriver' on port 37918.
16/10/30 09:59:30 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:30 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5b5dc73e-2332-43d0-9ee4-e4a45815ad60
16/10/30 09:59:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:30 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:30 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37918/jars/jni-spark-0.1.jar with timestamp 1477817970877
16/10/30 09:59:30 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38893.
16/10/30 09:59:31 INFO NettyBlockTransferService: Server created on 192.168.0.17:38893
16/10/30 09:59:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38893)
16/10/30 09:59:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38893 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38893)
16/10/30 09:59:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38893)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:32 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817972952
16/10/30 09:59:32 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-70fd22b9-c27a-4197-9ac7-f40f67201fa0/userFiles-8103c736-f909-4dea-8ece-cd507a0df902/SparkJNIPi.so
16/10/30 09:59:33 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:33 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:33 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:33 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:33 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:33 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38893 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:33 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:33 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:33 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:33 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817972952
16/10/30 09:59:33 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-70fd22b9-c27a-4197-9ac7-f40f67201fa0/userFiles-8103c736-f909-4dea-8ece-cd507a0df902/SparkJNIPi.so
16/10/30 09:59:33 INFO Executor: Fetching spark://192.168.0.17:37918/jars/jni-spark-0.1.jar with timestamp 1477817970877
16/10/30 09:59:33 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37918 after 26 ms (0 ms spent in bootstraps)
16/10/30 09:59:33 INFO Utils: Fetching spark://192.168.0.17:37918/jars/jni-spark-0.1.jar to /tmp/spark-70fd22b9-c27a-4197-9ac7-f40f67201fa0/userFiles-8103c736-f909-4dea-8ece-cd507a0df902/fetchFileTemp1136374581560338522.tmp
16/10/30 09:59:33 INFO Executor: Adding file:/tmp/spark-70fd22b9-c27a-4197-9ac7-f40f67201fa0/userFiles-8103c736-f909-4dea-8ece-cd507a0df902/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fc0024cef30, pid=23247, tid=0x00007fc00a6e7700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid23247.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9: 23247 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:35 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:35 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:35 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:35 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:35 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:35 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:36 INFO Utils: Successfully started service 'sparkDriver' on port 34698.
16/10/30 09:59:36 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:36 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4bad4427-b4f7-4707-91b4-378cfda9cfcf
16/10/30 09:59:36 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:36 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:36 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:36 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:36 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34698/jars/jni-spark-0.1.jar with timestamp 1477817976556
16/10/30 09:59:36 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34185.
16/10/30 09:59:36 INFO NettyBlockTransferService: Server created on 192.168.0.17:34185
16/10/30 09:59:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34185)
16/10/30 09:59:36 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34185 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34185)
16/10/30 09:59:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34185)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:38 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817978721
16/10/30 09:59:38 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-54eacfb4-ad9c-4235-bbd6-4cbab4f55d12/userFiles-7ee3b53c-28e5-47af-a51c-d211a242914e/SparkJNIPi.so
16/10/30 09:59:39 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:39 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:39 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:39 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:39 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34185 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:39 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:39 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:39 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817978721
16/10/30 09:59:39 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-54eacfb4-ad9c-4235-bbd6-4cbab4f55d12/userFiles-7ee3b53c-28e5-47af-a51c-d211a242914e/SparkJNIPi.so
16/10/30 09:59:40 INFO Executor: Fetching spark://192.168.0.17:34698/jars/jni-spark-0.1.jar with timestamp 1477817976556
16/10/30 09:59:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34698 after 39 ms (0 ms spent in bootstraps)
16/10/30 09:59:40 INFO Utils: Fetching spark://192.168.0.17:34698/jars/jni-spark-0.1.jar to /tmp/spark-54eacfb4-ad9c-4235-bbd6-4cbab4f55d12/userFiles-7ee3b53c-28e5-47af-a51c-d211a242914e/fetchFileTemp8469398658195197360.tmp
16/10/30 09:59:40 INFO Executor: Adding file:/tmp/spark-54eacfb4-ad9c-4235-bbd6-4cbab4f55d12/userFiles-7ee3b53c-28e5-47af-a51c-d211a242914e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f3c5d4d9b00, pid=23348, tid=0x00007f3cce3eb700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x2b00]  pci_system_cleanup+0x50
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid23348.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 23348 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:41 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:41 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:41 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:41 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:41 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:41 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:41 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:42 INFO Utils: Successfully started service 'sparkDriver' on port 42469.
16/10/30 09:59:42 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:42 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6fd6022a-44b2-4512-8be4-ea433cc3393a
16/10/30 09:59:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:42 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:42 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42469/jars/jni-spark-0.1.jar with timestamp 1477817982574
16/10/30 09:59:42 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:42 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38770.
16/10/30 09:59:42 INFO NettyBlockTransferService: Server created on 192.168.0.17:38770
16/10/30 09:59:42 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38770)
16/10/30 09:59:42 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38770 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38770)
16/10/30 09:59:42 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38770)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:44 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817984559
16/10/30 09:59:44 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-35bfd083-18ef-44dc-9200-555341a1a6fe/userFiles-e200edaf-cf52-464f-9b2e-0d8aff3e1204/SparkJNIPi.so
16/10/30 09:59:44 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:44 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:44 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:44 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:44 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:44 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38770 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:45 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817984559
16/10/30 09:59:45 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-35bfd083-18ef-44dc-9200-555341a1a6fe/userFiles-e200edaf-cf52-464f-9b2e-0d8aff3e1204/SparkJNIPi.so
16/10/30 09:59:45 INFO Executor: Fetching spark://192.168.0.17:42469/jars/jni-spark-0.1.jar with timestamp 1477817982574
16/10/30 09:59:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42469 after 34 ms (0 ms spent in bootstraps)
16/10/30 09:59:45 INFO Utils: Fetching spark://192.168.0.17:42469/jars/jni-spark-0.1.jar to /tmp/spark-35bfd083-18ef-44dc-9200-555341a1a6fe/userFiles-e200edaf-cf52-464f-9b2e-0d8aff3e1204/fetchFileTemp4604317641131203362.tmp
16/10/30 09:59:45 INFO Executor: Adding file:/tmp/spark-35bfd083-18ef-44dc-9200-555341a1a6fe/userFiles-e200edaf-cf52-464f-9b2e-0d8aff3e1204/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f57067e1f30, pid=23445, tid=0x00007f5776ceb700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid23445.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 23445 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:47 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:47 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:47 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:47 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:47 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:47 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:48 INFO Utils: Successfully started service 'sparkDriver' on port 34182.
16/10/30 09:59:48 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:48 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c31fb0fc-351c-45f6-8398-ff4926ca7d45
16/10/30 09:59:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:48 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:48 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34182/jars/jni-spark-0.1.jar with timestamp 1477817988851
16/10/30 09:59:48 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38919.
16/10/30 09:59:48 INFO NettyBlockTransferService: Server created on 192.168.0.17:38919
16/10/30 09:59:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38919)
16/10/30 09:59:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38919 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38919)
16/10/30 09:59:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38919)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:50 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817990789
16/10/30 09:59:50 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-093a1ee8-0031-4716-ae38-0dab8fae3c1f/userFiles-6eda6e95-d66e-4ddb-87a2-65117eaee423/SparkJNIPi.so
16/10/30 09:59:51 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:51 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:51 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:51 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:51 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38919 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:51 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817990789
16/10/30 09:59:51 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:51 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-093a1ee8-0031-4716-ae38-0dab8fae3c1f/userFiles-6eda6e95-d66e-4ddb-87a2-65117eaee423/SparkJNIPi.so
16/10/30 09:59:51 INFO Executor: Fetching spark://192.168.0.17:34182/jars/jni-spark-0.1.jar with timestamp 1477817988851
16/10/30 09:59:51 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34182 after 53 ms (0 ms spent in bootstraps)
16/10/30 09:59:51 INFO Utils: Fetching spark://192.168.0.17:34182/jars/jni-spark-0.1.jar to /tmp/spark-093a1ee8-0031-4716-ae38-0dab8fae3c1f/userFiles-6eda6e95-d66e-4ddb-87a2-65117eaee423/fetchFileTemp9012004135334266524.tmp
16/10/30 09:59:52 INFO Executor: Adding file:/tmp/spark-093a1ee8-0031-4716-ae38-0dab8fae3c1f/userFiles-6eda6e95-d66e-4ddb-87a2-65117eaee423/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 09:59:52 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 09:59:52 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:38919 (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:59:52 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 09:59:52 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38919 after 3 ms (0 ms spent in bootstraps)
16/10/30 09:59:52 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 09:59:52 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:38919 (size: 2.0 MB, free: 362.3 MB)
16/10/30 09:59:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 09:59:52 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:38919 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 09:59:52 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:38919 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 09:59:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 867 ms on localhost (1/2)
16/10/30 09:59:52 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.031 s
16/10/30 09:59:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 967 ms on localhost (2/2)
16/10/30 09:59:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 09:59:52 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.528635 s
Result: 3.1438637 in 1.92 seconds
16/10/30 09:59:52 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 09:59:52 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 09:59:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 09:59:52 INFO MemoryStore: MemoryStore cleared
16/10/30 09:59:52 INFO BlockManager: BlockManager stopped
16/10/30 09:59:52 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 09:59:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 09:59:52 INFO SparkContext: Successfully stopped SparkContext
16/10/30 09:59:52 INFO ShutdownHookManager: Shutdown hook called
16/10/30 09:59:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-093a1ee8-0031-4716-ae38-0dab8fae3c1f
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:54 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:54 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:54 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:54 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:54 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:54 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 09:59:54 INFO Utils: Successfully started service 'sparkDriver' on port 40390.
16/10/30 09:59:54 INFO SparkEnv: Registering MapOutputTracker
16/10/30 09:59:54 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 09:59:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b9d3c959-cf51-4b27-b01e-fbeeb2f9791e
16/10/30 09:59:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 09:59:55 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 09:59:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 09:59:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 09:59:55 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40390/jars/jni-spark-0.1.jar with timestamp 1477817995258
16/10/30 09:59:55 INFO Executor: Starting executor ID driver on host localhost
16/10/30 09:59:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35751.
16/10/30 09:59:55 INFO NettyBlockTransferService: Server created on 192.168.0.17:35751
16/10/30 09:59:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35751)
16/10/30 09:59:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35751 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35751)
16/10/30 09:59:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35751)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 09:59:57 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817997166
16/10/30 09:59:57 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-25a9d68a-3b66-4978-8ec8-2533bef39803/userFiles-86a82919-bdd3-49b1-8be2-99a3ec4a9331/SparkJNIPi.so
16/10/30 09:59:57 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 09:59:57 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 09:59:57 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 09:59:57 INFO DAGScheduler: Parents of final stage: List()
16/10/30 09:59:57 INFO DAGScheduler: Missing parents: List()
16/10/30 09:59:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 09:59:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 09:59:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 09:59:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35751 (size: 1698.0 B, free: 366.3 MB)
16/10/30 09:59:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 09:59:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 09:59:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 09:59:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 09:59:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 09:59:57 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 09:59:57 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477817997166
16/10/30 09:59:57 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-25a9d68a-3b66-4978-8ec8-2533bef39803/userFiles-86a82919-bdd3-49b1-8be2-99a3ec4a9331/SparkJNIPi.so
16/10/30 09:59:57 INFO Executor: Fetching spark://192.168.0.17:40390/jars/jni-spark-0.1.jar with timestamp 1477817995258
16/10/30 09:59:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40390 after 42 ms (0 ms spent in bootstraps)
16/10/30 09:59:57 INFO Utils: Fetching spark://192.168.0.17:40390/jars/jni-spark-0.1.jar to /tmp/spark-25a9d68a-3b66-4978-8ec8-2533bef39803/userFiles-86a82919-bdd3-49b1-8be2-99a3ec4a9331/fetchFileTemp7561852077968739442.tmp
16/10/30 09:59:57 INFO Executor: Adding file:/tmp/spark-25a9d68a-3b66-4978-8ec8-2533bef39803/userFiles-86a82919-bdd3-49b1-8be2-99a3ec4a9331/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f91c99f39dd, pid=23673, tid=0x00007f923a8ee700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid23673.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 23673 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 09:59:59 INFO SparkContext: Running Spark version 2.0.1
16/10/30 09:59:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 09:59:59 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 09:59:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 09:59:59 INFO SecurityManager: Changing view acls to: tudor
16/10/30 09:59:59 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 09:59:59 INFO SecurityManager: Changing view acls groups to: 
16/10/30 09:59:59 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 09:59:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:00:00 INFO Utils: Successfully started service 'sparkDriver' on port 35962.
16/10/30 10:00:00 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:00:00 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:00:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a5967792-1b39-4559-b8a8-9d551a66048f
16/10/30 10:00:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:00:00 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:00:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:00:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:00:00 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35962/jars/jni-spark-0.1.jar with timestamp 1477818000539
16/10/30 10:00:00 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:00:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43221.
16/10/30 10:00:00 INFO NettyBlockTransferService: Server created on 192.168.0.17:43221
16/10/30 10:00:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43221)
16/10/30 10:00:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43221 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43221)
16/10/30 10:00:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43221)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:00:02 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818002543
16/10/30 10:00:02 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-77556bed-f518-4101-9f0f-de2d37ec951d/userFiles-c262a569-457a-4689-9685-d27afe5a442e/SparkJNIPi.so
16/10/30 10:00:02 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:00:02 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:00:02 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:00:02 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:00:02 INFO DAGScheduler: Missing parents: List()
16/10/30 10:00:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:00:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:00:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:00:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43221 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:00:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:00:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:00:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:00:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:00:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:00:03 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818002543
16/10/30 10:00:03 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-77556bed-f518-4101-9f0f-de2d37ec951d/userFiles-c262a569-457a-4689-9685-d27afe5a442e/SparkJNIPi.so
16/10/30 10:00:03 INFO Executor: Fetching spark://192.168.0.17:35962/jars/jni-spark-0.1.jar with timestamp 1477818000539
16/10/30 10:00:03 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35962 after 43 ms (0 ms spent in bootstraps)
16/10/30 10:00:03 INFO Utils: Fetching spark://192.168.0.17:35962/jars/jni-spark-0.1.jar to /tmp/spark-77556bed-f518-4101-9f0f-de2d37ec951d/userFiles-c262a569-457a-4689-9685-d27afe5a442e/fetchFileTemp731904115492272440.tmp
16/10/30 10:00:03 INFO Executor: Adding file:/tmp/spark-77556bed-f518-4101-9f0f-de2d37ec951d/userFiles-c262a569-457a-4689-9685-d27afe5a442e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:00:04 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:00:04 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:43221 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:00:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:00:04 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43221 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:00:04 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:00:04 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:43221 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:00:04 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:00:05 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:43221 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:00:05 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:43221 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:00:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2002 ms on localhost (1/2)
16/10/30 10:00:05 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2010 ms on localhost (2/2)
16/10/30 10:00:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:00:05 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 2.087 s
16/10/30 10:00:05 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.355576 s
Result: 3.1412072 in 2.652 seconds
16/10/30 10:00:05 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:00:05 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:00:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:00:05 INFO MemoryStore: MemoryStore cleared
16/10/30 10:00:05 INFO BlockManager: BlockManager stopped
16/10/30 10:00:05 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:00:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:00:05 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:00:05 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:00:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-77556bed-f518-4101-9f0f-de2d37ec951d
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:00:06 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:00:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:00:06 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:00:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:00:07 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:00:07 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:00:07 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:00:07 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:00:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:00:07 INFO Utils: Successfully started service 'sparkDriver' on port 34769.
16/10/30 10:00:07 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:00:07 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:00:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd400768-6d30-4a56-8531-1ecc9190c7b5
16/10/30 10:00:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:00:07 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:00:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:00:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:00:08 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34769/jars/jni-spark-0.1.jar with timestamp 1477818008033
16/10/30 10:00:08 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:00:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34935.
16/10/30 10:00:08 INFO NettyBlockTransferService: Server created on 192.168.0.17:34935
16/10/30 10:00:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34935)
16/10/30 10:00:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34935 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34935)
16/10/30 10:00:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34935)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:00:10 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818010533
16/10/30 10:00:10 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2d1c3419-cf89-483d-887d-550a68009955/userFiles-161067ce-383f-449e-ad35-4eead7c77d34/SparkJNIPi.so
16/10/30 10:00:10 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:00:10 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:00:10 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:00:10 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:00:10 INFO DAGScheduler: Missing parents: List()
16/10/30 10:00:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:00:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:00:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:00:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34935 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:00:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:00:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:00:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:00:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:11 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:11 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:00:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:00:11 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818010533
16/10/30 10:00:11 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2d1c3419-cf89-483d-887d-550a68009955/userFiles-161067ce-383f-449e-ad35-4eead7c77d34/SparkJNIPi.so
16/10/30 10:00:11 INFO Executor: Fetching spark://192.168.0.17:34769/jars/jni-spark-0.1.jar with timestamp 1477818008033
16/10/30 10:00:11 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34769 after 30 ms (0 ms spent in bootstraps)
16/10/30 10:00:11 INFO Utils: Fetching spark://192.168.0.17:34769/jars/jni-spark-0.1.jar to /tmp/spark-2d1c3419-cf89-483d-887d-550a68009955/userFiles-161067ce-383f-449e-ad35-4eead7c77d34/fetchFileTemp7041452257449921335.tmp
16/10/30 10:00:11 INFO Executor: Adding file:/tmp/spark-2d1c3419-cf89-483d-887d-550a68009955/userFiles-161067ce-383f-449e-ad35-4eead7c77d34/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:00:13 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:00:13 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:00:13 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:34935 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:00:13 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:34935 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:00:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:00:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:00:13 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34935 after 8 ms (0 ms spent in bootstraps)
16/10/30 10:00:13 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:34935 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:00:13 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:34935 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:00:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2186 ms on localhost (1/2)
16/10/30 10:00:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2200 ms on localhost (2/2)
16/10/30 10:00:13 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 2.273 s
16/10/30 10:00:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:00:13 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.550726 s
Result: 3.1422725 in 2.826 seconds
16/10/30 10:00:13 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:00:13 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:00:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:00:13 INFO MemoryStore: MemoryStore cleared
16/10/30 10:00:13 INFO BlockManager: BlockManager stopped
16/10/30 10:00:13 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:00:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:00:13 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:00:13 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:00:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-2d1c3419-cf89-483d-887d-550a68009955
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:00:14 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:00:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:00:15 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:00:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:00:15 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:00:15 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:00:15 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:00:15 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:00:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:00:15 INFO Utils: Successfully started service 'sparkDriver' on port 39363.
16/10/30 10:00:15 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:00:15 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:00:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e8602765-fe28-413d-9b06-9b05409663b9
16/10/30 10:00:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:00:15 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:00:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:00:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:00:15 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39363/jars/jni-spark-0.1.jar with timestamp 1477818015946
16/10/30 10:00:16 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:00:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43696.
16/10/30 10:00:16 INFO NettyBlockTransferService: Server created on 192.168.0.17:43696
16/10/30 10:00:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43696)
16/10/30 10:00:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43696 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43696)
16/10/30 10:00:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43696)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:00:18 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818018091
16/10/30 10:00:18 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8fe3dc21-276b-484e-9c4e-0542fe3e5f60/userFiles-5fbdc529-a98c-4fbd-a4e4-9280d15c84f0/SparkJNIPi.so
16/10/30 10:00:18 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:00:18 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:00:18 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:00:18 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:00:18 INFO DAGScheduler: Missing parents: List()
16/10/30 10:00:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:00:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:00:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:00:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43696 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:00:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:00:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:00:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:00:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:18 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:00:18 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:00:18 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818018091
16/10/30 10:00:18 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8fe3dc21-276b-484e-9c4e-0542fe3e5f60/userFiles-5fbdc529-a98c-4fbd-a4e4-9280d15c84f0/SparkJNIPi.so
16/10/30 10:00:18 INFO Executor: Fetching spark://192.168.0.17:39363/jars/jni-spark-0.1.jar with timestamp 1477818015946
16/10/30 10:00:18 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39363 after 34 ms (0 ms spent in bootstraps)
16/10/30 10:00:18 INFO Utils: Fetching spark://192.168.0.17:39363/jars/jni-spark-0.1.jar to /tmp/spark-8fe3dc21-276b-484e-9c4e-0542fe3e5f60/userFiles-5fbdc529-a98c-4fbd-a4e4-9280d15c84f0/fetchFileTemp3265316191787630132.tmp
16/10/30 10:00:18 INFO Executor: Adding file:/tmp/spark-8fe3dc21-276b-484e-9c4e-0542fe3e5f60/userFiles-5fbdc529-a98c-4fbd-a4e4-9280d15c84f0/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:00:20 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:00:20 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:43696 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:00:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:00:20 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43696 after 8 ms (0 ms spent in bootstraps)
16/10/30 10:00:20 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:00:20 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:43696 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:00:20 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:00:20 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:43696 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:00:20 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:43696 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:00:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2035 ms on localhost (1/2)
16/10/30 10:00:20 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2041 ms on localhost (2/2)
16/10/30 10:00:20 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 2.108 s
16/10/30 10:00:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:00:20 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.363764 s
Result: 3.1420975 in 2.651 seconds
16/10/30 10:00:20 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:00:20 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:00:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.17:43696 in memory (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:00:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:00:20 INFO MemoryStore: MemoryStore cleared
16/10/30 10:00:20 INFO BlockManager: BlockManager stopped
16/10/30 10:00:20 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:00:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:00:20 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:00:20 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:00:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-8fe3dc21-276b-484e-9c4e-0542fe3e5f60
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:00:22 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:00:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:00:22 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:00:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:00:22 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:00:22 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:00:22 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:00:22 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:00:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:00:22 INFO Utils: Successfully started service 'sparkDriver' on port 46495.
16/10/30 10:00:22 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:00:22 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:00:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7dc433ae-9e5c-4b12-a335-fe10e3667268
16/10/30 10:00:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:00:22 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:00:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:00:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:00:23 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46495/jars/jni-spark-0.1.jar with timestamp 1477818023175
16/10/30 10:00:23 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:00:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37555.
16/10/30 10:00:23 INFO NettyBlockTransferService: Server created on 192.168.0.17:37555
16/10/30 10:00:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37555)
16/10/30 10:00:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37555 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37555)
16/10/30 10:00:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37555)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:00:24 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818024908
16/10/30 10:00:24 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d4ac5ed6-0e42-4ada-852b-9545524c0e68/userFiles-753ca89e-ccc5-4482-8798-5f556b8b04b0/SparkJNIPi.so
16/10/30 10:00:25 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:00:25 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:00:25 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:00:25 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:00:25 INFO DAGScheduler: Missing parents: List()
16/10/30 10:00:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:00:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:00:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:00:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37555 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:00:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:00:25 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:00:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:00:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:00:25 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:00:25 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818024908
16/10/30 10:00:25 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d4ac5ed6-0e42-4ada-852b-9545524c0e68/userFiles-753ca89e-ccc5-4482-8798-5f556b8b04b0/SparkJNIPi.so
16/10/30 10:00:25 INFO Executor: Fetching spark://192.168.0.17:46495/jars/jni-spark-0.1.jar with timestamp 1477818023175
16/10/30 10:00:25 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46495 after 39 ms (0 ms spent in bootstraps)
16/10/30 10:00:25 INFO Utils: Fetching spark://192.168.0.17:46495/jars/jni-spark-0.1.jar to /tmp/spark-d4ac5ed6-0e42-4ada-852b-9545524c0e68/userFiles-753ca89e-ccc5-4482-8798-5f556b8b04b0/fetchFileTemp4176161284609618744.tmp
16/10/30 10:00:25 INFO Executor: Adding file:/tmp/spark-d4ac5ed6-0e42-4ada-852b-9545524c0e68/userFiles-753ca89e-ccc5-4482-8798-5f556b8b04b0/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f8a56393f30, pid=24111, tid=0x00007f8a63cfe700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid24111.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 24111 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:00:28 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:00:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:00:29 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:00:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:00:29 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:00:29 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:00:29 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:00:29 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:00:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:00:29 INFO Utils: Successfully started service 'sparkDriver' on port 40950.
16/10/30 10:00:29 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:00:29 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:00:29 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7605bb8c-b462-4546-b710-5e70de35c281
16/10/30 10:00:29 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:00:29 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:00:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:00:30 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:00:30 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40950/jars/jni-spark-0.1.jar with timestamp 1477818030088
16/10/30 10:00:30 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:00:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42648.
16/10/30 10:00:30 INFO NettyBlockTransferService: Server created on 192.168.0.17:42648
16/10/30 10:00:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42648)
16/10/30 10:00:30 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42648 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42648)
16/10/30 10:00:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42648)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:00:31 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818031782
16/10/30 10:00:31 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-01401fc0-7dfd-4fad-b950-7851d2b11b56/userFiles-1d104506-6a3f-4f1b-b5eb-06cccbb4ead4/SparkJNIPi.so
16/10/30 10:00:32 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:00:32 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:00:32 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:00:32 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:00:32 INFO DAGScheduler: Missing parents: List()
16/10/30 10:00:32 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:00:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:00:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:00:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42648 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:00:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:00:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:00:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:00:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:32 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:00:32 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:00:32 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818031782
16/10/30 10:00:32 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-01401fc0-7dfd-4fad-b950-7851d2b11b56/userFiles-1d104506-6a3f-4f1b-b5eb-06cccbb4ead4/SparkJNIPi.so
16/10/30 10:00:32 INFO Executor: Fetching spark://192.168.0.17:40950/jars/jni-spark-0.1.jar with timestamp 1477818030088
16/10/30 10:00:32 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40950 after 32 ms (0 ms spent in bootstraps)
16/10/30 10:00:32 INFO Utils: Fetching spark://192.168.0.17:40950/jars/jni-spark-0.1.jar to /tmp/spark-01401fc0-7dfd-4fad-b950-7851d2b11b56/userFiles-1d104506-6a3f-4f1b-b5eb-06cccbb4ead4/fetchFileTemp3410286355763196650.tmp
16/10/30 10:00:32 INFO Executor: Adding file:/tmp/spark-01401fc0-7dfd-4fad-b950-7851d2b11b56/userFiles-1d104506-6a3f-4f1b-b5eb-06cccbb4ead4/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:00:42 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:00:42 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:42648 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:00:42 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 10:00:42 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42648 after 4 ms (0 ms spent in bootstraps)
16/10/30 10:00:42 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:42648 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:00:42 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 10310 ms on localhost (1/2)
Calling method randToSum
16/10/30 10:00:43 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:00:43 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:42648 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:00:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 10:00:44 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:42648 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:00:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 11985 ms on localhost (2/2)
16/10/30 10:00:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:00:44 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 12.220 s
16/10/30 10:00:44 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 12.483821 s
Result: 3.1417077 in 12.808 seconds
16/10/30 10:00:44 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:00:44 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:00:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:00:44 INFO MemoryStore: MemoryStore cleared
16/10/30 10:00:44 INFO BlockManager: BlockManager stopped
16/10/30 10:00:44 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:00:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:00:44 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:00:44 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:00:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-01401fc0-7dfd-4fad-b950-7851d2b11b56
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:00:45 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:00:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:00:46 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:00:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:00:46 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:00:46 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:00:46 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:00:46 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:00:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:00:46 INFO Utils: Successfully started service 'sparkDriver' on port 37293.
16/10/30 10:00:46 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:00:46 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:00:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-855d36e6-14f8-4a25-a7e9-b32b67d6c32f
16/10/30 10:00:46 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:00:46 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:00:47 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:00:47 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:00:47 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37293/jars/jni-spark-0.1.jar with timestamp 1477818047345
16/10/30 10:00:47 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:00:47 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39241.
16/10/30 10:00:47 INFO NettyBlockTransferService: Server created on 192.168.0.17:39241
16/10/30 10:00:47 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39241)
16/10/30 10:00:47 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39241 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39241)
16/10/30 10:00:47 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39241)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:00:49 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818049878
16/10/30 10:00:49 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-274f8125-52de-47a0-a85e-a57a99e2f453/userFiles-c30c3f9a-8a14-4a6f-ba50-20a69330e943/SparkJNIPi.so
16/10/30 10:00:50 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:00:50 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:00:50 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:00:50 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:00:50 INFO DAGScheduler: Missing parents: List()
16/10/30 10:00:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:00:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:00:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:00:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39241 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:00:50 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:00:50 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:00:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:00:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:50 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:00:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:00:50 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:00:50 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818049878
16/10/30 10:00:50 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-274f8125-52de-47a0-a85e-a57a99e2f453/userFiles-c30c3f9a-8a14-4a6f-ba50-20a69330e943/SparkJNIPi.so
16/10/30 10:00:50 INFO Executor: Fetching spark://192.168.0.17:37293/jars/jni-spark-0.1.jar with timestamp 1477818047345
16/10/30 10:00:50 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37293 after 37 ms (0 ms spent in bootstraps)
16/10/30 10:00:50 INFO Utils: Fetching spark://192.168.0.17:37293/jars/jni-spark-0.1.jar to /tmp/spark-274f8125-52de-47a0-a85e-a57a99e2f453/userFiles-c30c3f9a-8a14-4a6f-ba50-20a69330e943/fetchFileTemp8631011213316235986.tmp
16/10/30 10:00:50 INFO Executor: Adding file:/tmp/spark-274f8125-52de-47a0-a85e-a57a99e2f453/userFiles-c30c3f9a-8a14-4a6f-ba50-20a69330e943/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
[thread 139745979868928 also had an error]
#  SIGSEGV (0xb) at pc=0x00007f194809f9f2, pid=24327, tid=0x00007f1924ec4700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x6cf9f2]  jni_SetIntArrayRegion+0xc2
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid24327.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
./runTimesModes.sh: line 9: 24327 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:01:03 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:01:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:01:03 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:01:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:01:03 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:01:03 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:01:03 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:01:03 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:01:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:01:04 INFO Utils: Successfully started service 'sparkDriver' on port 39971.
16/10/30 10:01:04 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:01:04 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:01:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ff5e78be-f010-4cec-98de-b00489980820
16/10/30 10:01:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:01:04 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:01:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:01:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:01:04 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39971/jars/jni-spark-0.1.jar with timestamp 1477818064637
16/10/30 10:01:04 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:01:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41513.
16/10/30 10:01:04 INFO NettyBlockTransferService: Server created on 192.168.0.17:41513
16/10/30 10:01:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41513)
16/10/30 10:01:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41513 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41513)
16/10/30 10:01:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41513)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:01:06 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818066526
16/10/30 10:01:06 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-94df1c47-6f04-425a-906f-ca98af05f86a/userFiles-9ab4aff7-5a6d-412c-9d31-d51e023b7219/SparkJNIPi.so
16/10/30 10:01:06 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:01:06 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:01:06 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:01:06 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:01:06 INFO DAGScheduler: Missing parents: List()
16/10/30 10:01:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:01:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:01:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:01:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41513 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:01:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:01:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:01:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:01:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:01:07 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:01:07 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:01:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:01:07 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818066526
16/10/30 10:01:07 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-94df1c47-6f04-425a-906f-ca98af05f86a/userFiles-9ab4aff7-5a6d-412c-9d31-d51e023b7219/SparkJNIPi.so
16/10/30 10:01:07 INFO Executor: Fetching spark://192.168.0.17:39971/jars/jni-spark-0.1.jar with timestamp 1477818064637
16/10/30 10:01:07 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39971 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:01:07 INFO Utils: Fetching spark://192.168.0.17:39971/jars/jni-spark-0.1.jar to /tmp/spark-94df1c47-6f04-425a-906f-ca98af05f86a/userFiles-9ab4aff7-5a6d-412c-9d31-d51e023b7219/fetchFileTemp5474335652206331208.tmp
16/10/30 10:01:07 INFO Executor: Adding file:/tmp/spark-94df1c47-6f04-425a-906f-ca98af05f86a/userFiles-9ab4aff7-5a6d-412c-9d31-d51e023b7219/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:01:22 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:23 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:01:23 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:41513 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:01:23 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 10:01:23 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:23 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:01:23 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:01:23 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:01:23 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
16/10/30 10:01:23 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41513 after 7 ms (0 ms spent in bootstraps)
16/10/30 10:01:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:01:23 INFO TaskSchedulerImpl: Cancelling stage 0
16/10/30 10:01:23 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 16.173 s
16/10/30 10:01:23 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@31def2b1)
16/10/30 10:01:23 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1477818083292,JobFailed(org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:))
16/10/30 10:01:23 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 16.441625 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:23 ERROR Utils: Uncaught exception in thread task-result-getter-1
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:190)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:104)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:555)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Exception in thread "task-result-getter-1" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1148)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:190)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:104)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:555)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	... 2 more
16/10/30 10:01:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:01:23 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.0.17:41513 is closed
16/10/30 10:01:23 INFO RetryingBlockFetcher: Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
16/10/30 10:01:23 INFO MemoryStore: MemoryStore cleared
16/10/30 10:01:23 INFO BlockManager: BlockManager stopped
16/10/30 10:01:23 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:01:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:01:23 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:01:23 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:01:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-94df1c47-6f04-425a-906f-ca98af05f86a
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:01:25 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:01:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:01:26 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:01:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:01:26 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:01:26 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:01:26 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:01:26 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:01:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:01:26 INFO Utils: Successfully started service 'sparkDriver' on port 44617.
16/10/30 10:01:26 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:01:26 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:01:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7465448c-5ad4-4b42-ba3d-d7e4d45c0adb
16/10/30 10:01:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:01:26 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:01:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:01:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:01:27 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44617/jars/jni-spark-0.1.jar with timestamp 1477818087415
16/10/30 10:01:27 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:01:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42319.
16/10/30 10:01:27 INFO NettyBlockTransferService: Server created on 192.168.0.17:42319
16/10/30 10:01:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42319)
16/10/30 10:01:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42319 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42319)
16/10/30 10:01:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42319)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:01:30 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818090334
16/10/30 10:01:30 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-4a26fd38-6f5f-4ef7-97ac-3c78098a32de/userFiles-d4f19240-4359-484c-bbfc-eec5f1463bd5/SparkJNIPi.so
16/10/30 10:01:30 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:01:30 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:01:30 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:01:30 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:01:30 INFO DAGScheduler: Missing parents: List()
16/10/30 10:01:30 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:01:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:01:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:01:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42319 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:01:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:01:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:01:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:01:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:01:31 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:01:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:01:31 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:01:31 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818090334
16/10/30 10:01:31 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-4a26fd38-6f5f-4ef7-97ac-3c78098a32de/userFiles-d4f19240-4359-484c-bbfc-eec5f1463bd5/SparkJNIPi.so
16/10/30 10:01:31 INFO Executor: Fetching spark://192.168.0.17:44617/jars/jni-spark-0.1.jar with timestamp 1477818087415
16/10/30 10:01:31 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44617 after 40 ms (0 ms spent in bootstraps)
16/10/30 10:01:31 INFO Utils: Fetching spark://192.168.0.17:44617/jars/jni-spark-0.1.jar to /tmp/spark-4a26fd38-6f5f-4ef7-97ac-3c78098a32de/userFiles-d4f19240-4359-484c-bbfc-eec5f1463bd5/fetchFileTemp6923480588322407242.tmp
16/10/30 10:01:31 INFO Executor: Adding file:/tmp/spark-4a26fd38-6f5f-4ef7-97ac-3c78098a32de/userFiles-d4f19240-4359-484c-bbfc-eec5f1463bd5/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:01:47 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:47 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-1,5,main]
java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:47 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:01:47 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:42319 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:01:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 134874250 bytes result sent via BlockManager)
16/10/30 10:01:47 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:01:47 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:01:47 ERROR TaskSetManager: Task 1 in stage 0.0 failed 1 times; aborting job
16/10/30 10:01:47 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:01:47 INFO TaskSchedulerImpl: Cancelling stage 0
16/10/30 10:01:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:01:47 INFO TaskSchedulerImpl: Stage 0 was cancelled
16/10/30 10:01:47 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 16.545 s
16/10/30 10:01:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@4f870599)
16/10/30 10:01:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42319 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:01:47 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 17.071571 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:47 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1477818107973,JobFailed(org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3236)
	at java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)
	at java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)
	at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)
	at org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)
	at java.io.ObjectOutputStream$BlockDataOutputStream.write(ObjectOutputStream.java:1853)
	at java.io.ObjectOutputStream.write(ObjectOutputStream.java:709)
	at org.apache.spark.util.Utils$.writeByteBuffer(Utils.scala:234)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply$mcV$sp(TaskResult.scala:49)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.scheduler.DirectTaskResult$$anonfun$writeExternal$1.apply(TaskResult.scala:47)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:47)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:43)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:100)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:329)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:))
16/10/30 10:01:48 ERROR Utils: Uncaught exception in thread task-result-getter-1
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:190)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:104)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:555)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Exception in thread "task-result-getter-1" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1148)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:202)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:190)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:104)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:555)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	... 2 more
16/10/30 10:01:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:01:48 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /192.168.0.17:42319 is closed
16/10/30 10:01:48 ERROR OneForOneBlockFetcher: Failed while starting block fetches
java.io.IOException: Connection from /192.168.0.17:42319 closed
	at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:128)
	at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:109)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:257)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:208)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:194)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:828)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:621)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:357)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:357)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:48 INFO RetryingBlockFetcher: Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
16/10/30 10:01:48 INFO MemoryStore: MemoryStore cleared
16/10/30 10:01:48 INFO BlockManager: BlockManager stopped
16/10/30 10:01:48 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:01:48 ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=5648042643115511432, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=13 cap=13]}} to /192.168.0.17:43548; closing connection
java.io.IOException: Broken pipe
	at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
	at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
	at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
	at sun.nio.ch.IOUtil.write(IOUtil.java:65)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:471)
	at org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:141)
	at org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:123)
	at io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:254)
	at io.netty.channel.nio.AbstractNioByteChannel.doWrite(AbstractNioByteChannel.java:237)
	at io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:281)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:761)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:311)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:729)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1127)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelOutboundHandlerAdapter.flush(ChannelOutboundHandlerAdapter.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:644)
	at io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:117)
	at io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:663)
	at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:693)
	at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:681)
	at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:716)
	at io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:954)
	at io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:244)
	at org.apache.spark.network.server.TransportRequestHandler.respond(TransportRequestHandler.java:194)
	at org.apache.spark.network.server.TransportRequestHandler.access$000(TransportRequestHandler.java:55)
	at org.apache.spark.network.server.TransportRequestHandler$1.onSuccess(TransportRequestHandler.java:162)
	at org.apache.spark.network.netty.NettyBlockRpcServer.receive(NettyBlockRpcServer.scala:63)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:159)
	at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:119)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:51)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:266)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:01:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:01:48 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:01:48 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:01:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-4a26fd38-6f5f-4ef7-97ac-3c78098a32de
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:01:50 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:01:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:01:50 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:01:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:01:50 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:01:50 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:01:50 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:01:50 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:01:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:01:51 INFO Utils: Successfully started service 'sparkDriver' on port 41633.
16/10/30 10:01:51 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:01:51 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:01:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-46deeb08-d548-446e-90ae-0cce9fe89de8
16/10/30 10:01:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:01:51 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:01:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:01:51 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:01:51 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41633/jars/jni-spark-0.1.jar with timestamp 1477818111887
16/10/30 10:01:52 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:01:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36440.
16/10/30 10:01:52 INFO NettyBlockTransferService: Server created on 192.168.0.17:36440
16/10/30 10:01:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36440)
16/10/30 10:01:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36440 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36440)
16/10/30 10:01:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36440)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:01:54 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818114434
16/10/30 10:01:54 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2cb29e95-9ac6-4cb5-99f7-f31819980002/userFiles-de49c39f-6ffe-4cb7-a4aa-a7645d23ecdd/SparkJNIPi.so
16/10/30 10:01:54 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:01:54 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:01:54 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:01:54 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:01:54 INFO DAGScheduler: Missing parents: List()
16/10/30 10:01:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:01:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:01:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:01:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36440 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:01:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:01:55 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:01:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:01:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:01:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:01:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:01:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:01:55 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818114434
16/10/30 10:01:55 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2cb29e95-9ac6-4cb5-99f7-f31819980002/userFiles-de49c39f-6ffe-4cb7-a4aa-a7645d23ecdd/SparkJNIPi.so
16/10/30 10:01:55 INFO Executor: Fetching spark://192.168.0.17:41633/jars/jni-spark-0.1.jar with timestamp 1477818111887
16/10/30 10:01:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41633 after 56 ms (0 ms spent in bootstraps)
16/10/30 10:01:55 INFO Utils: Fetching spark://192.168.0.17:41633/jars/jni-spark-0.1.jar to /tmp/spark-2cb29e95-9ac6-4cb5-99f7-f31819980002/userFiles-de49c39f-6ffe-4cb7-a4aa-a7645d23ecdd/fetchFileTemp7292485781703263620.tmp
16/10/30 10:01:55 INFO Executor: Adding file:/tmp/spark-2cb29e95-9ac6-4cb5-99f7-f31819980002/userFiles-de49c39f-6ffe-4cb7-a4aa-a7645d23ecdd/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:01:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 5205 bytes result sent to driver
16/10/30 10:01:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 862 ms on localhost (1/2)
16/10/30 10:01:56 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 10:01:56 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.978 s
16/10/30 10:01:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 853 ms on localhost (2/2)
16/10/30 10:01:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:01:56 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.350659 s
Result: 3.1582031 in 1.797 seconds
16/10/30 10:01:56 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:01:56 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:01:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:01:56 INFO MemoryStore: MemoryStore cleared
16/10/30 10:01:56 INFO BlockManager: BlockManager stopped
16/10/30 10:01:56 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:01:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:01:56 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:01:56 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:01:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-2cb29e95-9ac6-4cb5-99f7-f31819980002
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:01:58 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:01:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:01:58 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:01:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:01:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:01:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:01:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:01:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:01:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:01:59 INFO Utils: Successfully started service 'sparkDriver' on port 34906.
16/10/30 10:01:59 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:01:59 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:01:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0ed916a0-bdfa-4bdc-98ac-5123032484a1
16/10/30 10:01:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:01:59 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:01:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:01:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:01:59 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34906/jars/jni-spark-0.1.jar with timestamp 1477818119663
16/10/30 10:01:59 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:01:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44450.
16/10/30 10:01:59 INFO NettyBlockTransferService: Server created on 192.168.0.17:44450
16/10/30 10:01:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44450)
16/10/30 10:01:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44450 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44450)
16/10/30 10:01:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44450)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:01 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818121836
16/10/30 10:02:01 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-dfee6ec6-d5f3-49a9-b406-1d2419e17dea/userFiles-dba2d02b-553b-44eb-b07b-66dae8c1b71e/SparkJNIPi.so
16/10/30 10:02:02 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:02 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:02 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:02 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:02 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44450 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:02 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:02 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:02 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818121836
16/10/30 10:02:02 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-dfee6ec6-d5f3-49a9-b406-1d2419e17dea/userFiles-dba2d02b-553b-44eb-b07b-66dae8c1b71e/SparkJNIPi.so
16/10/30 10:02:02 INFO Executor: Fetching spark://192.168.0.17:34906/jars/jni-spark-0.1.jar with timestamp 1477818119663
16/10/30 10:02:02 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34906 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:02:02 INFO Utils: Fetching spark://192.168.0.17:34906/jars/jni-spark-0.1.jar to /tmp/spark-dfee6ec6-d5f3-49a9-b406-1d2419e17dea/userFiles-dba2d02b-553b-44eb-b07b-66dae8c1b71e/fetchFileTemp1660661215642279761.tmp
16/10/30 10:02:02 INFO Executor: Adding file:/tmp/spark-dfee6ec6-d5f3-49a9-b406-1d2419e17dea/userFiles-dba2d02b-553b-44eb-b07b-66dae8c1b71e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 24942 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:04 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:04 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:04 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:04 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:04 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:04 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:05 INFO Utils: Successfully started service 'sparkDriver' on port 45327.
16/10/30 10:02:05 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:05 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b966a509-3f92-4989-a1ca-145b066b2167
16/10/30 10:02:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:05 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:05 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45327/jars/jni-spark-0.1.jar with timestamp 1477818125876
16/10/30 10:02:05 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36897.
16/10/30 10:02:06 INFO NettyBlockTransferService: Server created on 192.168.0.17:36897
16/10/30 10:02:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36897)
16/10/30 10:02:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36897 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36897)
16/10/30 10:02:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36897)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:08 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818128944
16/10/30 10:02:08 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-3297ea77-8e6a-4b75-805c-f6a59af5ef40/userFiles-a6e02701-4782-4914-86a4-a6590b77597e/SparkJNIPi.so
16/10/30 10:02:09 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:09 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:09 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:09 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:09 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:09 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36897 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:09 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:09 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:09 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:09 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818128944
16/10/30 10:02:09 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-3297ea77-8e6a-4b75-805c-f6a59af5ef40/userFiles-a6e02701-4782-4914-86a4-a6590b77597e/SparkJNIPi.so
16/10/30 10:02:09 INFO Executor: Fetching spark://192.168.0.17:45327/jars/jni-spark-0.1.jar with timestamp 1477818125876
16/10/30 10:02:09 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45327 after 22 ms (0 ms spent in bootstraps)
16/10/30 10:02:09 INFO Utils: Fetching spark://192.168.0.17:45327/jars/jni-spark-0.1.jar to /tmp/spark-3297ea77-8e6a-4b75-805c-f6a59af5ef40/userFiles-a6e02701-4782-4914-86a4-a6590b77597e/fetchFileTemp4675085640397412891.tmp
16/10/30 10:02:09 INFO Executor: Adding file:/tmp/spark-3297ea77-8e6a-4b75-805c-f6a59af5ef40/userFiles-a6e02701-4782-4914-86a4-a6590b77597e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7fb44ccf30, pid=25039, tid=0x00007f7fbc55d700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25039.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 25039 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:10 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:11 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:11 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:11 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:11 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:11 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:11 INFO Utils: Successfully started service 'sparkDriver' on port 45615.
16/10/30 10:02:11 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:11 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8f82b4d4-770a-4ad6-b876-ce5efa0e462d
16/10/30 10:02:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:11 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:12 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:12 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:12 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45615/jars/jni-spark-0.1.jar with timestamp 1477818132132
16/10/30 10:02:12 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40492.
16/10/30 10:02:12 INFO NettyBlockTransferService: Server created on 192.168.0.17:40492
16/10/30 10:02:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40492)
16/10/30 10:02:12 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40492 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40492)
16/10/30 10:02:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40492)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:14 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818134225
16/10/30 10:02:14 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-dcbb89bc-4c93-4645-a1ba-4446736b7d03/userFiles-e19bb487-4e1e-4a95-8a05-b1b81ba79861/SparkJNIPi.so
16/10/30 10:02:14 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:14 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:14 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:14 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:14 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:14 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40492 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:14 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:15 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:15 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818134225
16/10/30 10:02:15 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-dcbb89bc-4c93-4645-a1ba-4446736b7d03/userFiles-e19bb487-4e1e-4a95-8a05-b1b81ba79861/SparkJNIPi.so
16/10/30 10:02:15 INFO Executor: Fetching spark://192.168.0.17:45615/jars/jni-spark-0.1.jar with timestamp 1477818132132
16/10/30 10:02:15 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45615 after 42 ms (0 ms spent in bootstraps)
16/10/30 10:02:15 INFO Utils: Fetching spark://192.168.0.17:45615/jars/jni-spark-0.1.jar to /tmp/spark-dcbb89bc-4c93-4645-a1ba-4446736b7d03/userFiles-e19bb487-4e1e-4a95-8a05-b1b81ba79861/fetchFileTemp8741490377673989670.tmp
16/10/30 10:02:15 INFO Executor: Adding file:/tmp/spark-dcbb89bc-4c93-4645-a1ba-4446736b7d03/userFiles-e19bb487-4e1e-4a95-8a05-b1b81ba79861/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7b6b2709dd, pid=25153, tid=0x00007f7b732ef700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25153.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 25153 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:16 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:16 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:16 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:16 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:16 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:16 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:17 INFO Utils: Successfully started service 'sparkDriver' on port 36550.
16/10/30 10:02:17 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:17 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cc41bea5-31d2-423f-bbdb-f7c9a7b76477
16/10/30 10:02:17 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:17 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:17 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36550/jars/jni-spark-0.1.jar with timestamp 1477818137596
16/10/30 10:02:17 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36223.
16/10/30 10:02:17 INFO NettyBlockTransferService: Server created on 192.168.0.17:36223
16/10/30 10:02:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36223)
16/10/30 10:02:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36223 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36223)
16/10/30 10:02:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36223)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:19 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818139584
16/10/30 10:02:19 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-90704c6d-59b5-4834-a89a-45c278ce7ec5/userFiles-6beb191a-ddf1-4a55-98ae-65b4aeabb42e/SparkJNIPi.so
16/10/30 10:02:19 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:19 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:19 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:19 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:20 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36223 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:20 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:20 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:20 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818139584
16/10/30 10:02:20 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-90704c6d-59b5-4834-a89a-45c278ce7ec5/userFiles-6beb191a-ddf1-4a55-98ae-65b4aeabb42e/SparkJNIPi.so
16/10/30 10:02:20 INFO Executor: Fetching spark://192.168.0.17:36550/jars/jni-spark-0.1.jar with timestamp 1477818137596
16/10/30 10:02:20 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36550 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:02:20 INFO Utils: Fetching spark://192.168.0.17:36550/jars/jni-spark-0.1.jar to /tmp/spark-90704c6d-59b5-4834-a89a-45c278ce7ec5/userFiles-6beb191a-ddf1-4a55-98ae-65b4aeabb42e/fetchFileTemp4751622682054007431.tmp
16/10/30 10:02:20 INFO Executor: Adding file:/tmp/spark-90704c6d-59b5-4834-a89a-45c278ce7ec5/userFiles-6beb191a-ddf1-4a55-98ae-65b4aeabb42e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:02:20 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 34017 bytes result sent to driver
16/10/30 10:02:20 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 446 ms on localhost (1/2)
16/10/30 10:02:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 34017 bytes result sent to driver
16/10/30 10:02:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 522 ms on localhost (2/2)
16/10/30 10:02:20 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.537 s
16/10/30 10:02:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:02:20 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.801846 s
Result: 3.1402588 in 1.185 seconds
16/10/30 10:02:20 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:02:20 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:02:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:02:20 INFO MemoryStore: MemoryStore cleared
16/10/30 10:02:20 INFO BlockManager: BlockManager stopped
16/10/30 10:02:20 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:02:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:02:20 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:02:20 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:02:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-90704c6d-59b5-4834-a89a-45c278ce7ec5
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:22 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:22 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:22 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:22 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:22 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:22 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:22 INFO Utils: Successfully started service 'sparkDriver' on port 41205.
16/10/30 10:02:23 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:23 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-384a07e3-529e-4021-a6d5-3071cd3e4fbe
16/10/30 10:02:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:23 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:23 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41205/jars/jni-spark-0.1.jar with timestamp 1477818143366
16/10/30 10:02:23 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39506.
16/10/30 10:02:23 INFO NettyBlockTransferService: Server created on 192.168.0.17:39506
16/10/30 10:02:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39506)
16/10/30 10:02:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39506 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39506)
16/10/30 10:02:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39506)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:25 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818145882
16/10/30 10:02:25 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-82890088-3173-4d89-8270-7b77f28a0e64/userFiles-d206a98c-ed98-4b13-bf3c-13a90e281b7b/SparkJNIPi.so
16/10/30 10:02:26 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:26 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:26 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:26 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:26 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39506 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:27 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:27 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:27 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818145882
16/10/30 10:02:27 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-82890088-3173-4d89-8270-7b77f28a0e64/userFiles-d206a98c-ed98-4b13-bf3c-13a90e281b7b/SparkJNIPi.so
16/10/30 10:02:27 INFO Executor: Fetching spark://192.168.0.17:41205/jars/jni-spark-0.1.jar with timestamp 1477818143366
16/10/30 10:02:27 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41205 after 61 ms (0 ms spent in bootstraps)
16/10/30 10:02:27 INFO Utils: Fetching spark://192.168.0.17:41205/jars/jni-spark-0.1.jar to /tmp/spark-82890088-3173-4d89-8270-7b77f28a0e64/userFiles-d206a98c-ed98-4b13-bf3c-13a90e281b7b/fetchFileTemp3986739565030155116.tmp
16/10/30 10:02:27 INFO Executor: Adding file:/tmp/spark-82890088-3173-4d89-8270-7b77f28a0e64/userFiles-d206a98c-ed98-4b13-bf3c-13a90e281b7b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f66948dcf30, pid=25369, tid=0x00007f67053d0700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25369.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9: 25369 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:29 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:30 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:30 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:30 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:30 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:30 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:31 INFO Utils: Successfully started service 'sparkDriver' on port 45566.
16/10/30 10:02:31 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:31 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6447d62c-10ab-4343-bcac-bbee09d78a1e
16/10/30 10:02:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:31 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:32 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45566/jars/jni-spark-0.1.jar with timestamp 1477818152098
16/10/30 10:02:32 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35952.
16/10/30 10:02:32 INFO NettyBlockTransferService: Server created on 192.168.0.17:35952
16/10/30 10:02:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35952)
16/10/30 10:02:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35952 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35952)
16/10/30 10:02:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35952)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:35 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818155487
16/10/30 10:02:35 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-afe15b7c-238e-4977-81ef-5a4c7503faac/userFiles-d8f0f77f-1337-4713-9462-dca8c47b75e5/SparkJNIPi.so
16/10/30 10:02:35 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:35 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:35 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:35 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:35 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35952 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:36 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:36 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:36 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:36 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818155487
16/10/30 10:02:36 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-afe15b7c-238e-4977-81ef-5a4c7503faac/userFiles-d8f0f77f-1337-4713-9462-dca8c47b75e5/SparkJNIPi.so
16/10/30 10:02:36 INFO Executor: Fetching spark://192.168.0.17:45566/jars/jni-spark-0.1.jar with timestamp 1477818152098
16/10/30 10:02:36 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45566 after 58 ms (0 ms spent in bootstraps)
16/10/30 10:02:36 INFO Utils: Fetching spark://192.168.0.17:45566/jars/jni-spark-0.1.jar to /tmp/spark-afe15b7c-238e-4977-81ef-5a4c7503faac/userFiles-d8f0f77f-1337-4713-9462-dca8c47b75e5/fetchFileTemp6343242774431179456.tmp
16/10/30 10:02:36 INFO Executor: Adding file:/tmp/spark-afe15b7c-238e-4977-81ef-5a4c7503faac/userFiles-d8f0f77f-1337-4713-9462-dca8c47b75e5/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f9d65bdeb21, pid=25485, tid=0x00007f9dd5cdf700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x2b21]  pci_system_cleanup+0x71
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25485.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 25485 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:38 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:38 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:38 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:38 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:38 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:38 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:39 INFO Utils: Successfully started service 'sparkDriver' on port 38506.
16/10/30 10:02:39 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:39 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-926f75d4-9985-4f4e-9359-c01676479e6f
16/10/30 10:02:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:39 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:40 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38506/jars/jni-spark-0.1.jar with timestamp 1477818160037
16/10/30 10:02:40 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36780.
16/10/30 10:02:40 INFO NettyBlockTransferService: Server created on 192.168.0.17:36780
16/10/30 10:02:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36780)
16/10/30 10:02:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36780 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36780)
16/10/30 10:02:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36780)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:41 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818161883
16/10/30 10:02:41 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-49a43a70-c716-4b25-90da-cef4167b7e0b/userFiles-6f6fdfa5-4581-4eb2-bdd1-b3d26e86bd8b/SparkJNIPi.so
16/10/30 10:02:42 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:42 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:42 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:42 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:42 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36780 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:42 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:42 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:42 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:42 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818161883
16/10/30 10:02:42 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-49a43a70-c716-4b25-90da-cef4167b7e0b/userFiles-6f6fdfa5-4581-4eb2-bdd1-b3d26e86bd8b/SparkJNIPi.so
16/10/30 10:02:42 INFO Executor: Fetching spark://192.168.0.17:38506/jars/jni-spark-0.1.jar with timestamp 1477818160037
16/10/30 10:02:42 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38506 after 35 ms (0 ms spent in bootstraps)
16/10/30 10:02:42 INFO Utils: Fetching spark://192.168.0.17:38506/jars/jni-spark-0.1.jar to /tmp/spark-49a43a70-c716-4b25-90da-cef4167b7e0b/userFiles-6f6fdfa5-4581-4eb2-bdd1-b3d26e86bd8b/fetchFileTemp5653016990142126392.tmp
16/10/30 10:02:42 INFO Executor: Adding file:/tmp/spark-49a43a70-c716-4b25-90da-cef4167b7e0b/userFiles-6f6fdfa5-4581-4eb2-bdd1-b3d26e86bd8b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f98fbdfa013, pid=25600, tid=0x00007f99701a0700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x3013]  pci_device_unmap_region+0x23
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25600.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9: 25600 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:43 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:44 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:44 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:44 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:44 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:44 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:44 INFO Utils: Successfully started service 'sparkDriver' on port 37547.
16/10/30 10:02:44 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:44 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-de2650e9-48b8-47ed-9926-058227664636
16/10/30 10:02:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:44 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:44 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37547/jars/jni-spark-0.1.jar with timestamp 1477818164801
16/10/30 10:02:44 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39132.
16/10/30 10:02:44 INFO NettyBlockTransferService: Server created on 192.168.0.17:39132
16/10/30 10:02:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39132)
16/10/30 10:02:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39132 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39132)
16/10/30 10:02:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39132)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:46 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818166415
16/10/30 10:02:46 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-63fcb5ec-3773-48a7-9d4c-96ba4721729f/userFiles-00f9630e-c8ba-4d6a-8b15-15834fcf0eba/SparkJNIPi.so
16/10/30 10:02:46 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:46 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:46 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:46 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:46 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39132 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:46 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:46 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818166415
16/10/30 10:02:47 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-63fcb5ec-3773-48a7-9d4c-96ba4721729f/userFiles-00f9630e-c8ba-4d6a-8b15-15834fcf0eba/SparkJNIPi.so
16/10/30 10:02:47 INFO Executor: Fetching spark://192.168.0.17:37547/jars/jni-spark-0.1.jar with timestamp 1477818164801
16/10/30 10:02:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37547 after 32 ms (0 ms spent in bootstraps)
16/10/30 10:02:47 INFO Utils: Fetching spark://192.168.0.17:37547/jars/jni-spark-0.1.jar to /tmp/spark-63fcb5ec-3773-48a7-9d4c-96ba4721729f/userFiles-00f9630e-c8ba-4d6a-8b15-15834fcf0eba/fetchFileTemp1443028466443642102.tmp
16/10/30 10:02:47 INFO Executor: Adding file:/tmp/spark-63fcb5ec-3773-48a7-9d4c-96ba4721729f/userFiles-00f9630e-c8ba-4d6a-8b15-15834fcf0eba/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fc3f63cdf30, pid=25698, tid=0x00007fc3fe4e5700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25698.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 25698 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:48 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:48 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:48 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:48 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:48 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:48 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:48 INFO Utils: Successfully started service 'sparkDriver' on port 39350.
16/10/30 10:02:49 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:49 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-96e7a595-e461-4738-9393-e20fa9697302
16/10/30 10:02:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:49 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:49 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39350/jars/jni-spark-0.1.jar with timestamp 1477818169644
16/10/30 10:02:49 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41787.
16/10/30 10:02:49 INFO NettyBlockTransferService: Server created on 192.168.0.17:41787
16/10/30 10:02:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41787)
16/10/30 10:02:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41787 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41787)
16/10/30 10:02:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41787)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:51 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818171424
16/10/30 10:02:51 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-841d1895-6806-450f-bb51-e55887e78c67/userFiles-4704b1c3-384b-4585-a53f-cb6bf129ce9b/SparkJNIPi.so
16/10/30 10:02:51 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:51 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:51 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:51 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:51 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41787 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:52 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:52 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818171424
16/10/30 10:02:52 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-841d1895-6806-450f-bb51-e55887e78c67/userFiles-4704b1c3-384b-4585-a53f-cb6bf129ce9b/SparkJNIPi.so
16/10/30 10:02:52 INFO Executor: Fetching spark://192.168.0.17:39350/jars/jni-spark-0.1.jar with timestamp 1477818169644
16/10/30 10:02:52 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39350 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:02:52 INFO Utils: Fetching spark://192.168.0.17:39350/jars/jni-spark-0.1.jar to /tmp/spark-841d1895-6806-450f-bb51-e55887e78c67/userFiles-4704b1c3-384b-4585-a53f-cb6bf129ce9b/fetchFileTemp1707448297107092190.tmp
16/10/30 10:02:52 INFO Executor: Adding file:/tmp/spark-841d1895-6806-450f-bb51-e55887e78c67/userFiles-4704b1c3-384b-4585-a53f-cb6bf129ce9b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f9fcc8d99dd, pid=25797, tid=0x00007fa03d2ba700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25797.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 25797 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:53 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:53 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:53 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:53 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:53 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:53 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:53 INFO Utils: Successfully started service 'sparkDriver' on port 44459.
16/10/30 10:02:53 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:53 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fbd83943-ab2c-4841-9f2a-b1661bcc3b6f
16/10/30 10:02:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:54 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:54 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44459/jars/jni-spark-0.1.jar with timestamp 1477818174335
16/10/30 10:02:54 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:02:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43325.
16/10/30 10:02:54 INFO NettyBlockTransferService: Server created on 192.168.0.17:43325
16/10/30 10:02:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43325)
16/10/30 10:02:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43325 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43325)
16/10/30 10:02:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43325)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:02:56 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818176181
16/10/30 10:02:56 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8659be6c-6c5b-4ba2-8361-be45b0f008d0/userFiles-41f1b3ca-2dc1-4254-8940-c9e8797cf34a/SparkJNIPi.so
16/10/30 10:02:56 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:02:56 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:02:56 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:02:56 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:02:56 INFO DAGScheduler: Missing parents: List()
16/10/30 10:02:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:02:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:02:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:02:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43325 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:02:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:02:56 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:02:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:02:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:02:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:02:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:02:56 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818176181
16/10/30 10:02:56 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8659be6c-6c5b-4ba2-8361-be45b0f008d0/userFiles-41f1b3ca-2dc1-4254-8940-c9e8797cf34a/SparkJNIPi.so
16/10/30 10:02:56 INFO Executor: Fetching spark://192.168.0.17:44459/jars/jni-spark-0.1.jar with timestamp 1477818174335
16/10/30 10:02:56 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44459 after 27 ms (0 ms spent in bootstraps)
16/10/30 10:02:56 INFO Utils: Fetching spark://192.168.0.17:44459/jars/jni-spark-0.1.jar to /tmp/spark-8659be6c-6c5b-4ba2-8361-be45b0f008d0/userFiles-41f1b3ca-2dc1-4254-8940-c9e8797cf34a/fetchFileTemp8312636906086515411.tmp
16/10/30 10:02:56 INFO Executor: Adding file:/tmp/spark-8659be6c-6c5b-4ba2-8361-be45b0f008d0/userFiles-41f1b3ca-2dc1-4254-8940-c9e8797cf34a/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 25895 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:02:58 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:02:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:02:58 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:02:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:02:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:02:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:02:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:02:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:02:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:02:59 INFO Utils: Successfully started service 'sparkDriver' on port 40880.
16/10/30 10:02:59 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:02:59 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:02:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd9bff84-db4c-46d9-8131-d228c4c841c3
16/10/30 10:02:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:02:59 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:02:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:02:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:02:59 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40880/jars/jni-spark-0.1.jar with timestamp 1477818179993
16/10/30 10:03:00 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41863.
16/10/30 10:03:00 INFO NettyBlockTransferService: Server created on 192.168.0.17:41863
16/10/30 10:03:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41863)
16/10/30 10:03:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41863 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41863)
16/10/30 10:03:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41863)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:01 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818181685
16/10/30 10:03:01 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a3bf70e5-7556-48b3-9291-77429f8cd096/userFiles-c5d85125-80e9-435d-87c5-6335ab7a889b/SparkJNIPi.so
16/10/30 10:03:01 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:01 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:01 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:01 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:01 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41863 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:02 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:02 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:02 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:02 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:02 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:02 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818181685
16/10/30 10:03:02 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a3bf70e5-7556-48b3-9291-77429f8cd096/userFiles-c5d85125-80e9-435d-87c5-6335ab7a889b/SparkJNIPi.so
16/10/30 10:03:02 INFO Executor: Fetching spark://192.168.0.17:40880/jars/jni-spark-0.1.jar with timestamp 1477818179993
16/10/30 10:03:02 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40880 after 40 ms (3 ms spent in bootstraps)
16/10/30 10:03:02 INFO Utils: Fetching spark://192.168.0.17:40880/jars/jni-spark-0.1.jar to /tmp/spark-a3bf70e5-7556-48b3-9291-77429f8cd096/userFiles-c5d85125-80e9-435d-87c5-6335ab7a889b/fetchFileTemp6395094175268808029.tmp
16/10/30 10:03:02 INFO Executor: Adding file:/tmp/spark-a3bf70e5-7556-48b3-9291-77429f8cd096/userFiles-c5d85125-80e9-435d-87c5-6335ab7a889b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fc2b605d9dd, pid=25993, tid=0x00007fc326af0700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid25993.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 25993 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:03:03 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:03:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:03:04 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:03:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:03:04 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:03:04 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:03:04 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:03:04 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:03:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:03:05 INFO Utils: Successfully started service 'sparkDriver' on port 36846.
16/10/30 10:03:05 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:03:05 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:03:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-28ecb4d8-a0a3-4287-8967-cc22fbe86b2a
16/10/30 10:03:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:03:05 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:03:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:03:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:03:05 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36846/jars/jni-spark-0.1.jar with timestamp 1477818185426
16/10/30 10:03:05 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34765.
16/10/30 10:03:05 INFO NettyBlockTransferService: Server created on 192.168.0.17:34765
16/10/30 10:03:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34765)
16/10/30 10:03:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34765 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34765)
16/10/30 10:03:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34765)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:07 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818187936
16/10/30 10:03:07 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-af433c09-03cd-4e55-bd8c-e1b129199a79/userFiles-2d762a2e-235b-44c2-90cb-96933d4a6f04/SparkJNIPi.so
16/10/30 10:03:08 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:08 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:08 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:08 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:08 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34765 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:08 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:08 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:08 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:08 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818187936
16/10/30 10:03:08 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-af433c09-03cd-4e55-bd8c-e1b129199a79/userFiles-2d762a2e-235b-44c2-90cb-96933d4a6f04/SparkJNIPi.so
16/10/30 10:03:08 INFO Executor: Fetching spark://192.168.0.17:36846/jars/jni-spark-0.1.jar with timestamp 1477818185426
16/10/30 10:03:08 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36846 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:03:08 INFO Utils: Fetching spark://192.168.0.17:36846/jars/jni-spark-0.1.jar to /tmp/spark-af433c09-03cd-4e55-bd8c-e1b129199a79/userFiles-2d762a2e-235b-44c2-90cb-96933d4a6f04/fetchFileTemp973161942415614455.tmp
16/10/30 10:03:08 INFO Executor: Adding file:/tmp/spark-af433c09-03cd-4e55-bd8c-e1b129199a79/userFiles-2d762a2e-235b-44c2-90cb-96933d4a6f04/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f8a3e0da013, pid=26092, tid=0x00007f8a473f7700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x3013]  pci_device_unmap_region+0x23
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid26092.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 26092 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:03:10 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:03:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:03:10 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:03:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:03:10 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:03:10 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:03:10 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:03:10 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:03:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:03:10 INFO Utils: Successfully started service 'sparkDriver' on port 39759.
16/10/30 10:03:10 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:03:10 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:03:10 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-59f17394-ee37-4082-a931-8aa4ddfbb944
16/10/30 10:03:10 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:03:10 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:03:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:03:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:03:11 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39759/jars/jni-spark-0.1.jar with timestamp 1477818191185
16/10/30 10:03:11 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32867.
16/10/30 10:03:11 INFO NettyBlockTransferService: Server created on 192.168.0.17:32867
16/10/30 10:03:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 32867)
16/10/30 10:03:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:32867 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 32867)
16/10/30 10:03:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 32867)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:13 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818193059
16/10/30 10:03:13 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c241d75b-60a9-4d7d-b51d-aa65f7b1e972/userFiles-c98f47a0-4df5-4bdd-b542-2d15a33c7040/SparkJNIPi.so
16/10/30 10:03:13 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:13 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:13 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:13 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:13 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:32867 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:13 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:13 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:13 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818193059
16/10/30 10:03:13 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c241d75b-60a9-4d7d-b51d-aa65f7b1e972/userFiles-c98f47a0-4df5-4bdd-b542-2d15a33c7040/SparkJNIPi.so
16/10/30 10:03:13 INFO Executor: Fetching spark://192.168.0.17:39759/jars/jni-spark-0.1.jar with timestamp 1477818191185
16/10/30 10:03:13 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39759 after 30 ms (0 ms spent in bootstraps)
16/10/30 10:03:13 INFO Utils: Fetching spark://192.168.0.17:39759/jars/jni-spark-0.1.jar to /tmp/spark-c241d75b-60a9-4d7d-b51d-aa65f7b1e972/userFiles-c98f47a0-4df5-4bdd-b542-2d15a33c7040/fetchFileTemp5320277884120040139.tmp
16/10/30 10:03:14 INFO Executor: Adding file:/tmp/spark-c241d75b-60a9-4d7d-b51d-aa65f7b1e972/userFiles-c98f47a0-4df5-4bdd-b542-2d15a33c7040/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:03:14 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:03:14 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:32867 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:03:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 10:03:14 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:32867 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:03:14 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 10:03:14 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:32867 (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:03:14 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 10:03:14 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:32867 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:03:14 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:32867 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:03:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 899 ms on localhost (1/2)
16/10/30 10:03:14 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.965 s
16/10/30 10:03:14 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.329432 s
16/10/30 10:03:14 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 899 ms on localhost (2/2)
16/10/30 10:03:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result: 3.1425648 in 1.664 seconds
16/10/30 10:03:14 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:03:14 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:03:14 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:03:14 INFO MemoryStore: MemoryStore cleared
16/10/30 10:03:14 INFO BlockManager: BlockManager stopped
16/10/30 10:03:14 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:03:14 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:03:14 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:03:14 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:03:14 INFO ShutdownHookManager: Deleting directory /tmp/spark-c241d75b-60a9-4d7d-b51d-aa65f7b1e972
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:03:16 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:03:17 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:03:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:03:17 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:03:17 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:03:17 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:03:17 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:03:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:03:18 INFO Utils: Successfully started service 'sparkDriver' on port 46461.
16/10/30 10:03:18 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:03:18 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:03:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cd97d280-70d5-4cc2-87de-4c8c5ef774de
16/10/30 10:03:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:03:18 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:03:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:03:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:03:18 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46461/jars/jni-spark-0.1.jar with timestamp 1477818198754
16/10/30 10:03:18 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40361.
16/10/30 10:03:18 INFO NettyBlockTransferService: Server created on 192.168.0.17:40361
16/10/30 10:03:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40361)
16/10/30 10:03:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40361 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40361)
16/10/30 10:03:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40361)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:20 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818200966
16/10/30 10:03:20 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-64efe28d-a371-4851-96db-9d74192cc569/userFiles-66de45f8-a329-46f8-bb10-c9d4462039cb/SparkJNIPi.so
16/10/30 10:03:21 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:21 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:21 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:21 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:21 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40361 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:21 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:21 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:21 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818200966
16/10/30 10:03:21 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-64efe28d-a371-4851-96db-9d74192cc569/userFiles-66de45f8-a329-46f8-bb10-c9d4462039cb/SparkJNIPi.so
16/10/30 10:03:21 INFO Executor: Fetching spark://192.168.0.17:46461/jars/jni-spark-0.1.jar with timestamp 1477818198754
16/10/30 10:03:21 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46461 after 43 ms (0 ms spent in bootstraps)
16/10/30 10:03:21 INFO Utils: Fetching spark://192.168.0.17:46461/jars/jni-spark-0.1.jar to /tmp/spark-64efe28d-a371-4851-96db-9d74192cc569/userFiles-66de45f8-a329-46f8-bb10-c9d4462039cb/fetchFileTemp7789579378299220944.tmp
16/10/30 10:03:21 INFO Executor: Adding file:/tmp/spark-64efe28d-a371-4851-96db-9d74192cc569/userFiles-66de45f8-a329-46f8-bb10-c9d4462039cb/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:03:22 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:03:22 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:40361 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:03:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 10:03:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40361 after 3 ms (0 ms spent in bootstraps)
16/10/30 10:03:22 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 10:03:22 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:40361 (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:03:22 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 10:03:22 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:40361 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:03:22 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:40361 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:03:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 816 ms on localhost (1/2)
16/10/30 10:03:22 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 771 ms on localhost (2/2)
16/10/30 10:03:22 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.875 s
16/10/30 10:03:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:03:22 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.319610 s
Result: 3.1428204 in 1.634 seconds
16/10/30 10:03:22 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:03:22 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:03:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:03:22 INFO MemoryStore: MemoryStore cleared
16/10/30 10:03:22 INFO BlockManager: BlockManager stopped
16/10/30 10:03:22 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:03:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:03:22 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:03:22 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:03:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-64efe28d-a371-4851-96db-9d74192cc569
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:03:24 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:03:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:03:24 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:03:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:03:24 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:03:24 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:03:24 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:03:24 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:03:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:03:25 INFO Utils: Successfully started service 'sparkDriver' on port 33407.
16/10/30 10:03:25 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:03:25 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:03:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-13dfafc8-f102-4417-a912-65127ffdb6e7
16/10/30 10:03:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:03:25 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:03:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:03:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:03:25 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33407/jars/jni-spark-0.1.jar with timestamp 1477818205748
16/10/30 10:03:25 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36423.
16/10/30 10:03:25 INFO NettyBlockTransferService: Server created on 192.168.0.17:36423
16/10/30 10:03:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36423)
16/10/30 10:03:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36423 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36423)
16/10/30 10:03:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36423)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:28 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818208229
16/10/30 10:03:28 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2a600ee7-1071-4a62-9b91-b73213630556/userFiles-cf090081-7a07-4173-8c82-f9802bafff10/SparkJNIPi.so
16/10/30 10:03:28 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:28 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:28 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:28 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36423 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:28 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:29 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:29 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:29 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818208229
16/10/30 10:03:29 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2a600ee7-1071-4a62-9b91-b73213630556/userFiles-cf090081-7a07-4173-8c82-f9802bafff10/SparkJNIPi.so
16/10/30 10:03:29 INFO Executor: Fetching spark://192.168.0.17:33407/jars/jni-spark-0.1.jar with timestamp 1477818205748
16/10/30 10:03:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33407 after 36 ms (0 ms spent in bootstraps)
16/10/30 10:03:29 INFO Utils: Fetching spark://192.168.0.17:33407/jars/jni-spark-0.1.jar to /tmp/spark-2a600ee7-1071-4a62-9b91-b73213630556/userFiles-cf090081-7a07-4173-8c82-f9802bafff10/fetchFileTemp6971246307409585896.tmp
16/10/30 10:03:29 INFO Executor: Adding file:/tmp/spark-2a600ee7-1071-4a62-9b91-b73213630556/userFiles-cf090081-7a07-4173-8c82-f9802bafff10/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:03:29 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:03:29 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:36423 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:03:29 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 10:03:29 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 10:03:29 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:36423 (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:03:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 10:03:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36423 after 26 ms (0 ms spent in bootstraps)
16/10/30 10:03:29 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:36423 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:03:29 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:36423 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:03:29 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 886 ms on localhost (1/2)
16/10/30 10:03:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 942 ms on localhost (2/2)
16/10/30 10:03:29 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.960 s
16/10/30 10:03:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:03:29 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.262270 s
Result: 3.1420097 in 1.682 seconds
16/10/30 10:03:29 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:03:29 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:03:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:03:29 INFO MemoryStore: MemoryStore cleared
16/10/30 10:03:29 INFO BlockManager: BlockManager stopped
16/10/30 10:03:29 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:03:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:03:29 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:03:30 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:03:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-2a600ee7-1071-4a62-9b91-b73213630556
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:03:31 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:03:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:03:32 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:03:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:03:32 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:03:32 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:03:32 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:03:32 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:03:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:03:32 INFO Utils: Successfully started service 'sparkDriver' on port 43605.
16/10/30 10:03:32 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:03:32 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:03:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bbd7f339-e107-4448-99ba-98b31d0b9c9a
16/10/30 10:03:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:03:32 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:03:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:03:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:03:33 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43605/jars/jni-spark-0.1.jar with timestamp 1477818213073
16/10/30 10:03:33 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:33 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38451.
16/10/30 10:03:33 INFO NettyBlockTransferService: Server created on 192.168.0.17:38451
16/10/30 10:03:33 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38451)
16/10/30 10:03:33 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38451 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38451)
16/10/30 10:03:33 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38451)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:34 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818214825
16/10/30 10:03:34 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8816db88-4042-4a54-a0bd-20826aa30b5f/userFiles-d1410eff-0caa-42ec-ad3a-cf8be16d9f1c/SparkJNIPi.so
16/10/30 10:03:35 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:35 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:35 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:35 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:35 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:35 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38451 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:35 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:35 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:35 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:35 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818214825
16/10/30 10:03:35 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8816db88-4042-4a54-a0bd-20826aa30b5f/userFiles-d1410eff-0caa-42ec-ad3a-cf8be16d9f1c/SparkJNIPi.so
16/10/30 10:03:35 INFO Executor: Fetching spark://192.168.0.17:43605/jars/jni-spark-0.1.jar with timestamp 1477818213073
16/10/30 10:03:35 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43605 after 76 ms (0 ms spent in bootstraps)
16/10/30 10:03:35 INFO Utils: Fetching spark://192.168.0.17:43605/jars/jni-spark-0.1.jar to /tmp/spark-8816db88-4042-4a54-a0bd-20826aa30b5f/userFiles-d1410eff-0caa-42ec-ad3a-cf8be16d9f1c/fetchFileTemp3061337647671786116.tmp
16/10/30 10:03:35 INFO Executor: Adding file:/tmp/spark-8816db88-4042-4a54-a0bd-20826aa30b5f/userFiles-d1410eff-0caa-42ec-ad3a-cf8be16d9f1c/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:03:40 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:03:40 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:38451 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:03:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:03:40 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:03:40 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:38451 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:03:40 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:03:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38451 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:03:40 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:38451 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:03:41 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:38451 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:03:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5303 ms on localhost (1/2)
16/10/30 10:03:41 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 5.604 s
16/10/30 10:03:41 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5515 ms on localhost (2/2)
16/10/30 10:03:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:03:41 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 6.083611 s
Result: 3.1414466 in 6.341 seconds
16/10/30 10:03:41 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:03:41 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:03:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:03:41 INFO MemoryStore: MemoryStore cleared
16/10/30 10:03:41 INFO BlockManager: BlockManager stopped
16/10/30 10:03:41 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:03:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:03:41 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:03:41 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:03:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-8816db88-4042-4a54-a0bd-20826aa30b5f
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:03:43 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:03:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:03:43 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:03:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:03:43 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:03:43 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:03:43 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:03:43 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:03:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:03:44 INFO Utils: Successfully started service 'sparkDriver' on port 40384.
16/10/30 10:03:44 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:03:44 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:03:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-358b4b6c-1529-4f17-a66d-3ac8e105b55b
16/10/30 10:03:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:03:44 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:03:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:03:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:03:44 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40384/jars/jni-spark-0.1.jar with timestamp 1477818224870
16/10/30 10:03:45 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39683.
16/10/30 10:03:45 INFO NettyBlockTransferService: Server created on 192.168.0.17:39683
16/10/30 10:03:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39683)
16/10/30 10:03:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39683 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39683)
16/10/30 10:03:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39683)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:46 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818226791
16/10/30 10:03:46 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a5195e19-d6fd-43b4-896e-c4ce85e154e6/userFiles-9a320f2c-9fc8-40ab-b235-be4fa249ba4b/SparkJNIPi.so
16/10/30 10:03:47 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:47 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:47 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:47 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:47 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39683 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:47 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:47 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:47 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818226791
16/10/30 10:03:47 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a5195e19-d6fd-43b4-896e-c4ce85e154e6/userFiles-9a320f2c-9fc8-40ab-b235-be4fa249ba4b/SparkJNIPi.so
16/10/30 10:03:47 INFO Executor: Fetching spark://192.168.0.17:40384/jars/jni-spark-0.1.jar with timestamp 1477818224870
16/10/30 10:03:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40384 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:03:47 INFO Utils: Fetching spark://192.168.0.17:40384/jars/jni-spark-0.1.jar to /tmp/spark-a5195e19-d6fd-43b4-896e-c4ce85e154e6/userFiles-9a320f2c-9fc8-40ab-b235-be4fa249ba4b/fetchFileTemp5489400134378530954.tmp
16/10/30 10:03:47 INFO Executor: Adding file:/tmp/spark-a5195e19-d6fd-43b4-896e-c4ce85e154e6/userFiles-9a320f2c-9fc8-40ab-b235-be4fa249ba4b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:03:51 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:03:51 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:39683 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:03:51 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:03:51 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:03:51 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:39683 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:03:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:03:51 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39683 after 5 ms (0 ms spent in bootstraps)
16/10/30 10:03:51 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:39683 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:03:51 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:39683 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:03:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3607 ms on localhost (1/2)
16/10/30 10:03:51 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3704 ms on localhost (2/2)
16/10/30 10:03:51 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.790 s
16/10/30 10:03:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:03:51 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 4.243754 s
16/10/30 10:03:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.17:39683 in memory (size: 1698.0 B, free: 366.3 MB)
Result: 3.1417062 in 4.645 seconds
16/10/30 10:03:51 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:03:51 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:03:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:03:51 INFO MemoryStore: MemoryStore cleared
16/10/30 10:03:51 INFO BlockManager: BlockManager stopped
16/10/30 10:03:51 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:03:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:03:51 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:03:51 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:03:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5195e19-d6fd-43b4-896e-c4ce85e154e6
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:03:53 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:03:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:03:53 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:03:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:03:53 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:03:53 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:03:53 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:03:53 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:03:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:03:53 INFO Utils: Successfully started service 'sparkDriver' on port 44028.
16/10/30 10:03:53 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:03:53 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:03:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-834756da-2d25-4f61-b51c-0b17a7c34610
16/10/30 10:03:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:03:54 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:03:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:03:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:03:54 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44028/jars/jni-spark-0.1.jar with timestamp 1477818234342
16/10/30 10:03:54 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:03:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41962.
16/10/30 10:03:54 INFO NettyBlockTransferService: Server created on 192.168.0.17:41962
16/10/30 10:03:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41962)
16/10/30 10:03:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41962 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41962)
16/10/30 10:03:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41962)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:03:56 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818236588
16/10/30 10:03:56 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-48cac6dd-651e-4264-92f6-b391c77b8128/userFiles-1e2a542f-8fe6-48b4-8676-3f35abc26765/SparkJNIPi.so
16/10/30 10:03:57 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:03:57 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:03:57 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:03:57 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:03:57 INFO DAGScheduler: Missing parents: List()
16/10/30 10:03:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:03:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:03:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:03:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41962 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:03:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:03:57 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:03:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:03:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:03:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:03:57 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:03:57 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818236588
16/10/30 10:03:57 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-48cac6dd-651e-4264-92f6-b391c77b8128/userFiles-1e2a542f-8fe6-48b4-8676-3f35abc26765/SparkJNIPi.so
16/10/30 10:03:57 INFO Executor: Fetching spark://192.168.0.17:44028/jars/jni-spark-0.1.jar with timestamp 1477818234342
16/10/30 10:03:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44028 after 35 ms (0 ms spent in bootstraps)
16/10/30 10:03:57 INFO Utils: Fetching spark://192.168.0.17:44028/jars/jni-spark-0.1.jar to /tmp/spark-48cac6dd-651e-4264-92f6-b391c77b8128/userFiles-1e2a542f-8fe6-48b4-8676-3f35abc26765/fetchFileTemp4725523816867939281.tmp
16/10/30 10:03:57 INFO Executor: Adding file:/tmp/spark-48cac6dd-651e-4264-92f6-b391c77b8128/userFiles-1e2a542f-8fe6-48b4-8676-3f35abc26765/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:04:00 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:04:00 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:41962 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:04:00 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:04:00 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:04:00 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:41962 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:04:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:04:00 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41962 after 3 ms (0 ms spent in bootstraps)
16/10/30 10:04:00 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:41962 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:04:00 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:41962 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:04:00 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3196 ms on localhost (1/2)
16/10/30 10:04:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3424 ms on localhost (2/2)
16/10/30 10:04:00 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.454 s
16/10/30 10:04:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:04:00 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.746422 s
Result: 3.141054 in 4.267 seconds
16/10/30 10:04:00 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:04:00 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:04:00 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:04:00 INFO MemoryStore: MemoryStore cleared
16/10/30 10:04:00 INFO BlockManager: BlockManager stopped
16/10/30 10:04:00 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:04:00 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:04:00 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:04:00 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:04:00 INFO ShutdownHookManager: Deleting directory /tmp/spark-48cac6dd-651e-4264-92f6-b391c77b8128
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:04:02 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:04:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:04:02 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:04:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:04:02 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:04:02 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:04:02 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:04:02 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:04:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:04:03 INFO Utils: Successfully started service 'sparkDriver' on port 43494.
16/10/30 10:04:03 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:04:03 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:04:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c91bb91e-d72c-4e32-8b2e-8dbc8adfb3f7
16/10/30 10:04:03 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:04:03 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:04:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:04:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:04:03 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43494/jars/jni-spark-0.1.jar with timestamp 1477818243867
16/10/30 10:04:03 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:04:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35858.
16/10/30 10:04:04 INFO NettyBlockTransferService: Server created on 192.168.0.17:35858
16/10/30 10:04:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35858)
16/10/30 10:04:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35858 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35858)
16/10/30 10:04:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35858)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:04:06 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818246102
16/10/30 10:04:06 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-31ab7aa5-9989-490b-bdec-0340cb128531/userFiles-36e85e28-ff1f-47d6-b40e-91c14e2e1f4c/SparkJNIPi.so
16/10/30 10:04:06 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:04:06 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:04:06 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:04:06 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:04:06 INFO DAGScheduler: Missing parents: List()
16/10/30 10:04:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:04:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:04:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:04:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35858 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:04:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:04:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:04:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:04:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:04:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:04:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:04:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:04:06 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818246102
16/10/30 10:04:06 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-31ab7aa5-9989-490b-bdec-0340cb128531/userFiles-36e85e28-ff1f-47d6-b40e-91c14e2e1f4c/SparkJNIPi.so
16/10/30 10:04:06 INFO Executor: Fetching spark://192.168.0.17:43494/jars/jni-spark-0.1.jar with timestamp 1477818243867
16/10/30 10:04:06 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43494 after 22 ms (0 ms spent in bootstraps)
16/10/30 10:04:06 INFO Utils: Fetching spark://192.168.0.17:43494/jars/jni-spark-0.1.jar to /tmp/spark-31ab7aa5-9989-490b-bdec-0340cb128531/userFiles-36e85e28-ff1f-47d6-b40e-91c14e2e1f4c/fetchFileTemp3266645170220993304.tmp
16/10/30 10:04:06 INFO Executor: Adding file:/tmp/spark-31ab7aa5-9989-490b-bdec-0340cb128531/userFiles-36e85e28-ff1f-47d6-b40e-91c14e2e1f4c/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:04:10 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:04:10 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:35858 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:04:10 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:04:10 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35858 after 8 ms (0 ms spent in bootstraps)
16/10/30 10:04:10 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:04:10 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:35858 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:04:10 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:04:10 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:35858 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:04:10 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:35858 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:04:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4251 ms on localhost (1/2)
16/10/30 10:04:11 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4270 ms on localhost (2/2)
16/10/30 10:04:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:04:11 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 4.373 s
16/10/30 10:04:11 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 4.646586 s
Result: 3.1418898 in 4.931 seconds
16/10/30 10:04:11 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:04:11 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:04:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:04:11 INFO MemoryStore: MemoryStore cleared
16/10/30 10:04:11 INFO BlockManager: BlockManager stopped
16/10/30 10:04:11 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:04:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:04:11 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:04:11 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:04:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-31ab7aa5-9989-490b-bdec-0340cb128531
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:04:12 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:04:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:04:13 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:04:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:04:13 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:04:13 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:04:13 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:04:13 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:04:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:04:13 INFO Utils: Successfully started service 'sparkDriver' on port 45862.
16/10/30 10:04:13 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:04:13 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:04:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1baf92e5-83e4-4dd2-9b7a-53f6a7150fd9
16/10/30 10:04:14 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:04:14 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:04:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:04:14 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:04:14 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45862/jars/jni-spark-0.1.jar with timestamp 1477818254389
16/10/30 10:04:14 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:04:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41075.
16/10/30 10:04:14 INFO NettyBlockTransferService: Server created on 192.168.0.17:41075
16/10/30 10:04:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41075)
16/10/30 10:04:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41075 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41075)
16/10/30 10:04:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41075)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:04:16 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818256493
16/10/30 10:04:16 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-80374344-6c30-4cf0-983e-494858d47e7b/userFiles-48616869-1913-4708-a8d8-6003334b440e/SparkJNIPi.so
16/10/30 10:04:16 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:04:16 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:04:16 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:04:16 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:04:16 INFO DAGScheduler: Missing parents: List()
16/10/30 10:04:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:04:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:04:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:04:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41075 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:04:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:04:17 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:04:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:04:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:04:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:04:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:04:17 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818256493
16/10/30 10:04:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:04:17 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-80374344-6c30-4cf0-983e-494858d47e7b/userFiles-48616869-1913-4708-a8d8-6003334b440e/SparkJNIPi.so
16/10/30 10:04:17 INFO Executor: Fetching spark://192.168.0.17:45862/jars/jni-spark-0.1.jar with timestamp 1477818254389
16/10/30 10:04:17 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45862 after 45 ms (0 ms spent in bootstraps)
16/10/30 10:04:17 INFO Utils: Fetching spark://192.168.0.17:45862/jars/jni-spark-0.1.jar to /tmp/spark-80374344-6c30-4cf0-983e-494858d47e7b/userFiles-48616869-1913-4708-a8d8-6003334b440e/fetchFileTemp4338180023587053618.tmp
16/10/30 10:04:17 INFO Executor: Adding file:/tmp/spark-80374344-6c30-4cf0-983e-494858d47e7b/userFiles-48616869-1913-4708-a8d8-6003334b440e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:04:34 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:04:34 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:04:34 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:04:34 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:04:34 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:04:34 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 17.809509 s
Exception in thread "main" org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:816)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:816)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1685)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1604)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1798)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1287)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1797)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:559)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:215)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:177)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/10/30 10:04:34 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
16/10/30 10:04:34 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 17.556 s
16/10/30 10:04:34 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@ec3dc50)
16/10/30 10:04:34 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1477818274589,JobFailed(org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down))
16/10/30 10:04:34 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:04:34 INFO MemoryStore: MemoryStore cleared
16/10/30 10:04:34 INFO BlockManager: BlockManager stopped
16/10/30 10:04:34 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:04:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:04:34 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:04:34 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:04:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-80374344-6c30-4cf0-983e-494858d47e7b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:04:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:04:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:04:36 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:04:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:04:36 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:04:36 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:04:36 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:04:36 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:04:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:04:37 INFO Utils: Successfully started service 'sparkDriver' on port 43745.
16/10/30 10:04:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:04:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:04:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bd43bad5-6844-4583-b43d-12125b7087ae
16/10/30 10:04:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:04:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:04:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:04:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:04:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43745/jars/jni-spark-0.1.jar with timestamp 1477818277815
16/10/30 10:04:37 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:04:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37724.
16/10/30 10:04:37 INFO NettyBlockTransferService: Server created on 192.168.0.17:37724
16/10/30 10:04:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37724)
16/10/30 10:04:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37724 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37724)
16/10/30 10:04:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37724)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:04:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818279884
16/10/30 10:04:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-52b670f5-7260-448a-9188-25f2dc0e483d/userFiles-d44b4449-8987-44c7-b2a5-f0d72d269456/SparkJNIPi.so
16/10/30 10:04:40 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:04:40 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:04:40 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:04:40 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:04:40 INFO DAGScheduler: Missing parents: List()
16/10/30 10:04:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:04:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:04:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:04:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37724 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:04:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:04:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:04:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:04:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:04:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:04:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:04:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818279884
16/10/30 10:04:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:04:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-52b670f5-7260-448a-9188-25f2dc0e483d/userFiles-d44b4449-8987-44c7-b2a5-f0d72d269456/SparkJNIPi.so
16/10/30 10:04:40 INFO Executor: Fetching spark://192.168.0.17:43745/jars/jni-spark-0.1.jar with timestamp 1477818277815
16/10/30 10:04:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43745 after 34 ms (0 ms spent in bootstraps)
16/10/30 10:04:40 INFO Utils: Fetching spark://192.168.0.17:43745/jars/jni-spark-0.1.jar to /tmp/spark-52b670f5-7260-448a-9188-25f2dc0e483d/userFiles-d44b4449-8987-44c7-b2a5-f0d72d269456/fetchFileTemp6858625034654211160.tmp
16/10/30 10:04:40 INFO Executor: Adding file:/tmp/spark-52b670f5-7260-448a-9188-25f2dc0e483d/userFiles-d44b4449-8987-44c7-b2a5-f0d72d269456/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007ff7149789f2, pid=27125, tid=0x00007ff6ee0e8700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x6cf9f2]  jni_SetIntArrayRegion+0xc2
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid27125.log
[thread 140698523117312 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
./runTimesModes.sh: line 9: 27125 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:04:59 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:04:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:05:00 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:05:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:05:00 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:05:00 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:05:00 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:05:00 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:05:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:05:00 INFO Utils: Successfully started service 'sparkDriver' on port 34819.
16/10/30 10:05:00 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:05:00 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:05:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ab8382a6-c6fe-4329-9c9d-2f05f7f9b292
16/10/30 10:05:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:05:00 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:05:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:05:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:05:00 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34819/jars/jni-spark-0.1.jar with timestamp 1477818300906
16/10/30 10:05:00 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:05:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40310.
16/10/30 10:05:01 INFO NettyBlockTransferService: Server created on 192.168.0.17:40310
16/10/30 10:05:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40310)
16/10/30 10:05:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40310 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40310)
16/10/30 10:05:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40310)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:05:02 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818302753
16/10/30 10:05:02 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-5fefb978-02d1-4b25-8944-abfc0640802b/userFiles-1728c521-4be1-4f5b-9833-129419308a60/SparkJNIPi.so
16/10/30 10:05:03 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:05:03 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:05:03 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:05:03 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:05:03 INFO DAGScheduler: Missing parents: List()
16/10/30 10:05:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:05:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:05:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:05:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40310 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:05:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:05:03 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:05:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:05:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:05:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:05:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:05:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:05:03 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818302753
16/10/30 10:05:03 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-5fefb978-02d1-4b25-8944-abfc0640802b/userFiles-1728c521-4be1-4f5b-9833-129419308a60/SparkJNIPi.so
16/10/30 10:05:03 INFO Executor: Fetching spark://192.168.0.17:34819/jars/jni-spark-0.1.jar with timestamp 1477818300906
16/10/30 10:05:03 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34819 after 32 ms (0 ms spent in bootstraps)
16/10/30 10:05:03 INFO Utils: Fetching spark://192.168.0.17:34819/jars/jni-spark-0.1.jar to /tmp/spark-5fefb978-02d1-4b25-8944-abfc0640802b/userFiles-1728c521-4be1-4f5b-9833-129419308a60/fetchFileTemp161174586388450098.tmp
16/10/30 10:05:03 INFO Executor: Adding file:/tmp/spark-5fefb978-02d1-4b25-8944-abfc0640802b/userFiles-1728c521-4be1-4f5b-9833-129419308a60/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f37c55be9f2, pid=27228, tid=0x00007f372e3e4700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x6cf9f2]  jni_SetIntArrayRegion+0xc2
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid27228.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
./runTimesModes.sh: line 9: 27228 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:05:23 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:05:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:05:24 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:05:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:05:24 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:05:24 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:05:24 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:05:24 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:05:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:05:24 INFO Utils: Successfully started service 'sparkDriver' on port 40103.
16/10/30 10:05:24 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:05:24 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:05:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c125a312-9d13-4537-baf9-99ea6d981fc8
16/10/30 10:05:24 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:05:24 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:05:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:05:24 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:05:24 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40103/jars/jni-spark-0.1.jar with timestamp 1477818324926
16/10/30 10:05:25 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:05:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34442.
16/10/30 10:05:25 INFO NettyBlockTransferService: Server created on 192.168.0.17:34442
16/10/30 10:05:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34442)
16/10/30 10:05:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34442 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34442)
16/10/30 10:05:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34442)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:05:26 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818326818
16/10/30 10:05:26 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-75d22714-e88f-4f2c-9747-e0dec8608a4d/userFiles-a10b88a0-4bec-443d-86e3-8bd9dc569254/SparkJNIPi.so
16/10/30 10:05:27 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:05:27 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 2 output partitions
16/10/30 10:05:27 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:05:27 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:05:27 INFO DAGScheduler: Missing parents: List()
16/10/30 10:05:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:05:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:05:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1698.0 B, free 366.3 MB)
16/10/30 10:05:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34442 (size: 1698.0 B, free: 366.3 MB)
16/10/30 10:05:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:05:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:05:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
16/10/30 10:05:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:05:27 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5613 bytes)
16/10/30 10:05:27 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:05:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:05:27 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818326818
16/10/30 10:05:27 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-75d22714-e88f-4f2c-9747-e0dec8608a4d/userFiles-a10b88a0-4bec-443d-86e3-8bd9dc569254/SparkJNIPi.so
16/10/30 10:05:27 INFO Executor: Fetching spark://192.168.0.17:40103/jars/jni-spark-0.1.jar with timestamp 1477818324926
16/10/30 10:05:27 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40103 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:05:27 INFO Utils: Fetching spark://192.168.0.17:40103/jars/jni-spark-0.1.jar to /tmp/spark-75d22714-e88f-4f2c-9747-e0dec8608a4d/userFiles-a10b88a0-4bec-443d-86e3-8bd9dc569254/fetchFileTemp132809075610573566.tmp
16/10/30 10:05:27 INFO Executor: Adding file:/tmp/spark-75d22714-e88f-4f2c-9747-e0dec8608a4d/userFiles-a10b88a0-4bec-443d-86e3-8bd9dc569254/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:05:43 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:05:43 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:05:43 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:05:43 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:05:43 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:05:43 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
16/10/30 10:05:43 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(0,0,ResultTask,ExceptionFailure(java.lang.OutOfMemoryError,Java heap space,[Ljava.lang.StackTraceElement;@2afa0a62,java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
,Some(org.apache.spark.ThrowableSerializationWrapper@5c3fc3d0),Vector(AccumulableInfo(1,Some(internal.metrics.executorRunTime),Some(15651),None,true,true,None), AccumulableInfo(2,Some(internal.metrics.resultSize),Some(0),None,true,true,None), AccumulableInfo(3,Some(internal.metrics.jvmGCTime),Some(652),None,true,true,None)),Vector(LongAccumulator(id: 1, name: Some(internal.metrics.executorRunTime), value: 15651), LongAccumulator(id: 2, name: Some(internal.metrics.resultSize), value: 0), LongAccumulator(id: 3, name: Some(internal.metrics.jvmGCTime), value: 652))),org.apache.spark.scheduler.TaskInfo@6cf1feed,org.apache.spark.executor.TaskMetrics@41084d29)
16/10/30 10:05:43 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 15.967 s
16/10/30 10:05:43 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 16.244452 s
16/10/30 10:05:43 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@20a265e3)
Exception in thread "main" org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:816)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:816)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1685)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1604)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1798)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1287)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1797)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:559)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:215)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:177)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/10/30 10:05:43 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1477818343345,JobFailed(org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down))
16/10/30 10:05:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:05:43 INFO MemoryStore: MemoryStore cleared
16/10/30 10:05:43 INFO BlockManager: BlockManager stopped
16/10/30 10:05:43 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:05:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:05:43 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:05:43 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:05:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-75d22714-e88f-4f2c-9747-e0dec8608a4d
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:05:44 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:05:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:05:45 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:05:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:05:45 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:05:45 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:05:45 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:05:45 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:05:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:05:45 INFO Utils: Successfully started service 'sparkDriver' on port 40883.
16/10/30 10:05:45 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:05:45 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:05:45 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a0512a88-f62b-4236-8530-b54ae1d09e73
16/10/30 10:05:45 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:05:45 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:05:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:05:46 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:05:46 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40883/jars/jni-spark-0.1.jar with timestamp 1477818346327
16/10/30 10:05:46 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:05:46 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42308.
16/10/30 10:05:46 INFO NettyBlockTransferService: Server created on 192.168.0.17:42308
16/10/30 10:05:46 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42308)
16/10/30 10:05:46 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42308 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42308)
16/10/30 10:05:46 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42308)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:05:49 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818349044
16/10/30 10:05:49 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-e856d630-8c82-476c-906a-31a3b645179a/userFiles-4aaea6f2-12c3-417d-9cbb-c1f07d1343ef/SparkJNIPi.so
16/10/30 10:05:49 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:05:49 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:05:49 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:05:49 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:05:49 INFO DAGScheduler: Missing parents: List()
16/10/30 10:05:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:05:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:05:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:05:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42308 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:05:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:05:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:05:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:05:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:05:49 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:05:49 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:05:49 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:05:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:05:49 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:05:49 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:05:49 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:05:49 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818349044
16/10/30 10:05:49 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-e856d630-8c82-476c-906a-31a3b645179a/userFiles-4aaea6f2-12c3-417d-9cbb-c1f07d1343ef/SparkJNIPi.so
16/10/30 10:05:49 INFO Executor: Fetching spark://192.168.0.17:40883/jars/jni-spark-0.1.jar with timestamp 1477818346327
16/10/30 10:05:49 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40883 after 42 ms (0 ms spent in bootstraps)
16/10/30 10:05:49 INFO Utils: Fetching spark://192.168.0.17:40883/jars/jni-spark-0.1.jar to /tmp/spark-e856d630-8c82-476c-906a-31a3b645179a/userFiles-4aaea6f2-12c3-417d-9cbb-c1f07d1343ef/fetchFileTemp2143003715293188478.tmp
16/10/30 10:05:50 INFO Executor: Adding file:/tmp/spark-e856d630-8c82-476c-906a-31a3b645179a/userFiles-4aaea6f2-12c3-417d-9cbb-c1f07d1343ef/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:05:50 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 858 bytes result sent to driver
16/10/30 10:05:50 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:05:50 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:05:50 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 425 ms on localhost (1/4)
16/10/30 10:05:50 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 392 ms on localhost (2/4)
16/10/30 10:05:50 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 388 ms on localhost (3/4)
16/10/30 10:05:50 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 5205 bytes result sent to driver
16/10/30 10:05:50 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 529 ms on localhost (4/4)
16/10/30 10:05:50 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.590 s
16/10/30 10:05:50 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:05:50 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.922925 s
Result: 3.234375 in 1.28 seconds
16/10/30 10:05:50 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:05:50 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:05:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:05:50 INFO MemoryStore: MemoryStore cleared
16/10/30 10:05:50 INFO BlockManager: BlockManager stopped
16/10/30 10:05:50 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:05:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:05:50 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:05:50 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:05:50 INFO ShutdownHookManager: Deleting directory /tmp/spark-e856d630-8c82-476c-906a-31a3b645179a
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:05:52 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:05:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:05:52 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:05:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:05:52 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:05:52 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:05:52 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:05:52 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:05:52 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:05:53 INFO Utils: Successfully started service 'sparkDriver' on port 43499.
16/10/30 10:05:53 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:05:53 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:05:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fb297314-0ee3-429f-9d83-6d74937ba8f0
16/10/30 10:05:53 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:05:53 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:05:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:05:53 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:05:53 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43499/jars/jni-spark-0.1.jar with timestamp 1477818353478
16/10/30 10:05:53 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:05:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41537.
16/10/30 10:05:53 INFO NettyBlockTransferService: Server created on 192.168.0.17:41537
16/10/30 10:05:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41537)
16/10/30 10:05:53 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41537 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41537)
16/10/30 10:05:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41537)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:05:55 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818355051
16/10/30 10:05:55 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8562ba99-c222-4d6f-8172-3f4d3e725adf/userFiles-5769a047-74c8-4f6f-bcb7-ebffe476dc7b/SparkJNIPi.so
16/10/30 10:05:55 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:05:55 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:05:55 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:05:55 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:05:55 INFO DAGScheduler: Missing parents: List()
16/10/30 10:05:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:05:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:05:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:05:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41537 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:05:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:05:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:05:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:05:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:05:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:05:55 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:05:55 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:05:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:05:55 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818355051
16/10/30 10:05:55 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:05:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:05:55 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:05:55 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8562ba99-c222-4d6f-8172-3f4d3e725adf/userFiles-5769a047-74c8-4f6f-bcb7-ebffe476dc7b/SparkJNIPi.so
16/10/30 10:05:55 INFO Executor: Fetching spark://192.168.0.17:43499/jars/jni-spark-0.1.jar with timestamp 1477818353478
16/10/30 10:05:55 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43499 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:05:55 INFO Utils: Fetching spark://192.168.0.17:43499/jars/jni-spark-0.1.jar to /tmp/spark-8562ba99-c222-4d6f-8172-3f4d3e725adf/userFiles-5769a047-74c8-4f6f-bcb7-ebffe476dc7b/fetchFileTemp5415089400827747812.tmp
16/10/30 10:05:55 INFO Executor: Adding file:/tmp/spark-8562ba99-c222-4d6f-8172-3f4d3e725adf/userFiles-5769a047-74c8-4f6f-bcb7-ebffe476dc7b/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:05:55 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
16/10/30 10:05:55 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 858 bytes result sent to driver
16/10/30 10:05:55 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 858 bytes result sent to driver
16/10/30 10:05:55 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 278 ms on localhost (1/4)
16/10/30 10:05:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 244 ms on localhost (2/4)
16/10/30 10:05:55 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 241 ms on localhost (3/4)
16/10/30 10:05:56 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 5205 bytes result sent to driver
16/10/30 10:05:56 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 345 ms on localhost (4/4)
16/10/30 10:05:56 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.397 s
16/10/30 10:05:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:05:56 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.660015 s
Result: 3.0664062 in 0.951 seconds
16/10/30 10:05:56 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:05:56 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:05:56 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:05:56 INFO MemoryStore: MemoryStore cleared
16/10/30 10:05:56 INFO BlockManager: BlockManager stopped
16/10/30 10:05:56 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:05:56 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:05:56 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:05:56 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:05:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-8562ba99-c222-4d6f-8172-3f4d3e725adf
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:05:57 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:05:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:05:58 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:05:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:05:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:05:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:05:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:05:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:05:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:05:58 INFO Utils: Successfully started service 'sparkDriver' on port 40583.
16/10/30 10:05:58 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:05:58 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:05:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-147fcd7f-162f-4dd0-80f9-d08c4043227e
16/10/30 10:05:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:05:58 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:05:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:05:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:05:58 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40583/jars/jni-spark-0.1.jar with timestamp 1477818358737
16/10/30 10:05:58 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:05:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37331.
16/10/30 10:05:58 INFO NettyBlockTransferService: Server created on 192.168.0.17:37331
16/10/30 10:05:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37331)
16/10/30 10:05:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37331 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37331)
16/10/30 10:05:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37331)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:00 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818360458
16/10/30 10:06:00 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-ac20d73d-64bd-45de-9ffe-b86e0f2cd821/userFiles-93f41a5c-2e25-4c78-9852-3e488ffb3613/SparkJNIPi.so
16/10/30 10:06:00 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:00 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:00 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:00 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:00 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37331 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:01 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:01 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:01 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:01 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818360458
16/10/30 10:06:01 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:01 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:01 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:01 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-ac20d73d-64bd-45de-9ffe-b86e0f2cd821/userFiles-93f41a5c-2e25-4c78-9852-3e488ffb3613/SparkJNIPi.so
16/10/30 10:06:01 INFO Executor: Fetching spark://192.168.0.17:40583/jars/jni-spark-0.1.jar with timestamp 1477818358737
16/10/30 10:06:01 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40583 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:06:01 INFO Utils: Fetching spark://192.168.0.17:40583/jars/jni-spark-0.1.jar to /tmp/spark-ac20d73d-64bd-45de-9ffe-b86e0f2cd821/userFiles-93f41a5c-2e25-4c78-9852-3e488ffb3613/fetchFileTemp1035310031085194517.tmp
16/10/30 10:06:01 INFO Executor: Adding file:/tmp/spark-ac20d73d-64bd-45de-9ffe-b86e0f2cd821/userFiles-93f41a5c-2e25-4c78-9852-3e488ffb3613/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:01 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:01 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:06:01 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 226 ms on localhost (1/4)
16/10/30 10:06:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 274 ms on localhost (2/4)
16/10/30 10:06:01 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 248 ms on localhost (3/4)
16/10/30 10:06:01 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 5205 bytes result sent to driver
16/10/30 10:06:01 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 360 ms on localhost (4/4)
16/10/30 10:06:01 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.416 s
16/10/30 10:06:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:01 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.663117 s
Result: 3.109375 in 1.007 seconds
16/10/30 10:06:01 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:01 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:01 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:01 INFO BlockManager: BlockManager stopped
16/10/30 10:06:01 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:01 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:01 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-ac20d73d-64bd-45de-9ffe-b86e0f2cd821
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:02 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:03 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:03 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:03 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:03 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:03 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:03 INFO Utils: Successfully started service 'sparkDriver' on port 38040.
16/10/30 10:06:03 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:03 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b1bc33c7-63f4-404c-857d-d428ad26fbef
16/10/30 10:06:03 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:03 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:04 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38040/jars/jni-spark-0.1.jar with timestamp 1477818364067
16/10/30 10:06:04 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36758.
16/10/30 10:06:04 INFO NettyBlockTransferService: Server created on 192.168.0.17:36758
16/10/30 10:06:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36758)
16/10/30 10:06:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36758 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36758)
16/10/30 10:06:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36758)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:05 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818365957
16/10/30 10:06:05 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6d4bf1a6-cc66-4da8-87c5-2a0fc4786b0d/userFiles-882a938f-78d9-4bff-9a8f-7b4e6558f616/SparkJNIPi.so
16/10/30 10:06:06 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:06 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:06 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:06 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:06 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36758 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:06 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:06 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818365957
16/10/30 10:06:06 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6d4bf1a6-cc66-4da8-87c5-2a0fc4786b0d/userFiles-882a938f-78d9-4bff-9a8f-7b4e6558f616/SparkJNIPi.so
16/10/30 10:06:06 INFO Executor: Fetching spark://192.168.0.17:38040/jars/jni-spark-0.1.jar with timestamp 1477818364067
16/10/30 10:06:06 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38040 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:06:06 INFO Utils: Fetching spark://192.168.0.17:38040/jars/jni-spark-0.1.jar to /tmp/spark-6d4bf1a6-cc66-4da8-87c5-2a0fc4786b0d/userFiles-882a938f-78d9-4bff-9a8f-7b4e6558f616/fetchFileTemp2327947394581641762.tmp
16/10/30 10:06:06 INFO Executor: Adding file:/tmp/spark-6d4bf1a6-cc66-4da8-87c5-2a0fc4786b0d/userFiles-882a938f-78d9-4bff-9a8f-7b4e6558f616/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:06 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:06:06 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 858 bytes result sent to driver
16/10/30 10:06:06 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 284 ms on localhost (1/4)
16/10/30 10:06:06 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 282 ms on localhost (2/4)
16/10/30 10:06:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 346 ms on localhost (3/4)
16/10/30 10:06:07 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 5205 bytes result sent to driver
16/10/30 10:06:07 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 399 ms on localhost (4/4)
16/10/30 10:06:07 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.472 s
16/10/30 10:06:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:07 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.772432 s
Result: 3.1523438 in 1.096 seconds
16/10/30 10:06:07 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:07 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:07 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:07 INFO BlockManager: BlockManager stopped
16/10/30 10:06:07 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:07 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:07 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d4bf1a6-cc66-4da8-87c5-2a0fc4786b0d
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:08 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:08 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:08 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:08 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:08 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:08 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:09 INFO Utils: Successfully started service 'sparkDriver' on port 33358.
16/10/30 10:06:09 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:09 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-70fb9e60-19ca-4b3e-a358-e3c7891b215e
16/10/30 10:06:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:09 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:09 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33358/jars/jni-spark-0.1.jar with timestamp 1477818369635
16/10/30 10:06:09 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34855.
16/10/30 10:06:09 INFO NettyBlockTransferService: Server created on 192.168.0.17:34855
16/10/30 10:06:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34855)
16/10/30 10:06:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34855 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34855)
16/10/30 10:06:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34855)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:11 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818371634
16/10/30 10:06:11 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-be4e06a7-d9d3-43af-9363-e01ad9703b80/userFiles-908f4ad2-314d-4651-b371-659966683909/SparkJNIPi.so
16/10/30 10:06:12 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:12 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:12 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:12 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:12 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34855 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:12 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:12 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:12 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:12 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:12 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:12 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:12 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:12 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818371634
16/10/30 10:06:12 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-be4e06a7-d9d3-43af-9363-e01ad9703b80/userFiles-908f4ad2-314d-4651-b371-659966683909/SparkJNIPi.so
16/10/30 10:06:12 INFO Executor: Fetching spark://192.168.0.17:33358/jars/jni-spark-0.1.jar with timestamp 1477818369635
16/10/30 10:06:12 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33358 after 30 ms (0 ms spent in bootstraps)
16/10/30 10:06:12 INFO Utils: Fetching spark://192.168.0.17:33358/jars/jni-spark-0.1.jar to /tmp/spark-be4e06a7-d9d3-43af-9363-e01ad9703b80/userFiles-908f4ad2-314d-4651-b371-659966683909/fetchFileTemp6857832178355543939.tmp
16/10/30 10:06:12 INFO Executor: Adding file:/tmp/spark-be4e06a7-d9d3-43af-9363-e01ad9703b80/userFiles-908f4ad2-314d-4651-b371-659966683909/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
16/10/30 10:06:12 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:12 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 313 ms on localhost (1/4)
16/10/30 10:06:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 283 ms on localhost (2/4)
16/10/30 10:06:12 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 280 ms on localhost (3/4)
16/10/30 10:06:12 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 34017 bytes result sent to driver
16/10/30 10:06:12 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 379 ms on localhost (4/4)
16/10/30 10:06:12 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.430 s
16/10/30 10:06:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:12 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.701295 s
Result: 3.168457 in 1.067 seconds
16/10/30 10:06:12 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:12 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:12 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:12 INFO BlockManager: BlockManager stopped
16/10/30 10:06:12 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:12 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:12 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-be4e06a7-d9d3-43af-9363-e01ad9703b80
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:14 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:15 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:15 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:15 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:15 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:15 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:15 INFO Utils: Successfully started service 'sparkDriver' on port 44497.
16/10/30 10:06:15 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:15 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2d1abb1c-c97b-40d6-8622-3f5eedb497d5
16/10/30 10:06:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:16 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:16 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44497/jars/jni-spark-0.1.jar with timestamp 1477818376247
16/10/30 10:06:16 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33911.
16/10/30 10:06:16 INFO NettyBlockTransferService: Server created on 192.168.0.17:33911
16/10/30 10:06:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33911)
16/10/30 10:06:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33911 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33911)
16/10/30 10:06:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33911)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:18 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818378334
16/10/30 10:06:18 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d715e31b-8087-4cb6-bd56-5ecf6e7614a2/userFiles-000ce102-5965-49df-8914-d34f9e806a89/SparkJNIPi.so
16/10/30 10:06:18 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:18 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:18 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:18 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:18 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33911 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:19 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:19 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:19 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:19 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:19 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:19 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:19 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:19 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:19 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818378334
16/10/30 10:06:19 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d715e31b-8087-4cb6-bd56-5ecf6e7614a2/userFiles-000ce102-5965-49df-8914-d34f9e806a89/SparkJNIPi.so
16/10/30 10:06:19 INFO Executor: Fetching spark://192.168.0.17:44497/jars/jni-spark-0.1.jar with timestamp 1477818376247
16/10/30 10:06:19 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44497 after 37 ms (0 ms spent in bootstraps)
16/10/30 10:06:19 INFO Utils: Fetching spark://192.168.0.17:44497/jars/jni-spark-0.1.jar to /tmp/spark-d715e31b-8087-4cb6-bd56-5ecf6e7614a2/userFiles-000ce102-5965-49df-8914-d34f9e806a89/fetchFileTemp7315446476831904145.tmp
16/10/30 10:06:19 INFO Executor: Adding file:/tmp/spark-d715e31b-8087-4cb6-bd56-5ecf6e7614a2/userFiles-000ce102-5965-49df-8914-d34f9e806a89/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:19 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:19 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:06:19 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 289 ms on localhost (1/4)
16/10/30 10:06:19 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 315 ms on localhost (2/4)
16/10/30 10:06:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 363 ms on localhost (3/4)
16/10/30 10:06:19 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 34017 bytes result sent to driver
16/10/30 10:06:19 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.489 s
16/10/30 10:06:19 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 410 ms on localhost (4/4)
16/10/30 10:06:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:19 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.910399 s
Result: 3.1577148 in 1.36 seconds
16/10/30 10:06:19 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:19 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:19 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:19 INFO BlockManager: BlockManager stopped
16/10/30 10:06:19 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:19 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:19 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-d715e31b-8087-4cb6-bd56-5ecf6e7614a2
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:20 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:21 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:21 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:21 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:21 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:21 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:21 INFO Utils: Successfully started service 'sparkDriver' on port 44297.
16/10/30 10:06:21 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:21 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b6491672-91e4-4419-baef-f34868ef336b
16/10/30 10:06:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:22 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:22 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44297/jars/jni-spark-0.1.jar with timestamp 1477818382339
16/10/30 10:06:22 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45747.
16/10/30 10:06:22 INFO NettyBlockTransferService: Server created on 192.168.0.17:45747
16/10/30 10:06:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45747)
16/10/30 10:06:22 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45747 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45747)
16/10/30 10:06:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45747)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:24 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818384095
16/10/30 10:06:24 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-449f9d7f-2034-4a02-b286-8303a7fe8c6a/userFiles-c76412a9-7250-473a-a803-8413e4a40db0/SparkJNIPi.so
16/10/30 10:06:24 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:24 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:24 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:24 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:24 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45747 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:24 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:24 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:24 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:24 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:24 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:24 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:24 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:24 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818384095
16/10/30 10:06:24 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-449f9d7f-2034-4a02-b286-8303a7fe8c6a/userFiles-c76412a9-7250-473a-a803-8413e4a40db0/SparkJNIPi.so
16/10/30 10:06:24 INFO Executor: Fetching spark://192.168.0.17:44297/jars/jni-spark-0.1.jar with timestamp 1477818382339
16/10/30 10:06:24 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44297 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:06:24 INFO Utils: Fetching spark://192.168.0.17:44297/jars/jni-spark-0.1.jar to /tmp/spark-449f9d7f-2034-4a02-b286-8303a7fe8c6a/userFiles-c76412a9-7250-473a-a803-8413e4a40db0/fetchFileTemp3984807589719663475.tmp
16/10/30 10:06:24 INFO Executor: Adding file:/tmp/spark-449f9d7f-2034-4a02-b286-8303a7fe8c6a/userFiles-c76412a9-7250-473a-a803-8413e4a40db0/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:24 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:24 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:06:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 277 ms on localhost (1/4)
16/10/30 10:06:24 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 235 ms on localhost (2/4)
16/10/30 10:06:24 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 243 ms on localhost (3/4)
16/10/30 10:06:24 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 34017 bytes result sent to driver
16/10/30 10:06:24 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 325 ms on localhost (4/4)
16/10/30 10:06:24 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.387 s
16/10/30 10:06:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:25 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.617647 s
Result: 3.1611328 in 0.903 seconds
16/10/30 10:06:25 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:25 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:25 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:25 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:25 INFO BlockManager: BlockManager stopped
16/10/30 10:06:25 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:25 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:25 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-449f9d7f-2034-4a02-b286-8303a7fe8c6a
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:26 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:26 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:26 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:26 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:26 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:26 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:27 INFO Utils: Successfully started service 'sparkDriver' on port 46842.
16/10/30 10:06:27 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:27 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2c766804-5bac-47f8-8f81-e6487eeebcf0
16/10/30 10:06:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:27 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:27 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46842/jars/jni-spark-0.1.jar with timestamp 1477818387408
16/10/30 10:06:27 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33537.
16/10/30 10:06:27 INFO NettyBlockTransferService: Server created on 192.168.0.17:33537
16/10/30 10:06:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33537)
16/10/30 10:06:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33537 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33537)
16/10/30 10:06:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33537)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:29 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818389060
16/10/30 10:06:29 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-77046428-18d1-4462-ab8b-0038e31f9291/userFiles-5c518774-996c-4426-b9ef-43c3f12d2749/SparkJNIPi.so
16/10/30 10:06:29 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:29 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:29 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:29 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:29 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33537 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:29 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:29 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:29 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:29 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:29 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:29 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:29 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818389060
16/10/30 10:06:29 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-77046428-18d1-4462-ab8b-0038e31f9291/userFiles-5c518774-996c-4426-b9ef-43c3f12d2749/SparkJNIPi.so
16/10/30 10:06:29 INFO Executor: Fetching spark://192.168.0.17:46842/jars/jni-spark-0.1.jar with timestamp 1477818387408
16/10/30 10:06:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46842 after 41 ms (0 ms spent in bootstraps)
16/10/30 10:06:29 INFO Utils: Fetching spark://192.168.0.17:46842/jars/jni-spark-0.1.jar to /tmp/spark-77046428-18d1-4462-ab8b-0038e31f9291/userFiles-5c518774-996c-4426-b9ef-43c3f12d2749/fetchFileTemp4079311012027006888.tmp
16/10/30 10:06:29 INFO Executor: Adding file:/tmp/spark-77046428-18d1-4462-ab8b-0038e31f9291/userFiles-5c518774-996c-4426-b9ef-43c3f12d2749/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:29 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:06:29 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 858 bytes result sent to driver
16/10/30 10:06:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 383 ms on localhost (1/4)
16/10/30 10:06:30 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 329 ms on localhost (2/4)
16/10/30 10:06:30 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 333 ms on localhost (3/4)
16/10/30 10:06:30 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 34017 bytes result sent to driver
16/10/30 10:06:30 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 448 ms on localhost (4/4)
16/10/30 10:06:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:30 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.531 s
16/10/30 10:06:30 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.832776 s
Result: 3.147461 in 1.085 seconds
16/10/30 10:06:30 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:30 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:30 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:30 INFO BlockManager: BlockManager stopped
16/10/30 10:06:30 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:30 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:30 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-77046428-18d1-4462-ab8b-0038e31f9291
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:31 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:32 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:32 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:32 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:32 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:32 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:32 INFO Utils: Successfully started service 'sparkDriver' on port 39562.
16/10/30 10:06:32 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:32 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c2cebced-01e0-4192-93e5-77cfa353630e
16/10/30 10:06:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:32 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:32 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39562/jars/jni-spark-0.1.jar with timestamp 1477818392834
16/10/30 10:06:32 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45558.
16/10/30 10:06:32 INFO NettyBlockTransferService: Server created on 192.168.0.17:45558
16/10/30 10:06:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45558)
16/10/30 10:06:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45558 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45558)
16/10/30 10:06:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45558)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:34 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818394580
16/10/30 10:06:34 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-4e2bd1c6-a2df-4ad7-966c-766254dc6ad9/userFiles-38ce4e5b-7fda-4b94-a138-0b8adb439fa1/SparkJNIPi.so
16/10/30 10:06:34 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:34 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:34 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:34 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:34 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45558 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:35 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:35 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:35 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:35 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:35 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:35 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:35 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818394580
16/10/30 10:06:35 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-4e2bd1c6-a2df-4ad7-966c-766254dc6ad9/userFiles-38ce4e5b-7fda-4b94-a138-0b8adb439fa1/SparkJNIPi.so
16/10/30 10:06:35 INFO Executor: Fetching spark://192.168.0.17:39562/jars/jni-spark-0.1.jar with timestamp 1477818392834
16/10/30 10:06:35 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39562 after 22 ms (0 ms spent in bootstraps)
16/10/30 10:06:35 INFO Utils: Fetching spark://192.168.0.17:39562/jars/jni-spark-0.1.jar to /tmp/spark-4e2bd1c6-a2df-4ad7-966c-766254dc6ad9/userFiles-38ce4e5b-7fda-4b94-a138-0b8adb439fa1/fetchFileTemp4479135584050046547.tmp
16/10/30 10:06:35 INFO Executor: Adding file:/tmp/spark-4e2bd1c6-a2df-4ad7-966c-766254dc6ad9/userFiles-38ce4e5b-7fda-4b94-a138-0b8adb439fa1/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:35 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:35 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
16/10/30 10:06:35 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 222 ms on localhost (1/4)
16/10/30 10:06:35 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 234 ms on localhost (2/4)
16/10/30 10:06:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 276 ms on localhost (3/4)
16/10/30 10:06:35 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 264513 bytes result sent to driver
16/10/30 10:06:35 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 364 ms on localhost (4/4)
16/10/30 10:06:35 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.416 s
16/10/30 10:06:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:35 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.676728 s
Result: 3.1472778 in 0.945 seconds
16/10/30 10:06:35 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:35 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:35 INFO BlockManager: BlockManager stopped
16/10/30 10:06:35 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:35 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:35 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-4e2bd1c6-a2df-4ad7-966c-766254dc6ad9
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:37 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:37 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:37 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:37 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:37 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:37 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:37 INFO Utils: Successfully started service 'sparkDriver' on port 45556.
16/10/30 10:06:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-12c9b17d-6632-43a4-84a9-8e59ecefc01a
16/10/30 10:06:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:38 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:38 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45556/jars/jni-spark-0.1.jar with timestamp 1477818398290
16/10/30 10:06:38 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38846.
16/10/30 10:06:38 INFO NettyBlockTransferService: Server created on 192.168.0.17:38846
16/10/30 10:06:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38846)
16/10/30 10:06:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38846 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38846)
16/10/30 10:06:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38846)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:40 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818400147
16/10/30 10:06:40 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2108fc17-464b-452c-b701-664cca8236f9/userFiles-53db6c8b-c2a1-432c-86e7-5b82b15c192b/SparkJNIPi.so
16/10/30 10:06:40 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:40 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:40 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:40 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:40 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38846 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:40 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:40 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818400147
16/10/30 10:06:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:40 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:40 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2108fc17-464b-452c-b701-664cca8236f9/userFiles-53db6c8b-c2a1-432c-86e7-5b82b15c192b/SparkJNIPi.so
16/10/30 10:06:40 INFO Executor: Fetching spark://192.168.0.17:45556/jars/jni-spark-0.1.jar with timestamp 1477818398290
16/10/30 10:06:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45556 after 24 ms (0 ms spent in bootstraps)
16/10/30 10:06:40 INFO Utils: Fetching spark://192.168.0.17:45556/jars/jni-spark-0.1.jar to /tmp/spark-2108fc17-464b-452c-b701-664cca8236f9/userFiles-53db6c8b-c2a1-432c-86e7-5b82b15c192b/fetchFileTemp5164647603351667978.tmp
16/10/30 10:06:41 INFO Executor: Adding file:/tmp/spark-2108fc17-464b-452c-b701-664cca8236f9/userFiles-53db6c8b-c2a1-432c-86e7-5b82b15c192b/jni-spark-0.1.jar to class loader
16/10/30 10:06:41 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:41 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:41 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
Calling method randToSum
16/10/30 10:06:41 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 255 ms on localhost (1/4)
16/10/30 10:06:41 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 266 ms on localhost (2/4)
16/10/30 10:06:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 307 ms on localhost (3/4)
16/10/30 10:06:41 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 264513 bytes result sent to driver
16/10/30 10:06:41 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 409 ms on localhost (4/4)
16/10/30 10:06:41 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.463 s
16/10/30 10:06:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:41 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.786743 s
Result: 3.1432495 in 1.069 seconds
16/10/30 10:06:41 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:41 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:41 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:41 INFO BlockManager: BlockManager stopped
16/10/30 10:06:41 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:41 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:41 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-2108fc17-464b-452c-b701-664cca8236f9
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:42 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:43 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:43 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:43 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:43 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:43 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:43 INFO Utils: Successfully started service 'sparkDriver' on port 36974.
16/10/30 10:06:43 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:43 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-90dd611d-685b-40b6-928c-1c99bd7f07b3
16/10/30 10:06:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:43 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:44 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36974/jars/jni-spark-0.1.jar with timestamp 1477818404137
16/10/30 10:06:44 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45394.
16/10/30 10:06:44 INFO NettyBlockTransferService: Server created on 192.168.0.17:45394
16/10/30 10:06:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45394)
16/10/30 10:06:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45394 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45394)
16/10/30 10:06:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45394)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:45 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818405979
16/10/30 10:06:45 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8d53e601-2ff2-4572-ad85-c7ba7613d00c/userFiles-d7bfa3fc-954e-4fb8-b6ba-03e098ea60f3/SparkJNIPi.so
16/10/30 10:06:46 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:46 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:46 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:46 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:46 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45394 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:46 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:46 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:46 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:46 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:46 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:46 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818405979
16/10/30 10:06:46 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8d53e601-2ff2-4572-ad85-c7ba7613d00c/userFiles-d7bfa3fc-954e-4fb8-b6ba-03e098ea60f3/SparkJNIPi.so
16/10/30 10:06:46 INFO Executor: Fetching spark://192.168.0.17:36974/jars/jni-spark-0.1.jar with timestamp 1477818404137
16/10/30 10:06:46 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36974 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:06:46 INFO Utils: Fetching spark://192.168.0.17:36974/jars/jni-spark-0.1.jar to /tmp/spark-8d53e601-2ff2-4572-ad85-c7ba7613d00c/userFiles-d7bfa3fc-954e-4fb8-b6ba-03e098ea60f3/fetchFileTemp2346894043927735825.tmp
16/10/30 10:06:46 INFO Executor: Adding file:/tmp/spark-8d53e601-2ff2-4572-ad85-c7ba7613d00c/userFiles-d7bfa3fc-954e-4fb8-b6ba-03e098ea60f3/jni-spark-0.1.jar to class loader
16/10/30 10:06:46 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
Calling method randToSum
16/10/30 10:06:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:06:46 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 265 ms on localhost (1/4)
16/10/30 10:06:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 281 ms on localhost (2/4)
16/10/30 10:06:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 318 ms on localhost (3/4)
16/10/30 10:06:47 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 264513 bytes result sent to driver
16/10/30 10:06:47 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 401 ms on localhost (4/4)
16/10/30 10:06:47 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.464 s
16/10/30 10:06:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:47 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.779604 s
Result: 3.1345215 in 1.056 seconds
16/10/30 10:06:47 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:47 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:47 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:47 INFO BlockManager: BlockManager stopped
16/10/30 10:06:47 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:47 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:47 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d53e601-2ff2-4572-ad85-c7ba7613d00c
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:48 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:48 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:48 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:48 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:48 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:48 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:49 INFO Utils: Successfully started service 'sparkDriver' on port 37180.
16/10/30 10:06:49 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:49 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1fd5bee2-b212-4f25-bfeb-ec1360686246
16/10/30 10:06:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:49 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:49 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37180/jars/jni-spark-0.1.jar with timestamp 1477818409514
16/10/30 10:06:49 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38602.
16/10/30 10:06:49 INFO NettyBlockTransferService: Server created on 192.168.0.17:38602
16/10/30 10:06:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38602)
16/10/30 10:06:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38602 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38602)
16/10/30 10:06:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38602)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:51 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818411293
16/10/30 10:06:51 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-565b4a3c-d4a4-41c6-a407-83bf40f86149/userFiles-354150f5-97b0-49ce-b893-d91d285124b0/SparkJNIPi.so
16/10/30 10:06:51 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:51 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:51 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:51 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:51 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38602 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:51 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:51 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:51 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:51 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:51 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818411293
16/10/30 10:06:51 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:52 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-565b4a3c-d4a4-41c6-a407-83bf40f86149/userFiles-354150f5-97b0-49ce-b893-d91d285124b0/SparkJNIPi.so
16/10/30 10:06:52 INFO Executor: Fetching spark://192.168.0.17:37180/jars/jni-spark-0.1.jar with timestamp 1477818409514
16/10/30 10:06:52 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37180 after 40 ms (0 ms spent in bootstraps)
16/10/30 10:06:52 INFO Utils: Fetching spark://192.168.0.17:37180/jars/jni-spark-0.1.jar to /tmp/spark-565b4a3c-d4a4-41c6-a407-83bf40f86149/userFiles-354150f5-97b0-49ce-b893-d91d285124b0/fetchFileTemp7461836972316036600.tmp
16/10/30 10:06:52 INFO Executor: Adding file:/tmp/spark-565b4a3c-d4a4-41c6-a407-83bf40f86149/userFiles-354150f5-97b0-49ce-b893-d91d285124b0/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:06:52 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:06:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:06:52 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 516 ms on localhost (1/4)
16/10/30 10:06:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 495 ms on localhost (2/4)
16/10/30 10:06:52 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 493 ms on localhost (3/4)
16/10/30 10:06:52 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 264513 bytes result sent to driver
16/10/30 10:06:52 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 655 ms on localhost (4/4)
16/10/30 10:06:52 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.709 s
16/10/30 10:06:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:52 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.008751 s
Result: 3.1412354 in 1.323 seconds
16/10/30 10:06:52 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:52 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:52 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:52 INFO BlockManager: BlockManager stopped
16/10/30 10:06:52 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:52 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:52 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-565b4a3c-d4a4-41c6-a407-83bf40f86149
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:06:54 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:06:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:06:54 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:06:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:06:54 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:06:54 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:06:54 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:06:54 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:06:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:06:55 INFO Utils: Successfully started service 'sparkDriver' on port 42828.
16/10/30 10:06:55 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:06:55 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:06:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6a8f8334-171e-4032-a399-fd01e959ac3d
16/10/30 10:06:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:06:55 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:06:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:06:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:06:55 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42828/jars/jni-spark-0.1.jar with timestamp 1477818415767
16/10/30 10:06:55 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:06:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37508.
16/10/30 10:06:55 INFO NettyBlockTransferService: Server created on 192.168.0.17:37508
16/10/30 10:06:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37508)
16/10/30 10:06:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37508 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37508)
16/10/30 10:06:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37508)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:06:58 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818418046
16/10/30 10:06:58 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-846b5f85-6e62-4f98-ae06-681777e154b1/userFiles-7a61ef40-f870-4cda-a70a-d9d073f47d23/SparkJNIPi.so
16/10/30 10:06:58 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:06:58 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:06:58 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:06:58 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:06:58 INFO DAGScheduler: Missing parents: List()
16/10/30 10:06:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:06:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:06:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:06:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37508 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:06:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:06:58 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:06:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:06:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:58 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:58 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:06:58 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:06:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:06:58 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:06:58 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:06:58 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:06:58 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818418046
16/10/30 10:06:58 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-846b5f85-6e62-4f98-ae06-681777e154b1/userFiles-7a61ef40-f870-4cda-a70a-d9d073f47d23/SparkJNIPi.so
16/10/30 10:06:58 INFO Executor: Fetching spark://192.168.0.17:42828/jars/jni-spark-0.1.jar with timestamp 1477818415767
16/10/30 10:06:58 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42828 after 32 ms (0 ms spent in bootstraps)
16/10/30 10:06:58 INFO Utils: Fetching spark://192.168.0.17:42828/jars/jni-spark-0.1.jar to /tmp/spark-846b5f85-6e62-4f98-ae06-681777e154b1/userFiles-7a61ef40-f870-4cda-a70a-d9d073f47d23/fetchFileTemp7512957234739127246.tmp
16/10/30 10:06:59 INFO Executor: Adding file:/tmp/spark-846b5f85-6e62-4f98-ae06-681777e154b1/userFiles-7a61ef40-f870-4cda-a70a-d9d073f47d23/jni-spark-0.1.jar to class loader
16/10/30 10:06:59 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:06:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 858 bytes result sent to driver
16/10/30 10:06:59 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 858 bytes result sent to driver
Calling method randToSum
16/10/30 10:06:59 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 298 ms on localhost (1/4)
16/10/30 10:06:59 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 305 ms on localhost (2/4)
16/10/30 10:06:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 349 ms on localhost (3/4)
16/10/30 10:06:59 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:06:59 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:37508 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:06:59 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108481 bytes result sent via BlockManager)
16/10/30 10:06:59 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37508 after 3 ms (0 ms spent in bootstraps)
16/10/30 10:06:59 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:37508 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:06:59 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.577 s
16/10/30 10:06:59 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 499 ms on localhost (4/4)
16/10/30 10:06:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:06:59 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.995770 s
Result: 3.1415024 in 1.312 seconds
16/10/30 10:06:59 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:06:59 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:06:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:06:59 INFO MemoryStore: MemoryStore cleared
16/10/30 10:06:59 INFO BlockManager: BlockManager stopped
16/10/30 10:06:59 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:06:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:06:59 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:06:59 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:06:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-846b5f85-6e62-4f98-ae06-681777e154b1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:00 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:01 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:01 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:01 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:01 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:01 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:01 INFO Utils: Successfully started service 'sparkDriver' on port 39432.
16/10/30 10:07:01 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:01 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e669892-b316-4b55-bff8-9fe81a983678
16/10/30 10:07:01 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:02 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:02 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39432/jars/jni-spark-0.1.jar with timestamp 1477818422290
16/10/30 10:07:02 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33389.
16/10/30 10:07:02 INFO NettyBlockTransferService: Server created on 192.168.0.17:33389
16/10/30 10:07:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33389)
16/10/30 10:07:02 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33389 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33389)
16/10/30 10:07:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33389)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:04 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818424347
16/10/30 10:07:04 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-746c3dbf-1ca6-4fa3-854e-fab7fa5ab653/userFiles-ea3dbca3-ffad-4f04-a3bb-e29167dada12/SparkJNIPi.so
16/10/30 10:07:04 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:04 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:04 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:04 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:04 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:04 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33389 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:04 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:04 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:04 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:04 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:04 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:04 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:04 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:05 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818424347
16/10/30 10:07:05 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-746c3dbf-1ca6-4fa3-854e-fab7fa5ab653/userFiles-ea3dbca3-ffad-4f04-a3bb-e29167dada12/SparkJNIPi.so
16/10/30 10:07:05 INFO Executor: Fetching spark://192.168.0.17:39432/jars/jni-spark-0.1.jar with timestamp 1477818422290
16/10/30 10:07:05 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39432 after 51 ms (0 ms spent in bootstraps)
16/10/30 10:07:05 INFO Utils: Fetching spark://192.168.0.17:39432/jars/jni-spark-0.1.jar to /tmp/spark-746c3dbf-1ca6-4fa3-854e-fab7fa5ab653/userFiles-ea3dbca3-ffad-4f04-a3bb-e29167dada12/fetchFileTemp9015166368491276507.tmp
16/10/30 10:07:05 INFO Executor: Adding file:/tmp/spark-746c3dbf-1ca6-4fa3-854e-fab7fa5ab653/userFiles-ea3dbca3-ffad-4f04-a3bb-e29167dada12/jni-spark-0.1.jar to class loader
16/10/30 10:07:05 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 844 bytes result sent to driver
16/10/30 10:07:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 844 bytes result sent to driver
16/10/30 10:07:05 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 844 bytes result sent to driver
Calling method randToSum
16/10/30 10:07:05 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 395 ms on localhost (1/4)
16/10/30 10:07:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 452 ms on localhost (2/4)
16/10/30 10:07:05 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 397 ms on localhost (3/4)
16/10/30 10:07:05 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:07:05 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:33389 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:07:05 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108554 bytes result sent via BlockManager)
16/10/30 10:07:05 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33389 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:07:05 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:33389 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:07:05 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 591 ms on localhost (4/4)
16/10/30 10:07:05 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.676 s
16/10/30 10:07:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:05 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.960673 s
Result: 3.1457596 in 1.235 seconds
16/10/30 10:07:05 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:05 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:05 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:05 INFO BlockManager: BlockManager stopped
16/10/30 10:07:05 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:05 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:05 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-746c3dbf-1ca6-4fa3-854e-fab7fa5ab653
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:06 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:07 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:07 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:07 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:07 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:07 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:08 INFO Utils: Successfully started service 'sparkDriver' on port 40320.
16/10/30 10:07:08 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:08 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3adec681-5011-49da-bb58-debc5d6463d6
16/10/30 10:07:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:08 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:08 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:08 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40320/jars/jni-spark-0.1.jar with timestamp 1477818428496
16/10/30 10:07:08 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42281.
16/10/30 10:07:08 INFO NettyBlockTransferService: Server created on 192.168.0.17:42281
16/10/30 10:07:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42281)
16/10/30 10:07:08 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42281 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42281)
16/10/30 10:07:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42281)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:10 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818430209
16/10/30 10:07:10 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-5808f846-0341-4286-bce5-254f95500fe8/userFiles-40e66161-705a-4aba-aa68-02fb8aed9f9d/SparkJNIPi.so
16/10/30 10:07:10 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:10 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:10 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:10 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:10 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42281 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:10 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:10 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:10 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:10 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:10 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:10 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:10 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:10 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818430209
16/10/30 10:07:10 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-5808f846-0341-4286-bce5-254f95500fe8/userFiles-40e66161-705a-4aba-aa68-02fb8aed9f9d/SparkJNIPi.so
16/10/30 10:07:10 INFO Executor: Fetching spark://192.168.0.17:40320/jars/jni-spark-0.1.jar with timestamp 1477818428496
16/10/30 10:07:10 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40320 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:07:10 INFO Utils: Fetching spark://192.168.0.17:40320/jars/jni-spark-0.1.jar to /tmp/spark-5808f846-0341-4286-bce5-254f95500fe8/userFiles-40e66161-705a-4aba-aa68-02fb8aed9f9d/fetchFileTemp3656907621225092694.tmp
16/10/30 10:07:11 INFO Executor: Adding file:/tmp/spark-5808f846-0341-4286-bce5-254f95500fe8/userFiles-40e66161-705a-4aba-aa68-02fb8aed9f9d/jni-spark-0.1.jar to class loader
16/10/30 10:07:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:07:11 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 858 bytes result sent to driver
16/10/30 10:07:11 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:11 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 265 ms on localhost (1/4)
Calling method randToSum
16/10/30 10:07:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 326 ms on localhost (2/4)
16/10/30 10:07:11 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 289 ms on localhost (3/4)
16/10/30 10:07:11 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:07:11 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:42281 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:07:11 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108481 bytes result sent via BlockManager)
16/10/30 10:07:11 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42281 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:07:11 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:42281 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:07:11 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.703 s
16/10/30 10:07:11 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 637 ms on localhost (4/4)
16/10/30 10:07:11 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.013103 s
16/10/30 10:07:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
Result: 3.1384888 in 1.294 seconds
16/10/30 10:07:11 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:11 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:11 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:11 INFO BlockManager: BlockManager stopped
16/10/30 10:07:11 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:11 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:11 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-5808f846-0341-4286-bce5-254f95500fe8
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:12 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:13 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:13 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:13 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:13 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:13 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:13 INFO Utils: Successfully started service 'sparkDriver' on port 46166.
16/10/30 10:07:13 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:13 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5a59e1fb-2fc5-469a-addb-26b9093723ac
16/10/30 10:07:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:13 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:14 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46166/jars/jni-spark-0.1.jar with timestamp 1477818434014
16/10/30 10:07:14 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46049.
16/10/30 10:07:14 INFO NettyBlockTransferService: Server created on 192.168.0.17:46049
16/10/30 10:07:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46049)
16/10/30 10:07:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46049 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46049)
16/10/30 10:07:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46049)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:15 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818435799
16/10/30 10:07:15 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-f34ac45e-8ba4-4466-a1e3-5706277245ca/userFiles-cc2091a4-589e-456c-be87-61094be02438/SparkJNIPi.so
16/10/30 10:07:16 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:16 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:16 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:16 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:16 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46049 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:16 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:16 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:16 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:16 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:16 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:16 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:16 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818435799
16/10/30 10:07:16 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-f34ac45e-8ba4-4466-a1e3-5706277245ca/userFiles-cc2091a4-589e-456c-be87-61094be02438/SparkJNIPi.so
16/10/30 10:07:16 INFO Executor: Fetching spark://192.168.0.17:46166/jars/jni-spark-0.1.jar with timestamp 1477818434014
16/10/30 10:07:16 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46166 after 43 ms (0 ms spent in bootstraps)
16/10/30 10:07:16 INFO Utils: Fetching spark://192.168.0.17:46166/jars/jni-spark-0.1.jar to /tmp/spark-f34ac45e-8ba4-4466-a1e3-5706277245ca/userFiles-cc2091a4-589e-456c-be87-61094be02438/fetchFileTemp5755166874092036364.tmp
16/10/30 10:07:16 INFO Executor: Adding file:/tmp/spark-f34ac45e-8ba4-4466-a1e3-5706277245ca/userFiles-cc2091a4-589e-456c-be87-61094be02438/jni-spark-0.1.jar to class loader
16/10/30 10:07:16 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:07:16 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:07:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 280 ms on localhost (1/4)
16/10/30 10:07:16 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 245 ms on localhost (2/4)
16/10/30 10:07:16 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 243 ms on localhost (3/4)
Calling method randToSum
16/10/30 10:07:16 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:07:16 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:46049 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:07:16 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108481 bytes result sent via BlockManager)
16/10/30 10:07:16 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46049 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:07:17 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:46049 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:07:17 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 616 ms on localhost (4/4)
16/10/30 10:07:17 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.679 s
16/10/30 10:07:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:17 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.947260 s
Result: 3.143425 in 1.248 seconds
16/10/30 10:07:17 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:17 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:17 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:17 INFO BlockManager: BlockManager stopped
16/10/30 10:07:17 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:17 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:17 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-f34ac45e-8ba4-4466-a1e3-5706277245ca
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:18 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:18 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:18 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:18 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:18 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:18 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:19 INFO Utils: Successfully started service 'sparkDriver' on port 33827.
16/10/30 10:07:19 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:19 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e6f247f2-ae24-4d29-97c1-9fbb05a4fce8
16/10/30 10:07:19 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:19 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:19 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33827/jars/jni-spark-0.1.jar with timestamp 1477818439545
16/10/30 10:07:19 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:19 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43270.
16/10/30 10:07:19 INFO NettyBlockTransferService: Server created on 192.168.0.17:43270
16/10/30 10:07:19 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43270)
16/10/30 10:07:19 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43270 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43270)
16/10/30 10:07:19 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43270)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:21 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818441440
16/10/30 10:07:21 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-942fe624-fdda-4db4-a575-625421fefecb/userFiles-1de35fe9-3550-4c23-99af-c9bd9807d238/SparkJNIPi.so
16/10/30 10:07:21 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:21 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:21 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:21 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:21 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43270 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:21 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:22 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:22 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:22 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:22 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:22 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:22 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:22 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818441440
16/10/30 10:07:22 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-942fe624-fdda-4db4-a575-625421fefecb/userFiles-1de35fe9-3550-4c23-99af-c9bd9807d238/SparkJNIPi.so
16/10/30 10:07:22 INFO Executor: Fetching spark://192.168.0.17:33827/jars/jni-spark-0.1.jar with timestamp 1477818439545
16/10/30 10:07:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33827 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:07:22 INFO Utils: Fetching spark://192.168.0.17:33827/jars/jni-spark-0.1.jar to /tmp/spark-942fe624-fdda-4db4-a575-625421fefecb/userFiles-1de35fe9-3550-4c23-99af-c9bd9807d238/fetchFileTemp2278953333556767481.tmp
16/10/30 10:07:22 INFO Executor: Adding file:/tmp/spark-942fe624-fdda-4db4-a575-625421fefecb/userFiles-1de35fe9-3550-4c23-99af-c9bd9807d238/jni-spark-0.1.jar to class loader
16/10/30 10:07:22 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:07:22 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
16/10/30 10:07:22 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 489 ms on localhost (1/4)
16/10/30 10:07:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 556 ms on localhost (2/4)
16/10/30 10:07:22 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 513 ms on localhost (3/4)
16/10/30 10:07:22 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:07:22 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:43270 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:07:22 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:07:22 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43270 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:07:23 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:43270 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:07:23 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 929 ms on localhost (4/4)
16/10/30 10:07:23 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.053 s
16/10/30 10:07:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:23 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.340276 s
Result: 3.1420956 in 1.623 seconds
16/10/30 10:07:23 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:23 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:23 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:23 INFO BlockManager: BlockManager stopped
16/10/30 10:07:23 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:23 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:23 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-942fe624-fdda-4db4-a575-625421fefecb
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:24 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:24 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:24 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:24 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:24 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:24 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:25 INFO Utils: Successfully started service 'sparkDriver' on port 45030.
16/10/30 10:07:25 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:25 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e4bf4c1d-ae67-4974-8d96-ce964599c31d
16/10/30 10:07:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:25 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:25 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45030/jars/jni-spark-0.1.jar with timestamp 1477818445697
16/10/30 10:07:25 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39080.
16/10/30 10:07:25 INFO NettyBlockTransferService: Server created on 192.168.0.17:39080
16/10/30 10:07:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39080)
16/10/30 10:07:25 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39080 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39080)
16/10/30 10:07:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39080)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:27 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818447566
16/10/30 10:07:27 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-79e26e67-85fd-45d6-ae12-cfde7e2c921a/userFiles-ff193fa6-44fc-424c-94d5-9e72a040b1bf/SparkJNIPi.so
16/10/30 10:07:27 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:27 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:27 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:27 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:27 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39080 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:28 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:28 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:28 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:28 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:28 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:28 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:28 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818447566
16/10/30 10:07:28 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-79e26e67-85fd-45d6-ae12-cfde7e2c921a/userFiles-ff193fa6-44fc-424c-94d5-9e72a040b1bf/SparkJNIPi.so
16/10/30 10:07:28 INFO Executor: Fetching spark://192.168.0.17:45030/jars/jni-spark-0.1.jar with timestamp 1477818445697
16/10/30 10:07:28 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45030 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:07:28 INFO Utils: Fetching spark://192.168.0.17:45030/jars/jni-spark-0.1.jar to /tmp/spark-79e26e67-85fd-45d6-ae12-cfde7e2c921a/userFiles-ff193fa6-44fc-424c-94d5-9e72a040b1bf/fetchFileTemp7657790793600712560.tmp
16/10/30 10:07:28 INFO Executor: Adding file:/tmp/spark-79e26e67-85fd-45d6-ae12-cfde7e2c921a/userFiles-ff193fa6-44fc-424c-94d5-9e72a040b1bf/jni-spark-0.1.jar to class loader
16/10/30 10:07:28 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:07:28 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:28 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:07:28 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 286 ms on localhost (1/4)
16/10/30 10:07:28 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 269 ms on localhost (2/4)
16/10/30 10:07:28 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 272 ms on localhost (3/4)
Calling method randToSum
16/10/30 10:07:29 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:07:29 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:39080 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:07:29 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860225 bytes result sent via BlockManager)
16/10/30 10:07:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39080 after 4 ms (0 ms spent in bootstraps)
16/10/30 10:07:29 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:39080 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:07:29 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 920 ms on localhost (4/4)
16/10/30 10:07:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:29 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.036 s
16/10/30 10:07:29 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.354382 s
Result: 3.142088 in 1.634 seconds
16/10/30 10:07:29 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:29 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:29 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:29 INFO BlockManager: BlockManager stopped
16/10/30 10:07:29 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:29 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:29 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-79e26e67-85fd-45d6-ae12-cfde7e2c921a
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:30 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:30 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:30 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:30 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:30 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:30 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:31 INFO Utils: Successfully started service 'sparkDriver' on port 34533.
16/10/30 10:07:31 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:31 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e494570-8831-41fd-b7b5-335ac227b785
16/10/30 10:07:31 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:31 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:31 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34533/jars/jni-spark-0.1.jar with timestamp 1477818451703
16/10/30 10:07:31 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40028.
16/10/30 10:07:31 INFO NettyBlockTransferService: Server created on 192.168.0.17:40028
16/10/30 10:07:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40028)
16/10/30 10:07:31 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40028 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40028)
16/10/30 10:07:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40028)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:33 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818453574
16/10/30 10:07:33 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-78a06e7b-b49a-414f-bb0d-0dc155f4585c/userFiles-894c59b0-a0bf-45e9-aafb-1b6d2e5f7fc2/SparkJNIPi.so
16/10/30 10:07:33 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:33 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:33 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:33 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:33 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:33 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40028 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:34 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:34 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:34 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818453574
16/10/30 10:07:34 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:34 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:34 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:34 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-78a06e7b-b49a-414f-bb0d-0dc155f4585c/userFiles-894c59b0-a0bf-45e9-aafb-1b6d2e5f7fc2/SparkJNIPi.so
16/10/30 10:07:34 INFO Executor: Fetching spark://192.168.0.17:34533/jars/jni-spark-0.1.jar with timestamp 1477818451703
16/10/30 10:07:34 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34533 after 24 ms (0 ms spent in bootstraps)
16/10/30 10:07:34 INFO Utils: Fetching spark://192.168.0.17:34533/jars/jni-spark-0.1.jar to /tmp/spark-78a06e7b-b49a-414f-bb0d-0dc155f4585c/userFiles-894c59b0-a0bf-45e9-aafb-1b6d2e5f7fc2/fetchFileTemp4031219195720682011.tmp
16/10/30 10:07:34 INFO Executor: Adding file:/tmp/spark-78a06e7b-b49a-414f-bb0d-0dc155f4585c/userFiles-894c59b0-a0bf-45e9-aafb-1b6d2e5f7fc2/jni-spark-0.1.jar to class loader
16/10/30 10:07:34 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 858 bytes result sent to driver
Calling method randToSum
16/10/30 10:07:34 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:07:34 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 460 ms on localhost (1/4)
16/10/30 10:07:34 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 467 ms on localhost (2/4)
16/10/30 10:07:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 511 ms on localhost (3/4)
16/10/30 10:07:34 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:07:34 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:40028 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:07:34 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860225 bytes result sent via BlockManager)
16/10/30 10:07:34 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40028 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:07:35 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:40028 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:07:35 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 881 ms on localhost (4/4)
16/10/30 10:07:35 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.968 s
16/10/30 10:07:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:35 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.264815 s
Result: 3.1419783 in 1.549 seconds
16/10/30 10:07:35 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:35 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:35 INFO BlockManager: BlockManager stopped
16/10/30 10:07:35 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:35 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:35 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-78a06e7b-b49a-414f-bb0d-0dc155f4585c
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:36 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:36 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:36 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:36 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:36 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:37 INFO Utils: Successfully started service 'sparkDriver' on port 42220.
16/10/30 10:07:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-469f72f0-fc33-40b3-9b74-f9d658f813e7
16/10/30 10:07:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42220/jars/jni-spark-0.1.jar with timestamp 1477818457652
16/10/30 10:07:37 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41187.
16/10/30 10:07:37 INFO NettyBlockTransferService: Server created on 192.168.0.17:41187
16/10/30 10:07:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41187)
16/10/30 10:07:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41187 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41187)
16/10/30 10:07:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41187)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818459419
16/10/30 10:07:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-1817f328-3c5e-45dd-baa8-5697923eef19/userFiles-699a048e-2fe9-4285-9158-f946785a33a9/SparkJNIPi.so
16/10/30 10:07:39 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:39 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:39 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:39 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:39 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41187 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:39 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:40 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:40 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:40 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:40 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818459419
16/10/30 10:07:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-1817f328-3c5e-45dd-baa8-5697923eef19/userFiles-699a048e-2fe9-4285-9158-f946785a33a9/SparkJNIPi.so
16/10/30 10:07:40 INFO Executor: Fetching spark://192.168.0.17:42220/jars/jni-spark-0.1.jar with timestamp 1477818457652
16/10/30 10:07:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42220 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:07:40 INFO Utils: Fetching spark://192.168.0.17:42220/jars/jni-spark-0.1.jar to /tmp/spark-1817f328-3c5e-45dd-baa8-5697923eef19/userFiles-699a048e-2fe9-4285-9158-f946785a33a9/fetchFileTemp7113962032979690742.tmp
16/10/30 10:07:40 INFO Executor: Adding file:/tmp/spark-1817f328-3c5e-45dd-baa8-5697923eef19/userFiles-699a048e-2fe9-4285-9158-f946785a33a9/jni-spark-0.1.jar to class loader
16/10/30 10:07:40 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:07:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:07:40 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 286 ms on localhost (1/4)
16/10/30 10:07:40 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 464 ms on localhost (2/4)
16/10/30 10:07:40 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 467 ms on localhost (3/4)
Calling method randToSum
16/10/30 10:07:40 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:07:40 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:41187 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:07:40 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:07:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41187 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:07:41 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:41187 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:07:41 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 953 ms on localhost (4/4)
16/10/30 10:07:41 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.045 s
16/10/30 10:07:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:41 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.355135 s
Result: 3.1422586 in 1.625 seconds
16/10/30 10:07:41 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:41 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:41 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:41 INFO BlockManager: BlockManager stopped
16/10/30 10:07:41 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:41 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:41 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-1817f328-3c5e-45dd-baa8-5697923eef19
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:42 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:42 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:42 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:42 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:42 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:42 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:43 INFO Utils: Successfully started service 'sparkDriver' on port 42438.
16/10/30 10:07:43 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:43 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-717d447c-651a-40e9-b034-59c77dfcfc05
16/10/30 10:07:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:43 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:43 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42438/jars/jni-spark-0.1.jar with timestamp 1477818463576
16/10/30 10:07:43 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34324.
16/10/30 10:07:43 INFO NettyBlockTransferService: Server created on 192.168.0.17:34324
16/10/30 10:07:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34324)
16/10/30 10:07:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34324 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34324)
16/10/30 10:07:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34324)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:45 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818465309
16/10/30 10:07:45 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-fe3d7036-6bc7-40a3-8ae9-8435c8c8865f/userFiles-b0df1545-933d-4ebc-bc22-54679408134a/SparkJNIPi.so
16/10/30 10:07:45 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:45 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34324 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:45 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:45 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:45 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:45 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:45 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818465309
16/10/30 10:07:46 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-fe3d7036-6bc7-40a3-8ae9-8435c8c8865f/userFiles-b0df1545-933d-4ebc-bc22-54679408134a/SparkJNIPi.so
16/10/30 10:07:46 INFO Executor: Fetching spark://192.168.0.17:42438/jars/jni-spark-0.1.jar with timestamp 1477818463576
16/10/30 10:07:46 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42438 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:07:46 INFO Utils: Fetching spark://192.168.0.17:42438/jars/jni-spark-0.1.jar to /tmp/spark-fe3d7036-6bc7-40a3-8ae9-8435c8c8865f/userFiles-b0df1545-933d-4ebc-bc22-54679408134a/fetchFileTemp699465644132291764.tmp
16/10/30 10:07:46 INFO Executor: Adding file:/tmp/spark-fe3d7036-6bc7-40a3-8ae9-8435c8c8865f/userFiles-b0df1545-933d-4ebc-bc22-54679408134a/jni-spark-0.1.jar to class loader
16/10/30 10:07:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:07:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:07:46 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 299 ms on localhost (1/4)
16/10/30 10:07:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 346 ms on localhost (2/4)
16/10/30 10:07:46 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 315 ms on localhost (3/4)
Calling method randToSum
16/10/30 10:07:48 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:07:48 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:34324 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:07:48 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 134874250 bytes result sent via BlockManager)
16/10/30 10:07:48 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34324 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:07:49 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:34324 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:07:49 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 3500 ms on localhost (4/4)
16/10/30 10:07:49 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.730 s
16/10/30 10:07:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:49 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 4.040271 s
Result: 3.1418037 in 4.341 seconds
16/10/30 10:07:49 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:49 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:49 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:49 INFO BlockManager: BlockManager stopped
16/10/30 10:07:49 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:49 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:49 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-fe3d7036-6bc7-40a3-8ae9-8435c8c8865f
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:50 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:07:51 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:07:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:07:51 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:07:51 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:07:51 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:07:51 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:07:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:07:51 INFO Utils: Successfully started service 'sparkDriver' on port 32861.
16/10/30 10:07:51 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:07:51 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:07:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b93e1864-b420-4298-af5a-a68ffaa4863c
16/10/30 10:07:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:07:51 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:07:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:07:52 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:07:52 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:32861/jars/jni-spark-0.1.jar with timestamp 1477818472169
16/10/30 10:07:52 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:07:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33885.
16/10/30 10:07:52 INFO NettyBlockTransferService: Server created on 192.168.0.17:33885
16/10/30 10:07:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33885)
16/10/30 10:07:52 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33885 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33885)
16/10/30 10:07:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33885)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:07:54 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818474016
16/10/30 10:07:54 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-ce856856-1d15-4f2c-90e4-1c964e734a60/userFiles-2dcabe35-fc3b-4cc0-81b1-dbe56e39a4c9/SparkJNIPi.so
16/10/30 10:07:54 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:07:54 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:07:54 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:07:54 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:07:54 INFO DAGScheduler: Missing parents: List()
16/10/30 10:07:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:07:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:07:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:07:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33885 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:07:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:07:54 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:07:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:07:54 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:54 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:54 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:07:54 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:07:54 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:07:54 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:07:54 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:07:54 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:07:54 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818474016
16/10/30 10:07:54 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-ce856856-1d15-4f2c-90e4-1c964e734a60/userFiles-2dcabe35-fc3b-4cc0-81b1-dbe56e39a4c9/SparkJNIPi.so
16/10/30 10:07:54 INFO Executor: Fetching spark://192.168.0.17:32861/jars/jni-spark-0.1.jar with timestamp 1477818472169
16/10/30 10:07:54 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:32861 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:07:54 INFO Utils: Fetching spark://192.168.0.17:32861/jars/jni-spark-0.1.jar to /tmp/spark-ce856856-1d15-4f2c-90e4-1c964e734a60/userFiles-2dcabe35-fc3b-4cc0-81b1-dbe56e39a4c9/fetchFileTemp8842338308426570895.tmp
16/10/30 10:07:54 INFO Executor: Adding file:/tmp/spark-ce856856-1d15-4f2c-90e4-1c964e734a60/userFiles-2dcabe35-fc3b-4cc0-81b1-dbe56e39a4c9/jni-spark-0.1.jar to class loader
16/10/30 10:07:54 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:07:54 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:07:54 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
16/10/30 10:07:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2032 ms on localhost (1/4)
16/10/30 10:07:56 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2036 ms on localhost (2/4)
16/10/30 10:07:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2085 ms on localhost (3/4)
16/10/30 10:07:57 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:07:57 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:33885 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:07:57 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 134874250 bytes result sent via BlockManager)
16/10/30 10:07:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33885 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:07:58 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:33885 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:07:58 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 3396 ms on localhost (4/4)
16/10/30 10:07:58 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.613 s
16/10/30 10:07:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:07:58 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.916473 s
Result: 3.1416287 in 4.226 seconds
16/10/30 10:07:58 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:07:58 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:07:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:07:58 INFO MemoryStore: MemoryStore cleared
16/10/30 10:07:58 INFO BlockManager: BlockManager stopped
16/10/30 10:07:58 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:07:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:07:58 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:07:58 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:07:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-ce856856-1d15-4f2c-90e4-1c964e734a60
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:07:59 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:07:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:00 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:00 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:00 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:00 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:00 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:00 INFO Utils: Successfully started service 'sparkDriver' on port 35258.
16/10/30 10:08:00 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:00 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-13c9ecd9-9edb-4a46-85fc-a848468d1e02
16/10/30 10:08:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:00 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:00 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35258/jars/jni-spark-0.1.jar with timestamp 1477818480864
16/10/30 10:08:00 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33829.
16/10/30 10:08:00 INFO NettyBlockTransferService: Server created on 192.168.0.17:33829
16/10/30 10:08:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33829)
16/10/30 10:08:00 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33829 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33829)
16/10/30 10:08:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33829)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:02 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818482658
16/10/30 10:08:02 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-cc596aa1-051e-471f-bf93-b5cd3fb39315/userFiles-1c92dd6d-8076-4da0-a4e6-23494e036e76/SparkJNIPi.so
16/10/30 10:08:02 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:02 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:02 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:02 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:02 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:02 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33829 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:03 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:03 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:03 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:03 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:03 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:03 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818482658
16/10/30 10:08:03 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-cc596aa1-051e-471f-bf93-b5cd3fb39315/userFiles-1c92dd6d-8076-4da0-a4e6-23494e036e76/SparkJNIPi.so
16/10/30 10:08:03 INFO Executor: Fetching spark://192.168.0.17:35258/jars/jni-spark-0.1.jar with timestamp 1477818480864
16/10/30 10:08:03 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35258 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:08:03 INFO Utils: Fetching spark://192.168.0.17:35258/jars/jni-spark-0.1.jar to /tmp/spark-cc596aa1-051e-471f-bf93-b5cd3fb39315/userFiles-1c92dd6d-8076-4da0-a4e6-23494e036e76/fetchFileTemp8336206276632277869.tmp
16/10/30 10:08:03 INFO Executor: Adding file:/tmp/spark-cc596aa1-051e-471f-bf93-b5cd3fb39315/userFiles-1c92dd6d-8076-4da0-a4e6-23494e036e76/jni-spark-0.1.jar to class loader
16/10/30 10:08:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:03 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:08:03 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:08:03 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 334 ms on localhost (1/4)
16/10/30 10:08:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 386 ms on localhost (2/4)
16/10/30 10:08:03 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 348 ms on localhost (3/4)
Calling method randToSum
16/10/30 10:08:06 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:08:06 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:33829 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:08:06 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 134874250 bytes result sent via BlockManager)
16/10/30 10:08:06 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33829 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:08:06 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:33829 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:08:06 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 3444 ms on localhost (4/4)
16/10/30 10:08:06 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.669 s
16/10/30 10:08:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:08:06 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 3.986173 s
Result: 3.141468 in 4.285 seconds
16/10/30 10:08:06 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:08:06 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:08:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:08:06 INFO MemoryStore: MemoryStore cleared
16/10/30 10:08:06 INFO BlockManager: BlockManager stopped
16/10/30 10:08:06 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:08:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:08:07 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:08:07 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:08:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc596aa1-051e-471f-bf93-b5cd3fb39315
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:08 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:08 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:08 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:08 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:08 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:08 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:09 INFO Utils: Successfully started service 'sparkDriver' on port 41037.
16/10/30 10:08:09 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:09 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ae3bd873-212a-4673-90d1-6b947ddb6da8
16/10/30 10:08:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:09 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:09 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41037/jars/jni-spark-0.1.jar with timestamp 1477818489544
16/10/30 10:08:09 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34583.
16/10/30 10:08:09 INFO NettyBlockTransferService: Server created on 192.168.0.17:34583
16/10/30 10:08:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34583)
16/10/30 10:08:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34583 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34583)
16/10/30 10:08:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34583)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:11 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818491359
16/10/30 10:08:11 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a3d33e45-5e20-4456-a903-161f4140143d/userFiles-0612b82a-c29d-406b-8b01-30b51b6d12c3/SparkJNIPi.so
16/10/30 10:08:11 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:11 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:11 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:11 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:11 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34583 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:12 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:12 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:12 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:12 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:12 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818491359
16/10/30 10:08:12 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:12 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:12 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a3d33e45-5e20-4456-a903-161f4140143d/userFiles-0612b82a-c29d-406b-8b01-30b51b6d12c3/SparkJNIPi.so
16/10/30 10:08:12 INFO Executor: Fetching spark://192.168.0.17:41037/jars/jni-spark-0.1.jar with timestamp 1477818489544
16/10/30 10:08:12 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41037 after 24 ms (0 ms spent in bootstraps)
16/10/30 10:08:12 INFO Utils: Fetching spark://192.168.0.17:41037/jars/jni-spark-0.1.jar to /tmp/spark-a3d33e45-5e20-4456-a903-161f4140143d/userFiles-0612b82a-c29d-406b-8b01-30b51b6d12c3/fetchFileTemp5508163199055224017.tmp
16/10/30 10:08:12 INFO Executor: Adding file:/tmp/spark-a3d33e45-5e20-4456-a903-161f4140143d/userFiles-0612b82a-c29d-406b-8b01-30b51b6d12c3/jni-spark-0.1.jar to class loader
16/10/30 10:08:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:12 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 771 bytes result sent to driver
16/10/30 10:08:12 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:08:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 343 ms on localhost (1/4)
16/10/30 10:08:12 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 302 ms on localhost (2/4)
16/10/30 10:08:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 307 ms on localhost (3/4)
Calling method randToSum
16/10/30 10:08:14 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:08:14 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:34583 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:08:14 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 134874250 bytes result sent via BlockManager)
16/10/30 10:08:14 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34583 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:08:15 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:34583 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:08:15 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 3343 ms on localhost (4/4)
16/10/30 10:08:15 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 3.719 s
16/10/30 10:08:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:08:15 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 4.035501 s
16/10/30 10:08:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.0.17:34583 in memory (size: 1699.0 B, free: 366.3 MB)
Result: 3.1414018 in 4.459 seconds
16/10/30 10:08:15 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:08:15 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:08:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:08:15 INFO MemoryStore: MemoryStore cleared
16/10/30 10:08:15 INFO BlockManager: BlockManager stopped
16/10/30 10:08:15 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:08:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:08:15 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:08:15 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:08:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3d33e45-5e20-4456-a903-161f4140143d
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:17 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:17 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:17 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:17 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:17 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:17 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:17 INFO Utils: Successfully started service 'sparkDriver' on port 45128.
16/10/30 10:08:17 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:17 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d7fa1427-bae1-4b60-a171-281adc68fb16
16/10/30 10:08:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:18 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:18 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45128/jars/jni-spark-0.1.jar with timestamp 1477818498380
16/10/30 10:08:18 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36226.
16/10/30 10:08:18 INFO NettyBlockTransferService: Server created on 192.168.0.17:36226
16/10/30 10:08:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36226)
16/10/30 10:08:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36226 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36226)
16/10/30 10:08:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36226)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:20 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818500415
16/10/30 10:08:20 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-078eb9e6-11af-47ef-a3c2-9fc1f216dfc1/userFiles-b55e7f3a-f2e9-4d3f-bcb6-bd88cbf8ae03/SparkJNIPi.so
16/10/30 10:08:20 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:20 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:20 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:20 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:20 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36226 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:21 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:21 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:21 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:21 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:21 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:21 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:21 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:21 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818500415
16/10/30 10:08:21 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-078eb9e6-11af-47ef-a3c2-9fc1f216dfc1/userFiles-b55e7f3a-f2e9-4d3f-bcb6-bd88cbf8ae03/SparkJNIPi.so
16/10/30 10:08:21 INFO Executor: Fetching spark://192.168.0.17:45128/jars/jni-spark-0.1.jar with timestamp 1477818498380
16/10/30 10:08:21 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45128 after 37 ms (0 ms spent in bootstraps)
16/10/30 10:08:21 INFO Utils: Fetching spark://192.168.0.17:45128/jars/jni-spark-0.1.jar to /tmp/spark-078eb9e6-11af-47ef-a3c2-9fc1f216dfc1/userFiles-b55e7f3a-f2e9-4d3f-bcb6-bd88cbf8ae03/fetchFileTemp8476913403019387993.tmp
16/10/30 10:08:21 INFO Executor: Adding file:/tmp/spark-078eb9e6-11af-47ef-a3c2-9fc1f216dfc1/userFiles-b55e7f3a-f2e9-4d3f-bcb6-bd88cbf8ae03/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f0511af3f30, pid=30504, tid=0x00007f0525bde700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid30504.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 30504 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:22 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:22 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:22 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:22 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:22 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:22 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:23 INFO Utils: Successfully started service 'sparkDriver' on port 44642.
16/10/30 10:08:23 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:23 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a11280f0-e276-41b5-957a-b4a465dcb07a
16/10/30 10:08:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:23 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:23 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44642/jars/jni-spark-0.1.jar with timestamp 1477818503577
16/10/30 10:08:23 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37636.
16/10/30 10:08:23 INFO NettyBlockTransferService: Server created on 192.168.0.17:37636
16/10/30 10:08:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37636)
16/10/30 10:08:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37636 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37636)
16/10/30 10:08:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37636)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:25 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818505426
16/10/30 10:08:25 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-27a9baf6-8d3c-4970-9141-f2ea8d2fb886/userFiles-a30ac4cf-c788-4e74-843d-da3c3cbba172/SparkJNIPi.so
16/10/30 10:08:25 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:25 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:25 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:25 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:25 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37636 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:26 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:26 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:26 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:26 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:26 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:26 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:26 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:26 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:26 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:26 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818505426
16/10/30 10:08:26 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-27a9baf6-8d3c-4970-9141-f2ea8d2fb886/userFiles-a30ac4cf-c788-4e74-843d-da3c3cbba172/SparkJNIPi.so
16/10/30 10:08:26 INFO Executor: Fetching spark://192.168.0.17:44642/jars/jni-spark-0.1.jar with timestamp 1477818503577
16/10/30 10:08:26 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44642 after 36 ms (0 ms spent in bootstraps)
16/10/30 10:08:26 INFO Utils: Fetching spark://192.168.0.17:44642/jars/jni-spark-0.1.jar to /tmp/spark-27a9baf6-8d3c-4970-9141-f2ea8d2fb886/userFiles-a30ac4cf-c788-4e74-843d-da3c3cbba172/fetchFileTemp396355353983989102.tmp
16/10/30 10:08:26 INFO Executor: Adding file:/tmp/spark-27a9baf6-8d3c-4970-9141-f2ea8d2fb886/userFiles-a30ac4cf-c788-4e74-843d-da3c3cbba172/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:08:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:26 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:08:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 286 ms on localhost (1/4)
16/10/30 10:08:26 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 250 ms on localhost (2/4)
16/10/30 10:08:26 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 10:08:26 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 340 ms on localhost (3/4)
16/10/30 10:08:26 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 5118 bytes result sent to driver
16/10/30 10:08:26 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 400 ms on localhost (4/4)
16/10/30 10:08:26 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.471 s
16/10/30 10:08:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:08:26 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.775935 s
Result: 3.1679688 in 1.057 seconds
16/10/30 10:08:26 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:08:26 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:08:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:08:26 INFO MemoryStore: MemoryStore cleared
16/10/30 10:08:26 INFO BlockManager: BlockManager stopped
16/10/30 10:08:26 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:08:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:08:26 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:08:26 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:08:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-27a9baf6-8d3c-4970-9141-f2ea8d2fb886
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:27 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:28 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:28 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:28 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:28 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:28 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:28 INFO Utils: Successfully started service 'sparkDriver' on port 42883.
16/10/30 10:08:28 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:28 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-31664e74-e803-488c-be1f-b6799ce0c216
16/10/30 10:08:28 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:28 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:29 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42883/jars/jni-spark-0.1.jar with timestamp 1477818509010
16/10/30 10:08:29 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37818.
16/10/30 10:08:29 INFO NettyBlockTransferService: Server created on 192.168.0.17:37818
16/10/30 10:08:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37818)
16/10/30 10:08:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37818 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37818)
16/10/30 10:08:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37818)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:30 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818510828
16/10/30 10:08:30 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6e3437a3-7db7-4a01-ab08-5d2180011d79/userFiles-7bcf42e0-db72-4597-9db6-77c5704c7e43/SparkJNIPi.so
16/10/30 10:08:31 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:31 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:31 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:31 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:31 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37818 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:31 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:31 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:31 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:31 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:31 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:31 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:31 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:31 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:31 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:31 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:31 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:31 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818510828
16/10/30 10:08:31 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6e3437a3-7db7-4a01-ab08-5d2180011d79/userFiles-7bcf42e0-db72-4597-9db6-77c5704c7e43/SparkJNIPi.so
16/10/30 10:08:31 INFO Executor: Fetching spark://192.168.0.17:42883/jars/jni-spark-0.1.jar with timestamp 1477818509010
16/10/30 10:08:31 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42883 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:08:31 INFO Utils: Fetching spark://192.168.0.17:42883/jars/jni-spark-0.1.jar to /tmp/spark-6e3437a3-7db7-4a01-ab08-5d2180011d79/userFiles-7bcf42e0-db72-4597-9db6-77c5704c7e43/fetchFileTemp6581734008354919394.tmp
16/10/30 10:08:31 INFO Executor: Adding file:/tmp/spark-6e3437a3-7db7-4a01-ab08-5d2180011d79/userFiles-7bcf42e0-db72-4597-9db6-77c5704c7e43/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:08:31 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:08:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:31 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 225 ms on localhost (1/4)
16/10/30 10:08:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 274 ms on localhost (2/4)
16/10/30 10:08:31 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 10:08:31 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 326 ms on localhost (3/4)
16/10/30 10:08:31 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 5118 bytes result sent to driver
16/10/30 10:08:31 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 389 ms on localhost (4/4)
16/10/30 10:08:31 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.457 s
16/10/30 10:08:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:08:31 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.725767 s
Result: 3.1855469 in 1.009 seconds
16/10/30 10:08:31 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:08:31 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:08:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:08:31 INFO MemoryStore: MemoryStore cleared
16/10/30 10:08:31 INFO BlockManager: BlockManager stopped
16/10/30 10:08:31 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:08:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:08:31 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:08:31 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:08:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-6e3437a3-7db7-4a01-ab08-5d2180011d79
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:33 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:33 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:33 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:33 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:33 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:33 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:33 INFO Utils: Successfully started service 'sparkDriver' on port 35383.
16/10/30 10:08:33 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:34 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dd95eb21-4777-4ec2-9ab4-ae59dd286b15
16/10/30 10:08:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:34 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:34 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35383/jars/jni-spark-0.1.jar with timestamp 1477818514380
16/10/30 10:08:34 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42476.
16/10/30 10:08:34 INFO NettyBlockTransferService: Server created on 192.168.0.17:42476
16/10/30 10:08:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42476)
16/10/30 10:08:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42476 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42476)
16/10/30 10:08:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42476)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:36 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818516171
16/10/30 10:08:36 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-86a0a950-8c9e-4062-9397-648db36b7668/userFiles-7eb49847-b819-4078-988d-58ae89686aa7/SparkJNIPi.so
16/10/30 10:08:36 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:36 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:36 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:36 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:36 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42476 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:36 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:36 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:36 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:36 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:36 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:36 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:36 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818516171
16/10/30 10:08:36 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-86a0a950-8c9e-4062-9397-648db36b7668/userFiles-7eb49847-b819-4078-988d-58ae89686aa7/SparkJNIPi.so
16/10/30 10:08:36 INFO Executor: Fetching spark://192.168.0.17:35383/jars/jni-spark-0.1.jar with timestamp 1477818514380
16/10/30 10:08:36 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35383 after 30 ms (0 ms spent in bootstraps)
16/10/30 10:08:36 INFO Utils: Fetching spark://192.168.0.17:35383/jars/jni-spark-0.1.jar to /tmp/spark-86a0a950-8c9e-4062-9397-648db36b7668/userFiles-7eb49847-b819-4078-988d-58ae89686aa7/fetchFileTemp6369084368429947321.tmp
16/10/30 10:08:36 INFO Executor: Adding file:/tmp/spark-86a0a950-8c9e-4062-9397-648db36b7668/userFiles-7eb49847-b819-4078-988d-58ae89686aa7/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:08:37 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:08:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:37 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 225 ms on localhost (1/4)
16/10/30 10:08:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 272 ms on localhost (2/4)
16/10/30 10:08:37 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 5205 bytes result sent to driver
16/10/30 10:08:37 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 329 ms on localhost (3/4)
16/10/30 10:08:37 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 5205 bytes result sent to driver
16/10/30 10:08:37 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 377 ms on localhost (4/4)
16/10/30 10:08:37 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.433 s
16/10/30 10:08:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:08:37 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.732505 s
Result: 3.0546875 in 0.998 seconds
16/10/30 10:08:37 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:08:37 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:08:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:08:37 INFO MemoryStore: MemoryStore cleared
16/10/30 10:08:37 INFO BlockManager: BlockManager stopped
16/10/30 10:08:37 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:08:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:08:37 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:08:37 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:08:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-86a0a950-8c9e-4062-9397-648db36b7668
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:38 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:38 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:38 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:38 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:38 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:38 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:39 INFO Utils: Successfully started service 'sparkDriver' on port 44033.
16/10/30 10:08:39 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:39 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ffd5a77c-4391-4d7d-9305-11d570b8d64f
16/10/30 10:08:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:39 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:39 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44033/jars/jni-spark-0.1.jar with timestamp 1477818519685
16/10/30 10:08:39 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36790.
16/10/30 10:08:39 INFO NettyBlockTransferService: Server created on 192.168.0.17:36790
16/10/30 10:08:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36790)
16/10/30 10:08:39 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36790 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36790)
16/10/30 10:08:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36790)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:41 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818521660
16/10/30 10:08:41 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6b910182-243f-43fc-948d-93c345d7dbd2/userFiles-d2d25dbb-1b1e-4e36-b1ce-5160fe26a650/SparkJNIPi.so
16/10/30 10:08:41 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:41 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:41 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:41 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:41 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36790 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:42 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:42 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:42 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:42 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:42 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818521660
16/10/30 10:08:42 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:42 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:42 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6b910182-243f-43fc-948d-93c345d7dbd2/userFiles-d2d25dbb-1b1e-4e36-b1ce-5160fe26a650/SparkJNIPi.so
16/10/30 10:08:42 INFO Executor: Fetching spark://192.168.0.17:44033/jars/jni-spark-0.1.jar with timestamp 1477818519685
16/10/30 10:08:42 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44033 after 43 ms (0 ms spent in bootstraps)
16/10/30 10:08:42 INFO Utils: Fetching spark://192.168.0.17:44033/jars/jni-spark-0.1.jar to /tmp/spark-6b910182-243f-43fc-948d-93c345d7dbd2/userFiles-d2d25dbb-1b1e-4e36-b1ce-5160fe26a650/fetchFileTemp7693882383891963033.tmp
16/10/30 10:08:42 INFO Executor: Adding file:/tmp/spark-6b910182-243f-43fc-948d-93c345d7dbd2/userFiles-d2d25dbb-1b1e-4e36-b1ce-5160fe26a650/jni-spark-0.1.jar to class loader
16/10/30 10:08:42 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
Calling method randToSum
Calling method randToSum
16/10/30 10:08:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:42 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 278 ms on localhost (1/4)
16/10/30 10:08:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 314 ms on localhost (2/4)
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f51846d0013, pid=30977, tid=0x00007f51f49ac700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x3013]  pci_device_unmap_region+0x23
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid30977.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 30977 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:43 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:44 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:44 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:44 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:44 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:44 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:44 INFO Utils: Successfully started service 'sparkDriver' on port 33677.
16/10/30 10:08:44 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:44 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-11e7a562-a4e4-4e5e-9318-fa7720dcd693
16/10/30 10:08:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:44 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:45 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33677/jars/jni-spark-0.1.jar with timestamp 1477818525157
16/10/30 10:08:45 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42252.
16/10/30 10:08:45 INFO NettyBlockTransferService: Server created on 192.168.0.17:42252
16/10/30 10:08:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42252)
16/10/30 10:08:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42252 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42252)
16/10/30 10:08:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42252)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:46 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818526879
16/10/30 10:08:46 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-666a57c1-ca06-415f-bc56-f9a59d53bbaa/userFiles-24c3feba-e751-4811-a741-15d3a5769ed4/SparkJNIPi.so
16/10/30 10:08:47 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:47 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:47 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:47 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:47 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:47 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:47 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42252 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:47 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:47 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:47 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:47 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:47 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:47 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:47 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818526879
16/10/30 10:08:47 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-666a57c1-ca06-415f-bc56-f9a59d53bbaa/userFiles-24c3feba-e751-4811-a741-15d3a5769ed4/SparkJNIPi.so
16/10/30 10:08:47 INFO Executor: Fetching spark://192.168.0.17:33677/jars/jni-spark-0.1.jar with timestamp 1477818525157
16/10/30 10:08:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33677 after 32 ms (0 ms spent in bootstraps)
16/10/30 10:08:47 INFO Utils: Fetching spark://192.168.0.17:33677/jars/jni-spark-0.1.jar to /tmp/spark-666a57c1-ca06-415f-bc56-f9a59d53bbaa/userFiles-24c3feba-e751-4811-a741-15d3a5769ed4/fetchFileTemp2829042598204697628.tmp
16/10/30 10:08:47 INFO Executor: Adding file:/tmp/spark-666a57c1-ca06-415f-bc56-f9a59d53bbaa/userFiles-24c3feba-e751-4811-a741-15d3a5769ed4/jni-spark-0.1.jar to class loader
Calling method randToSum
16/10/30 10:08:47 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
Calling method randToSum
16/10/30 10:08:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:47 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 225 ms on localhost (1/4)
16/10/30 10:08:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 284 ms on localhost (2/4)
16/10/30 10:08:47 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 34017 bytes result sent to driver
16/10/30 10:08:47 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 319 ms on localhost (3/4)
16/10/30 10:08:47 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 34017 bytes result sent to driver
16/10/30 10:08:47 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.451 s
16/10/30 10:08:47 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 396 ms on localhost (4/4)
16/10/30 10:08:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:08:47 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.747407 s
Result: 3.1452637 in 1.029 seconds
16/10/30 10:08:47 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:08:47 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:08:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:08:47 INFO MemoryStore: MemoryStore cleared
16/10/30 10:08:47 INFO BlockManager: BlockManager stopped
16/10/30 10:08:47 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:08:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:08:47 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:08:47 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:08:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-666a57c1-ca06-415f-bc56-f9a59d53bbaa
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:49 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:49 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:49 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:49 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:49 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:49 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:49 INFO Utils: Successfully started service 'sparkDriver' on port 39005.
16/10/30 10:08:49 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:49 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:50 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7c5b495a-20c7-40f6-a8c5-0163b7f4c191
16/10/30 10:08:50 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:50 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:50 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39005/jars/jni-spark-0.1.jar with timestamp 1477818530352
16/10/30 10:08:50 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39523.
16/10/30 10:08:50 INFO NettyBlockTransferService: Server created on 192.168.0.17:39523
16/10/30 10:08:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39523)
16/10/30 10:08:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39523 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39523)
16/10/30 10:08:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39523)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:52 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818532529
16/10/30 10:08:52 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-c5fbacaf-f292-4fc8-abe4-3ca9eae65be2/userFiles-235016b9-5dfd-4a73-bfe5-811b31db0e0a/SparkJNIPi.so
16/10/30 10:08:52 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:52 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:52 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:52 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:52 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:53 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:53 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:53 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39523 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:53 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:53 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:53 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:53 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:53 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:53 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:53 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:53 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818532529
16/10/30 10:08:53 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:53 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:53 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:53 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-c5fbacaf-f292-4fc8-abe4-3ca9eae65be2/userFiles-235016b9-5dfd-4a73-bfe5-811b31db0e0a/SparkJNIPi.so
16/10/30 10:08:53 INFO Executor: Fetching spark://192.168.0.17:39005/jars/jni-spark-0.1.jar with timestamp 1477818530352
16/10/30 10:08:53 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39005 after 24 ms (0 ms spent in bootstraps)
16/10/30 10:08:53 INFO Utils: Fetching spark://192.168.0.17:39005/jars/jni-spark-0.1.jar to /tmp/spark-c5fbacaf-f292-4fc8-abe4-3ca9eae65be2/userFiles-235016b9-5dfd-4a73-bfe5-811b31db0e0a/fetchFileTemp9217266137826711643.tmp
16/10/30 10:08:53 INFO Executor: Adding file:/tmp/spark-c5fbacaf-f292-4fc8-abe4-3ca9eae65be2/userFiles-235016b9-5dfd-4a73-bfe5-811b31db0e0a/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:08:53 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 858 bytes result sent to driver
16/10/30 10:08:53 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f94e2bdf9dd, pid=31208, tid=0x00007f9552fee700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid31208.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 31208 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:54 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:08:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:08:54 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:08:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:08:54 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:08:54 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:08:54 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:08:54 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:08:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:08:55 INFO Utils: Successfully started service 'sparkDriver' on port 32840.
16/10/30 10:08:55 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:08:55 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:08:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b824b45a-3816-4438-9e22-b033e2a5d812
16/10/30 10:08:55 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:08:55 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:08:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:08:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:08:55 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:32840/jars/jni-spark-0.1.jar with timestamp 1477818535714
16/10/30 10:08:55 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:08:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36493.
16/10/30 10:08:55 INFO NettyBlockTransferService: Server created on 192.168.0.17:36493
16/10/30 10:08:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36493)
16/10/30 10:08:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36493 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36493)
16/10/30 10:08:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36493)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:08:57 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818537590
16/10/30 10:08:57 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-416b8743-6c8e-4736-a8cf-fffa29f05782/userFiles-22b97962-4e14-4e42-9400-13a06178c2b7/SparkJNIPi.so
16/10/30 10:08:57 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:08:57 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:08:57 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:08:57 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:08:57 INFO DAGScheduler: Missing parents: List()
16/10/30 10:08:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:08:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:08:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:08:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36493 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:08:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:08:58 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:08:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:08:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:58 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:58 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:08:58 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:08:58 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:08:58 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:08:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:08:58 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:08:58 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818537590
16/10/30 10:08:58 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-416b8743-6c8e-4736-a8cf-fffa29f05782/userFiles-22b97962-4e14-4e42-9400-13a06178c2b7/SparkJNIPi.so
16/10/30 10:08:58 INFO Executor: Fetching spark://192.168.0.17:32840/jars/jni-spark-0.1.jar with timestamp 1477818535714
16/10/30 10:08:58 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:32840 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:08:58 INFO Utils: Fetching spark://192.168.0.17:32840/jars/jni-spark-0.1.jar to /tmp/spark-416b8743-6c8e-4736-a8cf-fffa29f05782/userFiles-22b97962-4e14-4e42-9400-13a06178c2b7/fetchFileTemp87834883668686023.tmp
16/10/30 10:08:58 INFO Executor: Adding file:/tmp/spark-416b8743-6c8e-4736-a8cf-fffa29f05782/userFiles-22b97962-4e14-4e42-9400-13a06178c2b7/jni-spark-0.1.jar to class loader
16/10/30 10:08:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:08:58 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
Calling method randToSum
Calling method randToSum
16/10/30 10:08:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 265 ms on localhost (1/4)
16/10/30 10:08:58 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 238 ms on localhost (2/4)
16/10/30 10:08:58 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 34017 bytes result sent to driver
16/10/30 10:08:58 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 346 ms on localhost (3/4)
16/10/30 10:08:58 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 33930 bytes result sent to driver
16/10/30 10:08:58 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 418 ms on localhost (4/4)
16/10/30 10:08:58 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.466 s
16/10/30 10:08:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:08:58 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.754482 s
Result: 3.1315918 in 1.042 seconds
16/10/30 10:08:58 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:08:58 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:08:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:08:58 INFO MemoryStore: MemoryStore cleared
16/10/30 10:08:58 INFO BlockManager: BlockManager stopped
16/10/30 10:08:58 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:08:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:08:58 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:08:58 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:08:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-416b8743-6c8e-4736-a8cf-fffa29f05782
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:08:59 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:00 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:00 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:00 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:00 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:00 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:00 INFO Utils: Successfully started service 'sparkDriver' on port 42128.
16/10/30 10:09:00 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:00 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e5dd76c4-52c0-43d5-b0df-6363a15cc8ad
16/10/30 10:09:00 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:01 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:01 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:01 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42128/jars/jni-spark-0.1.jar with timestamp 1477818541314
16/10/30 10:09:01 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40825.
16/10/30 10:09:01 INFO NettyBlockTransferService: Server created on 192.168.0.17:40825
16/10/30 10:09:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40825)
16/10/30 10:09:01 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40825 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40825)
16/10/30 10:09:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40825)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:03 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818543080
16/10/30 10:09:03 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-ec4110e6-6abc-4830-b588-02f6422fef8e/userFiles-6a2b2a8d-cddb-4acb-884e-60f16dfc8862/SparkJNIPi.so
16/10/30 10:09:03 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:03 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:03 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:03 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:03 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:03 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40825 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:03 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:03 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:03 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:03 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:03 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818543080
16/10/30 10:09:03 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:03 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:03 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:03 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-ec4110e6-6abc-4830-b588-02f6422fef8e/userFiles-6a2b2a8d-cddb-4acb-884e-60f16dfc8862/SparkJNIPi.so
16/10/30 10:09:03 INFO Executor: Fetching spark://192.168.0.17:42128/jars/jni-spark-0.1.jar with timestamp 1477818541314
16/10/30 10:09:03 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42128 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:09:03 INFO Utils: Fetching spark://192.168.0.17:42128/jars/jni-spark-0.1.jar to /tmp/spark-ec4110e6-6abc-4830-b588-02f6422fef8e/userFiles-6a2b2a8d-cddb-4acb-884e-60f16dfc8862/fetchFileTemp3235735253697707338.tmp
16/10/30 10:09:03 INFO Executor: Adding file:/tmp/spark-ec4110e6-6abc-4830-b588-02f6422fef8e/userFiles-6a2b2a8d-cddb-4acb-884e-60f16dfc8862/jni-spark-0.1.jar to class loader
16/10/30 10:09:03 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
Calling method randToSum
Calling method randToSum
16/10/30 10:09:03 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 206 ms on localhost (1/4)
16/10/30 10:09:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 263 ms on localhost (2/4)
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fa42926e9dd, pid=31440, tid=0x00007fa4995d8700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid31440.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9: 31440 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:05 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:05 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:05 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:05 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:05 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:05 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:05 INFO Utils: Successfully started service 'sparkDriver' on port 41831.
16/10/30 10:09:05 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:05 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dc374692-4841-4c09-bd29-fade6a4fe448
16/10/30 10:09:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:06 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:06 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:06 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41831/jars/jni-spark-0.1.jar with timestamp 1477818546261
16/10/30 10:09:06 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41612.
16/10/30 10:09:06 INFO NettyBlockTransferService: Server created on 192.168.0.17:41612
16/10/30 10:09:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41612)
16/10/30 10:09:06 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41612 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41612)
16/10/30 10:09:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41612)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:08 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818548002
16/10/30 10:09:08 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-55505b3c-9eb8-49cd-907b-0c6d7de5765a/userFiles-315e1ddd-12e9-4ddf-a3e1-8bdd588f5efc/SparkJNIPi.so
16/10/30 10:09:08 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:08 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:08 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:08 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:08 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41612 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:08 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:08 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:08 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:08 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:08 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818548002
16/10/30 10:09:08 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:08 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:08 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:08 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-55505b3c-9eb8-49cd-907b-0c6d7de5765a/userFiles-315e1ddd-12e9-4ddf-a3e1-8bdd588f5efc/SparkJNIPi.so
16/10/30 10:09:08 INFO Executor: Fetching spark://192.168.0.17:41831/jars/jni-spark-0.1.jar with timestamp 1477818546261
16/10/30 10:09:08 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41831 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:09:08 INFO Utils: Fetching spark://192.168.0.17:41831/jars/jni-spark-0.1.jar to /tmp/spark-55505b3c-9eb8-49cd-907b-0c6d7de5765a/userFiles-315e1ddd-12e9-4ddf-a3e1-8bdd588f5efc/fetchFileTemp2243049994975846054.tmp
16/10/30 10:09:08 INFO Executor: Adding file:/tmp/spark-55505b3c-9eb8-49cd-907b-0c6d7de5765a/userFiles-315e1ddd-12e9-4ddf-a3e1-8bdd588f5efc/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:09:08 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 283 ms on localhost (1/4)
16/10/30 10:09:08 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 248 ms on localhost (2/4)
16/10/30 10:09:09 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 264513 bytes result sent to driver
16/10/30 10:09:09 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 361 ms on localhost (3/4)
16/10/30 10:09:09 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 264426 bytes result sent to driver
16/10/30 10:09:09 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 421 ms on localhost (4/4)
16/10/30 10:09:09 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.476 s
16/10/30 10:09:09 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:09:09 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.764313 s
Result: 3.1483765 in 1.065 seconds
16/10/30 10:09:09 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:09:09 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:09:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:09:09 INFO MemoryStore: MemoryStore cleared
16/10/30 10:09:09 INFO BlockManager: BlockManager stopped
16/10/30 10:09:09 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:09:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:09:09 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:09:09 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:09:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-55505b3c-9eb8-49cd-907b-0c6d7de5765a
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:10 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:10 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:10 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:10 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:10 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:10 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:11 INFO Utils: Successfully started service 'sparkDriver' on port 39842.
16/10/30 10:09:11 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:11 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d13adfd7-de16-4f3d-ad88-8f4f38ccf813
16/10/30 10:09:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:11 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:11 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39842/jars/jni-spark-0.1.jar with timestamp 1477818551600
16/10/30 10:09:11 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35722.
16/10/30 10:09:11 INFO NettyBlockTransferService: Server created on 192.168.0.17:35722
16/10/30 10:09:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35722)
16/10/30 10:09:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35722 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35722)
16/10/30 10:09:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35722)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:13 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818553425
16/10/30 10:09:13 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-67cf7662-8653-41a9-b59e-0abe3bea2a7f/userFiles-e18791aa-7693-4af2-b5f2-be724653f514/SparkJNIPi.so
16/10/30 10:09:13 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:13 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:13 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:13 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:13 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35722 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:14 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:14 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:14 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:14 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:14 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:14 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818553425
16/10/30 10:09:14 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:14 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-67cf7662-8653-41a9-b59e-0abe3bea2a7f/userFiles-e18791aa-7693-4af2-b5f2-be724653f514/SparkJNIPi.so
16/10/30 10:09:14 INFO Executor: Fetching spark://192.168.0.17:39842/jars/jni-spark-0.1.jar with timestamp 1477818551600
16/10/30 10:09:14 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39842 after 33 ms (0 ms spent in bootstraps)
16/10/30 10:09:14 INFO Utils: Fetching spark://192.168.0.17:39842/jars/jni-spark-0.1.jar to /tmp/spark-67cf7662-8653-41a9-b59e-0abe3bea2a7f/userFiles-e18791aa-7693-4af2-b5f2-be724653f514/fetchFileTemp3200475432438578388.tmp
16/10/30 10:09:14 INFO Executor: Adding file:/tmp/spark-67cf7662-8653-41a9-b59e-0abe3bea2a7f/userFiles-e18791aa-7693-4af2-b5f2-be724653f514/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
16/10/30 10:09:14 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 249 ms on localhost (1/4)
16/10/30 10:09:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 301 ms on localhost (2/4)
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fb7f64adf30, pid=31670, tid=0x00007fb7fa4e5700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid31670.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9: 31670 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:15 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:15 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:15 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:15 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:15 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:15 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:16 INFO Utils: Successfully started service 'sparkDriver' on port 35843.
16/10/30 10:09:16 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:16 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-792d213a-6038-4d34-93e1-54a8860416e2
16/10/30 10:09:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:16 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:16 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:35843/jars/jni-spark-0.1.jar with timestamp 1477818556627
16/10/30 10:09:16 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36950.
16/10/30 10:09:16 INFO NettyBlockTransferService: Server created on 192.168.0.17:36950
16/10/30 10:09:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36950)
16/10/30 10:09:16 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36950 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36950)
16/10/30 10:09:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36950)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:18 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818558535
16/10/30 10:09:18 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-63505d6d-8486-4696-839b-2320a34dc09b/userFiles-068c8d08-d7b4-40df-a9fe-2f988e052b03/SparkJNIPi.so
16/10/30 10:09:18 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:18 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:18 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:18 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:18 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36950 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:19 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:19 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:19 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:19 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:19 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:19 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818558535
16/10/30 10:09:19 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:19 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:19 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:19 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-63505d6d-8486-4696-839b-2320a34dc09b/userFiles-068c8d08-d7b4-40df-a9fe-2f988e052b03/SparkJNIPi.so
16/10/30 10:09:19 INFO Executor: Fetching spark://192.168.0.17:35843/jars/jni-spark-0.1.jar with timestamp 1477818556627
16/10/30 10:09:19 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35843 after 24 ms (0 ms spent in bootstraps)
16/10/30 10:09:19 INFO Utils: Fetching spark://192.168.0.17:35843/jars/jni-spark-0.1.jar to /tmp/spark-63505d6d-8486-4696-839b-2320a34dc09b/userFiles-068c8d08-d7b4-40df-a9fe-2f988e052b03/fetchFileTemp7638058804899711923.tmp
16/10/30 10:09:19 INFO Executor: Adding file:/tmp/spark-63505d6d-8486-4696-839b-2320a34dc09b/userFiles-068c8d08-d7b4-40df-a9fe-2f988e052b03/jni-spark-0.1.jar to class loader
16/10/30 10:09:19 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:19 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 214 ms on localhost (1/4)
16/10/30 10:09:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 255 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f7fdbdf99dd, pid=31778, tid=0x00007f80503a9700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid31778.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 31778 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:20 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:20 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:21 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:21 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:21 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:21 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:21 INFO Utils: Successfully started service 'sparkDriver' on port 42666.
16/10/30 10:09:21 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:21 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5212e3c7-76ed-48a7-ab00-6c08120e484c
16/10/30 10:09:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:21 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:21 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:21 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42666/jars/jni-spark-0.1.jar with timestamp 1477818561790
16/10/30 10:09:21 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39288.
16/10/30 10:09:21 INFO NettyBlockTransferService: Server created on 192.168.0.17:39288
16/10/30 10:09:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39288)
16/10/30 10:09:21 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39288 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39288)
16/10/30 10:09:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39288)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:23 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818563558
16/10/30 10:09:23 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-0c30b57c-4a44-4429-b2cb-942ab2533bcb/userFiles-ac5658b1-10eb-4a28-8e41-a0ac8b59ce3a/SparkJNIPi.so
16/10/30 10:09:23 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:23 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:23 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:23 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:23 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39288 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:24 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:24 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:24 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:24 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:24 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:24 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:24 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:24 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:24 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:24 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:24 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818563558
16/10/30 10:09:24 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-0c30b57c-4a44-4429-b2cb-942ab2533bcb/userFiles-ac5658b1-10eb-4a28-8e41-a0ac8b59ce3a/SparkJNIPi.so
16/10/30 10:09:24 INFO Executor: Fetching spark://192.168.0.17:42666/jars/jni-spark-0.1.jar with timestamp 1477818561790
16/10/30 10:09:24 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42666 after 27 ms (0 ms spent in bootstraps)
16/10/30 10:09:24 INFO Utils: Fetching spark://192.168.0.17:42666/jars/jni-spark-0.1.jar to /tmp/spark-0c30b57c-4a44-4429-b2cb-942ab2533bcb/userFiles-ac5658b1-10eb-4a28-8e41-a0ac8b59ce3a/fetchFileTemp432211020976749330.tmp
16/10/30 10:09:24 INFO Executor: Adding file:/tmp/spark-0c30b57c-4a44-4429-b2cb-942ab2533bcb/userFiles-ac5658b1-10eb-4a28-8e41-a0ac8b59ce3a/jni-spark-0.1.jar to class loader
16/10/30 10:09:24 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:24 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:24 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 283 ms on localhost (1/4)
16/10/30 10:09:24 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 242 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:09:24 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:09:24 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:39288 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:09:24 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108481 bytes result sent via BlockManager)
16/10/30 10:09:24 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39288 after 18 ms (0 ms spent in bootstraps)
16/10/30 10:09:24 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 10:09:24 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:39288 (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:09:24 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 10:09:24 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:39288 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:09:24 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:39288 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:09:24 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 539 ms on localhost (3/4)
16/10/30 10:09:24 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.634 s
16/10/30 10:09:24 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 569 ms on localhost (4/4)
16/10/30 10:09:24 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:09:24 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.928220 s
Result: 3.141224 in 1.209 seconds
16/10/30 10:09:24 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:09:24 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:09:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:09:24 INFO MemoryStore: MemoryStore cleared
16/10/30 10:09:24 INFO BlockManager: BlockManager stopped
16/10/30 10:09:24 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:09:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:09:24 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:09:24 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:09:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-0c30b57c-4a44-4429-b2cb-942ab2533bcb
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:26 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:26 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:26 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:26 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:26 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:26 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:26 INFO Utils: Successfully started service 'sparkDriver' on port 39722.
16/10/30 10:09:26 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:26 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6b551dd7-dae0-46a3-8135-4579c2549fb6
16/10/30 10:09:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:26 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:27 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39722/jars/jni-spark-0.1.jar with timestamp 1477818567205
16/10/30 10:09:27 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45594.
16/10/30 10:09:27 INFO NettyBlockTransferService: Server created on 192.168.0.17:45594
16/10/30 10:09:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45594)
16/10/30 10:09:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45594 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45594)
16/10/30 10:09:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45594)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:29 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818569098
16/10/30 10:09:29 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-49e6f6c0-285f-48b8-bd94-8fc35212650d/userFiles-a8694a8c-25d7-4b8d-b0e4-b1510006e051/SparkJNIPi.so
16/10/30 10:09:29 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:29 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:29 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:29 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:29 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45594 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:29 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:29 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:29 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:29 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:29 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:29 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:29 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818569098
16/10/30 10:09:29 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-49e6f6c0-285f-48b8-bd94-8fc35212650d/userFiles-a8694a8c-25d7-4b8d-b0e4-b1510006e051/SparkJNIPi.so
16/10/30 10:09:29 INFO Executor: Fetching spark://192.168.0.17:39722/jars/jni-spark-0.1.jar with timestamp 1477818567205
16/10/30 10:09:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39722 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:09:29 INFO Utils: Fetching spark://192.168.0.17:39722/jars/jni-spark-0.1.jar to /tmp/spark-49e6f6c0-285f-48b8-bd94-8fc35212650d/userFiles-a8694a8c-25d7-4b8d-b0e4-b1510006e051/fetchFileTemp4495413666192350180.tmp
16/10/30 10:09:29 INFO Executor: Adding file:/tmp/spark-49e6f6c0-285f-48b8-bd94-8fc35212650d/userFiles-a8694a8c-25d7-4b8d-b0e4-b1510006e051/jni-spark-0.1.jar to class loader
16/10/30 10:09:29 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:30 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 251 ms on localhost (1/4)
16/10/30 10:09:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 314 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:09:30 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:09:30 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:45594 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:09:30 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108481 bytes result sent via BlockManager)
16/10/30 10:09:30 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45594 after 8 ms (0 ms spent in bootstraps)
16/10/30 10:09:30 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 10:09:30 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:45594 (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:09:30 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 10:09:30 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:45594 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:09:30 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 562 ms on localhost (3/4)
16/10/30 10:09:30 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:45594 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:09:30 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 0.630 s
16/10/30 10:09:30 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 580 ms on localhost (4/4)
16/10/30 10:09:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:09:30 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 0.933377 s
Result: 3.141739 in 1.233 seconds
16/10/30 10:09:30 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:09:30 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:09:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:09:30 INFO MemoryStore: MemoryStore cleared
16/10/30 10:09:30 INFO BlockManager: BlockManager stopped
16/10/30 10:09:30 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:09:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:09:30 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:09:30 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:09:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-49e6f6c0-285f-48b8-bd94-8fc35212650d
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:31 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:32 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:32 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:32 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:32 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:32 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:32 INFO Utils: Successfully started service 'sparkDriver' on port 44133.
16/10/30 10:09:32 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:32 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-95ac44f9-89ce-40e5-b96f-2e508eb643b7
16/10/30 10:09:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:32 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:32 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44133/jars/jni-spark-0.1.jar with timestamp 1477818572814
16/10/30 10:09:32 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37429.
16/10/30 10:09:32 INFO NettyBlockTransferService: Server created on 192.168.0.17:37429
16/10/30 10:09:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37429)
16/10/30 10:09:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37429 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37429)
16/10/30 10:09:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37429)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:34 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818574558
16/10/30 10:09:34 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-9fb592df-9c1a-4460-a4b1-b8a3b1ebd71f/userFiles-5ffd15d6-d774-4309-a4f5-49f21e46e7e1/SparkJNIPi.so
16/10/30 10:09:34 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:34 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:34 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:34 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:34 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37429 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:35 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:35 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:35 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:35 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:35 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:35 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:35 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:35 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818574558
16/10/30 10:09:35 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-9fb592df-9c1a-4460-a4b1-b8a3b1ebd71f/userFiles-5ffd15d6-d774-4309-a4f5-49f21e46e7e1/SparkJNIPi.so
16/10/30 10:09:35 INFO Executor: Fetching spark://192.168.0.17:44133/jars/jni-spark-0.1.jar with timestamp 1477818572814
16/10/30 10:09:35 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44133 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:09:35 INFO Utils: Fetching spark://192.168.0.17:44133/jars/jni-spark-0.1.jar to /tmp/spark-9fb592df-9c1a-4460-a4b1-b8a3b1ebd71f/userFiles-5ffd15d6-d774-4309-a4f5-49f21e46e7e1/fetchFileTemp5157140833117330369.tmp
16/10/30 10:09:35 INFO Executor: Adding file:/tmp/spark-9fb592df-9c1a-4460-a4b1-b8a3b1ebd71f/userFiles-5ffd15d6-d774-4309-a4f5-49f21e46e7e1/jni-spark-0.1.jar to class loader
16/10/30 10:09:35 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:35 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 215 ms on localhost (1/4)
16/10/30 10:09:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 278 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f6b86df8f30, pid=32144, tid=0x00007f6b8f4ef700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid32144.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 32144 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:37 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:37 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:37 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:37 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:37 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:37 INFO Utils: Successfully started service 'sparkDriver' on port 37926.
16/10/30 10:09:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dbd3b8bc-7303-413b-84d6-ea78fc19f74f
16/10/30 10:09:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37926/jars/jni-spark-0.1.jar with timestamp 1477818577802
16/10/30 10:09:37 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43439.
16/10/30 10:09:37 INFO NettyBlockTransferService: Server created on 192.168.0.17:43439
16/10/30 10:09:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43439)
16/10/30 10:09:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43439 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43439)
16/10/30 10:09:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43439)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818579643
16/10/30 10:09:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-a4e82025-65fb-4e08-9081-1b3270b4bee6/userFiles-7539bb0e-50ed-4249-ac95-026970b26473/SparkJNIPi.so
16/10/30 10:09:39 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:39 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:39 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:39 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:39 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43439 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:40 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:40 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818579643
16/10/30 10:09:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:40 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:40 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-a4e82025-65fb-4e08-9081-1b3270b4bee6/userFiles-7539bb0e-50ed-4249-ac95-026970b26473/SparkJNIPi.so
16/10/30 10:09:40 INFO Executor: Fetching spark://192.168.0.17:37926/jars/jni-spark-0.1.jar with timestamp 1477818577802
16/10/30 10:09:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37926 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:09:40 INFO Utils: Fetching spark://192.168.0.17:37926/jars/jni-spark-0.1.jar to /tmp/spark-a4e82025-65fb-4e08-9081-1b3270b4bee6/userFiles-7539bb0e-50ed-4249-ac95-026970b26473/fetchFileTemp4243462211764460328.tmp
16/10/30 10:09:40 INFO Executor: Adding file:/tmp/spark-a4e82025-65fb-4e08-9081-1b3270b4bee6/userFiles-7539bb0e-50ed-4249-ac95-026970b26473/jni-spark-0.1.jar to class loader
16/10/30 10:09:40 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 858 bytes result sent to driver
16/10/30 10:09:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:40 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 219 ms on localhost (1/4)
16/10/30 10:09:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 278 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f71e985ef30, pid=32252, tid=0x00007f725a4e5700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid32252.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9: 32252 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:41 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:42 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:42 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:42 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:42 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:42 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:42 INFO Utils: Successfully started service 'sparkDriver' on port 42776.
16/10/30 10:09:42 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:42 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-84d509a5-b56d-47ca-8a72-92c073617e7c
16/10/30 10:09:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:42 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:42 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42776/jars/jni-spark-0.1.jar with timestamp 1477818582947
16/10/30 10:09:43 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42263.
16/10/30 10:09:43 INFO NettyBlockTransferService: Server created on 192.168.0.17:42263
16/10/30 10:09:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42263)
16/10/30 10:09:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42263 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42263)
16/10/30 10:09:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42263)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:44 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818584735
16/10/30 10:09:44 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-1b1ae7b8-ee27-4f0b-95a3-659dbba1551c/userFiles-33b8631a-a806-4e7d-bf3d-da5ec6118c7c/SparkJNIPi.so
16/10/30 10:09:45 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:45 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42263 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:45 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:45 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818584735
16/10/30 10:09:45 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:45 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:45 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:45 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-1b1ae7b8-ee27-4f0b-95a3-659dbba1551c/userFiles-33b8631a-a806-4e7d-bf3d-da5ec6118c7c/SparkJNIPi.so
16/10/30 10:09:45 INFO Executor: Fetching spark://192.168.0.17:42776/jars/jni-spark-0.1.jar with timestamp 1477818582947
16/10/30 10:09:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42776 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:09:45 INFO Utils: Fetching spark://192.168.0.17:42776/jars/jni-spark-0.1.jar to /tmp/spark-1b1ae7b8-ee27-4f0b-95a3-659dbba1551c/userFiles-33b8631a-a806-4e7d-bf3d-da5ec6118c7c/fetchFileTemp693270902690223995.tmp
16/10/30 10:09:45 INFO Executor: Adding file:/tmp/spark-1b1ae7b8-ee27-4f0b-95a3-659dbba1551c/userFiles-33b8631a-a806-4e7d-bf3d-da5ec6118c7c/jni-spark-0.1.jar to class loader
16/10/30 10:09:45 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:45 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 239 ms on localhost (1/4)
16/10/30 10:09:45 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:45 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 293 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:09:47 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:09:47 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:42263 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:09:47 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:09:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42263 after 6 ms (0 ms spent in bootstraps)
16/10/30 10:09:47 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:09:47 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:42263 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:09:47 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:09:47 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:42263 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:09:47 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:42263 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:09:47 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1918 ms on localhost (3/4)
16/10/30 10:09:47 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1988 ms on localhost (4/4)
16/10/30 10:09:47 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 2.060 s
16/10/30 10:09:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:09:47 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.361345 s
Result: 3.1417346 in 2.671 seconds
16/10/30 10:09:47 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:09:47 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:09:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:09:47 INFO MemoryStore: MemoryStore cleared
16/10/30 10:09:47 INFO BlockManager: BlockManager stopped
16/10/30 10:09:47 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:09:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:09:47 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:09:47 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:09:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-1b1ae7b8-ee27-4f0b-95a3-659dbba1551c
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:48 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:49 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:49 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:49 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:49 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:49 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:49 INFO Utils: Successfully started service 'sparkDriver' on port 36046.
16/10/30 10:09:49 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:49 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bc38e880-86ae-40e2-8c88-2a927f9f3355
16/10/30 10:09:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:49 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:49 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36046/jars/jni-spark-0.1.jar with timestamp 1477818589922
16/10/30 10:09:49 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35991.
16/10/30 10:09:50 INFO NettyBlockTransferService: Server created on 192.168.0.17:35991
16/10/30 10:09:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35991)
16/10/30 10:09:50 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35991 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35991)
16/10/30 10:09:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35991)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:51 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818591724
16/10/30 10:09:51 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-fa6030f6-8e06-431e-a75b-4bd231ad6d48/userFiles-4dd7d86a-b2b2-4bee-9997-830a0e1c9925/SparkJNIPi.so
16/10/30 10:09:52 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:52 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:52 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:52 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:52 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:52 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:52 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:52 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:52 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35991 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:52 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:52 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:52 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:52 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:52 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:09:52 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:52 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818591724
16/10/30 10:09:52 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-fa6030f6-8e06-431e-a75b-4bd231ad6d48/userFiles-4dd7d86a-b2b2-4bee-9997-830a0e1c9925/SparkJNIPi.so
16/10/30 10:09:52 INFO Executor: Fetching spark://192.168.0.17:36046/jars/jni-spark-0.1.jar with timestamp 1477818589922
16/10/30 10:09:52 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36046 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:09:52 INFO Utils: Fetching spark://192.168.0.17:36046/jars/jni-spark-0.1.jar to /tmp/spark-fa6030f6-8e06-431e-a75b-4bd231ad6d48/userFiles-4dd7d86a-b2b2-4bee-9997-830a0e1c9925/fetchFileTemp3577148603588941402.tmp
16/10/30 10:09:52 INFO Executor: Adding file:/tmp/spark-fa6030f6-8e06-431e-a75b-4bd231ad6d48/userFiles-4dd7d86a-b2b2-4bee-9997-830a0e1c9925/jni-spark-0.1.jar to class loader
16/10/30 10:09:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:09:52 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:09:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 316 ms on localhost (1/4)
16/10/30 10:09:52 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 271 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:09:54 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:09:54 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:35991 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:09:54 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:09:54 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35991 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:09:54 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:09:54 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:35991 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:09:54 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:09:54 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:35991 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:09:54 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2536 ms on localhost (3/4)
16/10/30 10:09:54 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:35991 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:09:55 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2607 ms on localhost (4/4)
16/10/30 10:09:55 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 2.702 s
16/10/30 10:09:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:09:55 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.998800 s
Result: 3.1417623 in 3.291 seconds
16/10/30 10:09:55 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:09:55 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:09:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:09:55 INFO MemoryStore: MemoryStore cleared
16/10/30 10:09:55 INFO BlockManager: BlockManager stopped
16/10/30 10:09:55 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:09:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:09:55 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:09:55 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:09:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-fa6030f6-8e06-431e-a75b-4bd231ad6d48
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:09:56 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:09:56 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:09:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:09:56 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:09:56 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:09:56 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:09:56 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:09:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:09:57 INFO Utils: Successfully started service 'sparkDriver' on port 41537.
16/10/30 10:09:57 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:09:57 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:09:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-93fd9e7b-d6cf-4390-a65c-4de565bbab9a
16/10/30 10:09:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:09:57 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:09:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:09:57 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:09:57 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41537/jars/jni-spark-0.1.jar with timestamp 1477818597558
16/10/30 10:09:57 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:09:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35689.
16/10/30 10:09:57 INFO NettyBlockTransferService: Server created on 192.168.0.17:35689
16/10/30 10:09:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35689)
16/10/30 10:09:57 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35689 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35689)
16/10/30 10:09:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35689)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:09:59 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818599325
16/10/30 10:09:59 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-b87df056-7735-467d-82ee-676033ce70dc/userFiles-398328f3-a166-4ef8-87ef-6302c7bc29e2/SparkJNIPi.so
16/10/30 10:09:59 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:09:59 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:09:59 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:09:59 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:09:59 INFO DAGScheduler: Missing parents: List()
16/10/30 10:09:59 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:09:59 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:09:59 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:09:59 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35689 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:09:59 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:09:59 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:09:59 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:09:59 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:59 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:59 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:09:59 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:09:59 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:09:59 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:09:59 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:09:59 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818599325
16/10/30 10:09:59 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:10:00 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-b87df056-7735-467d-82ee-676033ce70dc/userFiles-398328f3-a166-4ef8-87ef-6302c7bc29e2/SparkJNIPi.so
16/10/30 10:10:00 INFO Executor: Fetching spark://192.168.0.17:41537/jars/jni-spark-0.1.jar with timestamp 1477818597558
16/10/30 10:10:00 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41537 after 22 ms (0 ms spent in bootstraps)
16/10/30 10:10:00 INFO Utils: Fetching spark://192.168.0.17:41537/jars/jni-spark-0.1.jar to /tmp/spark-b87df056-7735-467d-82ee-676033ce70dc/userFiles-398328f3-a166-4ef8-87ef-6302c7bc29e2/fetchFileTemp6357307931125192628.tmp
16/10/30 10:10:00 INFO Executor: Adding file:/tmp/spark-b87df056-7735-467d-82ee-676033ce70dc/userFiles-398328f3-a166-4ef8-87ef-6302c7bc29e2/jni-spark-0.1.jar to class loader
16/10/30 10:10:00 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 844 bytes result sent to driver
16/10/30 10:10:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 844 bytes result sent to driver
16/10/30 10:10:00 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 342 ms on localhost (1/4)
16/10/30 10:10:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 399 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:10:01 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:10:01 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:35689 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:10:01 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:10:01 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:35689 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:10:01 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:10:01 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:35689 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:10:01 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:10:01 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:35689 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:10:01 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:35689 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:10:01 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1670 ms on localhost (3/4)
16/10/30 10:10:01 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1741 ms on localhost (4/4)
16/10/30 10:10:01 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.822 s
16/10/30 10:10:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:10:01 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.133777 s
Result: 3.141625 in 2.421 seconds
16/10/30 10:10:01 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:10:01 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:10:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:10:01 INFO MemoryStore: MemoryStore cleared
16/10/30 10:10:01 INFO BlockManager: BlockManager stopped
16/10/30 10:10:01 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:10:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:10:01 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:10:01 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:10:01 INFO ShutdownHookManager: Deleting directory /tmp/spark-b87df056-7735-467d-82ee-676033ce70dc
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:10:03 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:10:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:10:03 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:10:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:10:03 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:10:03 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:10:03 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:10:03 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:10:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:10:03 INFO Utils: Successfully started service 'sparkDriver' on port 41197.
16/10/30 10:10:03 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:10:03 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:10:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0be67fa2-2559-402b-9549-c430f2e613c8
16/10/30 10:10:03 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:10:04 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:10:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:10:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:10:04 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41197/jars/jni-spark-0.1.jar with timestamp 1477818604299
16/10/30 10:10:04 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:10:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46782.
16/10/30 10:10:04 INFO NettyBlockTransferService: Server created on 192.168.0.17:46782
16/10/30 10:10:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46782)
16/10/30 10:10:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46782 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46782)
16/10/30 10:10:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46782)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:10:06 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818606181
16/10/30 10:10:06 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-bf35f411-559a-4837-aff6-6a494da9af9b/userFiles-87c82f17-7004-42b9-b352-122539ad49f2/SparkJNIPi.so
16/10/30 10:10:06 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:10:06 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:10:06 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:10:06 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:10:06 INFO DAGScheduler: Missing parents: List()
16/10/30 10:10:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:10:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:10:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:10:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46782 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:10:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:10:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:10:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:10:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:10:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:10:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:10:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:10:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:10:06 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818606181
16/10/30 10:10:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:10:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:10:06 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:10:06 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-bf35f411-559a-4837-aff6-6a494da9af9b/userFiles-87c82f17-7004-42b9-b352-122539ad49f2/SparkJNIPi.so
16/10/30 10:10:06 INFO Executor: Fetching spark://192.168.0.17:41197/jars/jni-spark-0.1.jar with timestamp 1477818604299
16/10/30 10:10:06 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41197 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:10:06 INFO Utils: Fetching spark://192.168.0.17:41197/jars/jni-spark-0.1.jar to /tmp/spark-bf35f411-559a-4837-aff6-6a494da9af9b/userFiles-87c82f17-7004-42b9-b352-122539ad49f2/fetchFileTemp1829004674019394107.tmp
16/10/30 10:10:06 INFO Executor: Adding file:/tmp/spark-bf35f411-559a-4837-aff6-6a494da9af9b/userFiles-87c82f17-7004-42b9-b352-122539ad49f2/jni-spark-0.1.jar to class loader
16/10/30 10:10:07 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 844 bytes result sent to driver
16/10/30 10:10:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 844 bytes result sent to driver
16/10/30 10:10:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 406 ms on localhost (1/4)
16/10/30 10:10:07 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 398 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:10:08 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:10:08 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:46782 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:10:08 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:10:08 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46782 after 4 ms (0 ms spent in bootstraps)
16/10/30 10:10:08 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:10:08 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:46782 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:10:08 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:10:08 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:46782 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:10:08 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2097 ms on localhost (3/4)
16/10/30 10:10:08 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:46782 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:10:08 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2144 ms on localhost (4/4)
16/10/30 10:10:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:10:08 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 2.236 s
16/10/30 10:10:09 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 2.546987 s
Result: 3.1420355 in 2.833 seconds
16/10/30 10:10:09 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:10:09 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:10:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:10:09 INFO MemoryStore: MemoryStore cleared
16/10/30 10:10:09 INFO BlockManager: BlockManager stopped
16/10/30 10:10:09 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:10:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:10:09 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:10:09 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:10:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf35f411-559a-4837-aff6-6a494da9af9b
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:10:10 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:10:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:10:10 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:10:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:10:10 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:10:10 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:10:10 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:10:10 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:10:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:10:11 INFO Utils: Successfully started service 'sparkDriver' on port 33760.
16/10/30 10:10:11 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:10:11 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:10:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c8d43cc8-ea90-4493-9536-f52e6a66c3e9
16/10/30 10:10:11 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:10:11 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:10:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:10:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:10:11 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33760/jars/jni-spark-0.1.jar with timestamp 1477818611510
16/10/30 10:10:11 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:10:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34678.
16/10/30 10:10:11 INFO NettyBlockTransferService: Server created on 192.168.0.17:34678
16/10/30 10:10:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34678)
16/10/30 10:10:11 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34678 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34678)
16/10/30 10:10:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34678)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:10:13 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818613586
16/10/30 10:10:13 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-384f64bf-ea34-4a29-aba1-14e6746652cf/userFiles-8822eceb-a8c7-449a-a51e-f32d7439e55d/SparkJNIPi.so
16/10/30 10:10:13 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:10:13 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:10:13 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:10:13 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:10:13 INFO DAGScheduler: Missing parents: List()
16/10/30 10:10:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:10:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:10:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:10:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34678 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:10:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:10:14 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:10:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:10:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:10:14 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:10:14 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:10:14 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:10:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:10:14 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:10:14 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818613586
16/10/30 10:10:14 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:10:14 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:10:14 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-384f64bf-ea34-4a29-aba1-14e6746652cf/userFiles-8822eceb-a8c7-449a-a51e-f32d7439e55d/SparkJNIPi.so
16/10/30 10:10:14 INFO Executor: Fetching spark://192.168.0.17:33760/jars/jni-spark-0.1.jar with timestamp 1477818611510
16/10/30 10:10:14 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33760 after 33 ms (0 ms spent in bootstraps)
16/10/30 10:10:14 INFO Utils: Fetching spark://192.168.0.17:33760/jars/jni-spark-0.1.jar to /tmp/spark-384f64bf-ea34-4a29-aba1-14e6746652cf/userFiles-8822eceb-a8c7-449a-a51e-f32d7439e55d/fetchFileTemp2557818579362797495.tmp
16/10/30 10:10:14 INFO Executor: Adding file:/tmp/spark-384f64bf-ea34-4a29-aba1-14e6746652cf/userFiles-8822eceb-a8c7-449a-a51e-f32d7439e55d/jni-spark-0.1.jar to class loader
16/10/30 10:10:14 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 858 bytes result sent to driver
16/10/30 10:10:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:10:14 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 364 ms on localhost (1/4)
16/10/30 10:10:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 434 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f2da91439f2, pid=412, tid=0x00007f2d81fe0700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x6cf9f2]  jni_SetIntArrayRegion+0xc2
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid412.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
./runTimesModes.sh: line 9:   412 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:10:33 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:10:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:10:33 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:10:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:10:33 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:10:33 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:10:33 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:10:33 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:10:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:10:34 INFO Utils: Successfully started service 'sparkDriver' on port 42008.
16/10/30 10:10:34 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:10:34 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:10:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0cc8e446-ddf4-4daf-ae2f-6a547f819bc4
16/10/30 10:10:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:10:34 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:10:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:10:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:10:34 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42008/jars/jni-spark-0.1.jar with timestamp 1477818634438
16/10/30 10:10:34 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:10:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38132.
16/10/30 10:10:34 INFO NettyBlockTransferService: Server created on 192.168.0.17:38132
16/10/30 10:10:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38132)
16/10/30 10:10:34 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38132 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38132)
16/10/30 10:10:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38132)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:10:36 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818636200
16/10/30 10:10:36 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-065a4264-31ee-4ae8-a532-6ddc6d021f18/userFiles-d374a3bd-01d4-4d3d-9351-fce867292fe4/SparkJNIPi.so
16/10/30 10:10:36 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:10:36 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:10:36 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:10:36 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:10:36 INFO DAGScheduler: Missing parents: List()
16/10/30 10:10:36 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:10:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:10:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:10:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38132 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:10:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:10:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:10:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:10:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:10:36 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:10:36 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:10:36 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:10:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:10:36 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:10:36 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:10:36 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818636200
16/10/30 10:10:36 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:10:36 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-065a4264-31ee-4ae8-a532-6ddc6d021f18/userFiles-d374a3bd-01d4-4d3d-9351-fce867292fe4/SparkJNIPi.so
16/10/30 10:10:36 INFO Executor: Fetching spark://192.168.0.17:42008/jars/jni-spark-0.1.jar with timestamp 1477818634438
16/10/30 10:10:36 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42008 after 30 ms (0 ms spent in bootstraps)
16/10/30 10:10:36 INFO Utils: Fetching spark://192.168.0.17:42008/jars/jni-spark-0.1.jar to /tmp/spark-065a4264-31ee-4ae8-a532-6ddc6d021f18/userFiles-d374a3bd-01d4-4d3d-9351-fce867292fe4/fetchFileTemp3224536847853695370.tmp
16/10/30 10:10:36 INFO Executor: Adding file:/tmp/spark-065a4264-31ee-4ae8-a532-6ddc6d021f18/userFiles-d374a3bd-01d4-4d3d-9351-fce867292fe4/jni-spark-0.1.jar to class loader
16/10/30 10:10:37 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 858 bytes result sent to driver
16/10/30 10:10:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:10:37 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 330 ms on localhost (1/4)
16/10/30 10:10:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 386 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:10:53 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:10:53 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:38132 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:10:53 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 134874250 bytes result sent via BlockManager)
16/10/30 10:10:53 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38132 after 3 ms (0 ms spent in bootstraps)
16/10/30 10:10:54 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 109.0 MB)
16/10/30 10:10:54 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:38132 (size: 128.6 MB, free: 109.0 MB)
16/10/30 10:10:54 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 10:10:54 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:38132 in memory (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:10:55 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 17833 ms on localhost (3/4)
16/10/30 10:10:55 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:38132 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:10:55 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 18463 ms on localhost (4/4)
16/10/30 10:10:55 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 18.725 s
16/10/30 10:10:55 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:10:55 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 19.047069 s
Result: 3.1413856 in 19.378 seconds
16/10/30 10:10:55 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:10:55 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:10:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:10:55 INFO MemoryStore: MemoryStore cleared
16/10/30 10:10:55 INFO BlockManager: BlockManager stopped
16/10/30 10:10:55 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:10:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:10:55 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:10:55 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:10:55 INFO ShutdownHookManager: Deleting directory /tmp/spark-065a4264-31ee-4ae8-a532-6ddc6d021f18
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:10:56 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:10:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:10:57 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:10:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:10:57 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:10:57 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:10:57 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:10:57 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:10:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:10:57 INFO Utils: Successfully started service 'sparkDriver' on port 41019.
16/10/30 10:10:57 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:10:57 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:10:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2cce0540-a361-4a7f-9bec-444bcb26ead4
16/10/30 10:10:57 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:10:57 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:10:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:10:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:10:58 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41019/jars/jni-spark-0.1.jar with timestamp 1477818658135
16/10/30 10:10:58 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:10:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33425.
16/10/30 10:10:58 INFO NettyBlockTransferService: Server created on 192.168.0.17:33425
16/10/30 10:10:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33425)
16/10/30 10:10:58 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33425 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33425)
16/10/30 10:10:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33425)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:10:59 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818659914
16/10/30 10:10:59 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-70998adc-636e-4b6e-aa47-1853e64bf2a5/userFiles-70eedf8a-d8cf-4038-bf7d-8d0763584a78/SparkJNIPi.so
16/10/30 10:11:00 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:11:00 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:11:00 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:11:00 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:11:00 INFO DAGScheduler: Missing parents: List()
16/10/30 10:11:00 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:11:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:11:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:11:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33425 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:11:00 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:11:00 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:11:00 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:11:00 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:11:00 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:00 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:11:00 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:00 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:11:00 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:11:00 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:11:00 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:11:00 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818659914
16/10/30 10:11:00 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-70998adc-636e-4b6e-aa47-1853e64bf2a5/userFiles-70eedf8a-d8cf-4038-bf7d-8d0763584a78/SparkJNIPi.so
16/10/30 10:11:00 INFO Executor: Fetching spark://192.168.0.17:41019/jars/jni-spark-0.1.jar with timestamp 1477818658135
16/10/30 10:11:00 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41019 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:11:00 INFO Utils: Fetching spark://192.168.0.17:41019/jars/jni-spark-0.1.jar to /tmp/spark-70998adc-636e-4b6e-aa47-1853e64bf2a5/userFiles-70eedf8a-d8cf-4038-bf7d-8d0763584a78/fetchFileTemp349052727066595124.tmp
16/10/30 10:11:00 INFO Executor: Adding file:/tmp/spark-70998adc-636e-4b6e-aa47-1853e64bf2a5/userFiles-70eedf8a-d8cf-4038-bf7d-8d0763584a78/jni-spark-0.1.jar to class loader
16/10/30 10:11:00 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:11:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:11:00 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 280 ms on localhost (1/4)
16/10/30 10:11:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 334 ms on localhost (2/4)
Calling method randToSum
16/10/30 10:11:10 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:11:10 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:33425 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:11:10 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 134874250 bytes result sent via BlockManager)
16/10/30 10:11:10 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33425 after 1 ms (0 ms spent in bootstraps)
Calling method randToSum
16/10/30 10:11:11 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:33425 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:11:11 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 10732 ms on localhost (3/4)
16/10/30 10:11:12 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:11:12 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:33425 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:11:12 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 10:11:12 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:33425 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:11:12 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 12241 ms on localhost (4/4)
16/10/30 10:11:12 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 12.470 s
16/10/30 10:11:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:11:13 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 12.795307 s
Result: 3.1416752 in 13.103 seconds
16/10/30 10:11:13 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:11:13 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:11:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:11:13 INFO MemoryStore: MemoryStore cleared
16/10/30 10:11:13 INFO BlockManager: BlockManager stopped
16/10/30 10:11:13 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:11:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:11:13 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:11:13 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:11:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-70998adc-636e-4b6e-aa47-1853e64bf2a5
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:11:14 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:11:14 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:11:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:11:14 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:11:14 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:11:14 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:11:14 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:11:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:11:15 INFO Utils: Successfully started service 'sparkDriver' on port 40735.
16/10/30 10:11:15 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:11:15 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:11:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-34d05597-26e2-4370-8093-4a2784b14106
16/10/30 10:11:15 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:11:15 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:11:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:11:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:11:15 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:40735/jars/jni-spark-0.1.jar with timestamp 1477818675537
16/10/30 10:11:15 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:11:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40673.
16/10/30 10:11:15 INFO NettyBlockTransferService: Server created on 192.168.0.17:40673
16/10/30 10:11:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40673)
16/10/30 10:11:15 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40673 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40673)
16/10/30 10:11:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40673)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:11:17 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818677220
16/10/30 10:11:17 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d5b10ddf-901a-4f2f-a30b-aa61951a1a76/userFiles-72711b74-11ca-410a-8084-a826ac4b3f96/SparkJNIPi.so
16/10/30 10:11:17 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:11:17 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:11:17 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:11:17 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:11:17 INFO DAGScheduler: Missing parents: List()
16/10/30 10:11:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:11:17 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:11:17 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:11:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40673 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:11:17 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:11:17 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:11:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:11:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:11:17 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:17 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5526 bytes)
16/10/30 10:11:17 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:11:17 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:11:17 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:11:17 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:11:17 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818677220
16/10/30 10:11:17 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d5b10ddf-901a-4f2f-a30b-aa61951a1a76/userFiles-72711b74-11ca-410a-8084-a826ac4b3f96/SparkJNIPi.so
16/10/30 10:11:17 INFO Executor: Fetching spark://192.168.0.17:40735/jars/jni-spark-0.1.jar with timestamp 1477818675537
16/10/30 10:11:17 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40735 after 27 ms (0 ms spent in bootstraps)
16/10/30 10:11:17 INFO Utils: Fetching spark://192.168.0.17:40735/jars/jni-spark-0.1.jar to /tmp/spark-d5b10ddf-901a-4f2f-a30b-aa61951a1a76/userFiles-72711b74-11ca-410a-8084-a826ac4b3f96/fetchFileTemp35404882552082601.tmp
16/10/30 10:11:18 INFO Executor: Adding file:/tmp/spark-d5b10ddf-901a-4f2f-a30b-aa61951a1a76/userFiles-72711b74-11ca-410a-8084-a826ac4b3f96/jni-spark-0.1.jar to class loader
16/10/30 10:11:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 771 bytes result sent to driver
16/10/30 10:11:18 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 771 bytes result sent to driver
16/10/30 10:11:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 407 ms on localhost (1/4)
16/10/30 10:11:18 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 388 ms on localhost (2/4)
Calling method randToSum
Calling method randToSum
16/10/30 10:11:30 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 128.6 MB, free 237.7 MB)
16/10/30 10:11:30 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:40673 (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:11:30 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 134874250 bytes result sent via BlockManager)
16/10/30 10:11:30 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40673 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:11:31 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 128.6 MB, free 109.0 MB)
16/10/30 10:11:31 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:40673 (size: 128.6 MB, free: 109.0 MB)
16/10/30 10:11:31 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 134874250 bytes result sent via BlockManager)
16/10/30 10:11:31 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:40673 in memory (size: 128.6 MB, free: 237.7 MB)
16/10/30 10:11:32 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 13726 ms on localhost (3/4)
16/10/30 10:11:32 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:40673 in memory (size: 128.6 MB, free: 366.3 MB)
16/10/30 10:11:32 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 14276 ms on localhost (4/4)
16/10/30 10:11:32 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 14.535 s
16/10/30 10:11:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:11:32 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 14.861948 s
Result: 3.141792 in 15.154 seconds
16/10/30 10:11:32 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:11:32 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:11:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:11:32 INFO MemoryStore: MemoryStore cleared
16/10/30 10:11:32 INFO BlockManager: BlockManager stopped
16/10/30 10:11:32 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:11:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:11:32 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:11:32 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:11:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5b10ddf-901a-4f2f-a30b-aa61951a1a76
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:11:33 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:11:34 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:11:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:11:34 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:11:34 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:11:34 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:11:34 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:11:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:11:34 INFO Utils: Successfully started service 'sparkDriver' on port 41333.
16/10/30 10:11:34 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:11:34 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:11:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8c7f3b5b-5c42-456e-ae5f-5b2d954cc9cc
16/10/30 10:11:34 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:11:34 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:11:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:11:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:11:34 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41333/jars/jni-spark-0.1.jar with timestamp 1477818694968
16/10/30 10:11:35 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:11:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36770.
16/10/30 10:11:35 INFO NettyBlockTransferService: Server created on 192.168.0.17:36770
16/10/30 10:11:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36770)
16/10/30 10:11:35 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36770 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36770)
16/10/30 10:11:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36770)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:11:36 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818696845
16/10/30 10:11:36 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-15612b30-13c8-4346-af3f-b105917e2ca1/userFiles-cd083f8e-3554-4abb-9271-1b27a2bda329/SparkJNIPi.so
16/10/30 10:11:37 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:11:37 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:11:37 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:11:37 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:11:37 INFO DAGScheduler: Missing parents: List()
16/10/30 10:11:37 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:11:37 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:11:37 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:11:37 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36770 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:11:37 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:11:37 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:11:37 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:11:37 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:37 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:37 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:37 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:37 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:11:37 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:11:37 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:11:37 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:11:37 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818696845
16/10/30 10:11:37 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-15612b30-13c8-4346-af3f-b105917e2ca1/userFiles-cd083f8e-3554-4abb-9271-1b27a2bda329/SparkJNIPi.so
16/10/30 10:11:37 INFO Executor: Fetching spark://192.168.0.17:41333/jars/jni-spark-0.1.jar with timestamp 1477818694968
16/10/30 10:11:37 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41333 after 26 ms (0 ms spent in bootstraps)
16/10/30 10:11:37 INFO Utils: Fetching spark://192.168.0.17:41333/jars/jni-spark-0.1.jar to /tmp/spark-15612b30-13c8-4346-af3f-b105917e2ca1/userFiles-cd083f8e-3554-4abb-9271-1b27a2bda329/fetchFileTemp6493019664125564784.tmp
16/10/30 10:11:37 INFO Executor: Adding file:/tmp/spark-15612b30-13c8-4346-af3f-b105917e2ca1/userFiles-cd083f8e-3554-4abb-9271-1b27a2bda329/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f29b2be2f30, pid=974, tid=0x00007f29badec700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid974.log
[thread 139817207654144 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:   974 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:11:38 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:11:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:11:39 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:11:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:11:39 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:11:39 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:11:39 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:11:39 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:11:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:11:39 INFO Utils: Successfully started service 'sparkDriver' on port 36695.
16/10/30 10:11:39 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:11:39 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:11:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-896e60b3-ac3e-40f6-8ade-07f243de5c3b
16/10/30 10:11:39 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:11:39 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:11:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:11:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:11:40 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36695/jars/jni-spark-0.1.jar with timestamp 1477818700083
16/10/30 10:11:40 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:11:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40681.
16/10/30 10:11:40 INFO NettyBlockTransferService: Server created on 192.168.0.17:40681
16/10/30 10:11:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40681)
16/10/30 10:11:40 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40681 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40681)
16/10/30 10:11:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40681)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:11:41 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818701756
16/10/30 10:11:41 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-bd4bf6bf-3488-49bd-a689-99ad03cc00fd/userFiles-63c18f03-50e4-4ede-a3f8-bc5d376685af/SparkJNIPi.so
16/10/30 10:11:42 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:11:42 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:11:42 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:11:42 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:11:42 INFO DAGScheduler: Missing parents: List()
16/10/30 10:11:42 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:11:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:11:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:11:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40681 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:11:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:11:42 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:11:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:11:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:42 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:42 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:42 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:11:42 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:11:42 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:11:42 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:11:42 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818701756
16/10/30 10:11:42 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-bd4bf6bf-3488-49bd-a689-99ad03cc00fd/userFiles-63c18f03-50e4-4ede-a3f8-bc5d376685af/SparkJNIPi.so
16/10/30 10:11:42 INFO Executor: Fetching spark://192.168.0.17:36695/jars/jni-spark-0.1.jar with timestamp 1477818700083
16/10/30 10:11:42 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36695 after 37 ms (0 ms spent in bootstraps)
16/10/30 10:11:42 INFO Utils: Fetching spark://192.168.0.17:36695/jars/jni-spark-0.1.jar to /tmp/spark-bd4bf6bf-3488-49bd-a689-99ad03cc00fd/userFiles-63c18f03-50e4-4ede-a3f8-bc5d376685af/fetchFileTemp6762741228546264323.tmp
16/10/30 10:11:42 INFO Executor: Adding file:/tmp/spark-bd4bf6bf-3488-49bd-a689-99ad03cc00fd/userFiles-63c18f03-50e4-4ede-a3f8-bc5d376685af/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fd0c9df7f30, pid=1094, tid=0x00007fd139fe0700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid1094.log
[thread 140536598894336 also had an error]
[thread 140536596788992 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:  1094 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:11:43 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:11:44 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:11:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:11:44 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:11:44 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:11:44 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:11:44 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:11:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:11:44 INFO Utils: Successfully started service 'sparkDriver' on port 37377.
16/10/30 10:11:44 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:11:44 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:11:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dfdeb401-1f91-48b4-8d2b-f2146f48d2e2
16/10/30 10:11:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:11:44 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:11:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:11:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:11:44 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37377/jars/jni-spark-0.1.jar with timestamp 1477818704861
16/10/30 10:11:44 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:11:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36914.
16/10/30 10:11:45 INFO NettyBlockTransferService: Server created on 192.168.0.17:36914
16/10/30 10:11:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36914)
16/10/30 10:11:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36914 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36914)
16/10/30 10:11:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36914)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:11:46 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818706445
16/10/30 10:11:46 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-333e05eb-89d7-4a3c-91e5-f8e9b9b86853/userFiles-5b271092-f700-40fa-b3a7-2c4803fa7df9/SparkJNIPi.so
16/10/30 10:11:46 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:11:46 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:11:46 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:11:46 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:11:46 INFO DAGScheduler: Missing parents: List()
16/10/30 10:11:46 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:11:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:11:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:11:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36914 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:11:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:11:47 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:11:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:11:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:47 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:47 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:47 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:47 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:11:47 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:11:47 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:11:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:11:47 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818706445
16/10/30 10:11:47 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-333e05eb-89d7-4a3c-91e5-f8e9b9b86853/userFiles-5b271092-f700-40fa-b3a7-2c4803fa7df9/SparkJNIPi.so
16/10/30 10:11:47 INFO Executor: Fetching spark://192.168.0.17:37377/jars/jni-spark-0.1.jar with timestamp 1477818704861
16/10/30 10:11:47 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37377 after 31 ms (0 ms spent in bootstraps)
16/10/30 10:11:47 INFO Utils: Fetching spark://192.168.0.17:37377/jars/jni-spark-0.1.jar to /tmp/spark-333e05eb-89d7-4a3c-91e5-f8e9b9b86853/userFiles-5b271092-f700-40fa-b3a7-2c4803fa7df9/fetchFileTemp2730691100831302400.tmp
16/10/30 10:11:47 INFO Executor: Adding file:/tmp/spark-333e05eb-89d7-4a3c-91e5-f8e9b9b86853/userFiles-5b271092-f700-40fa-b3a7-2c4803fa7df9/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV[thread 140382843021056 also had an error] (0xb)
 at pc=0x00007fad516d3f30, pid=1216, tid=0x00007fad6d8db700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid1216.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:  1216 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:11:48 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:11:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:11:48 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:11:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:11:48 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:11:48 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:11:48 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:11:48 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:11:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:11:49 INFO Utils: Successfully started service 'sparkDriver' on port 34680.
16/10/30 10:11:49 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:11:49 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:11:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e5922cda-aa40-4939-b1f5-aa4563b60f26
16/10/30 10:11:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:11:49 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:11:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:11:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:11:49 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34680/jars/jni-spark-0.1.jar with timestamp 1477818709567
16/10/30 10:11:49 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:11:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42920.
16/10/30 10:11:49 INFO NettyBlockTransferService: Server created on 192.168.0.17:42920
16/10/30 10:11:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42920)
16/10/30 10:11:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42920 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42920)
16/10/30 10:11:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42920)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:11:51 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818711434
16/10/30 10:11:51 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-bebfcfed-f7d0-4632-a3c1-04914e2c4cae/userFiles-94ae2b8c-81b5-4117-bb12-90dbc654dfbd/SparkJNIPi.so
16/10/30 10:11:51 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:11:51 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:11:51 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:11:51 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:11:51 INFO DAGScheduler: Missing parents: List()
16/10/30 10:11:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:11:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:11:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:11:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42920 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:11:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:11:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:11:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:11:52 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:52 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:52 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:52 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:52 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:11:52 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:11:52 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:11:52 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:11:52 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818711434
16/10/30 10:11:52 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-bebfcfed-f7d0-4632-a3c1-04914e2c4cae/userFiles-94ae2b8c-81b5-4117-bb12-90dbc654dfbd/SparkJNIPi.so
16/10/30 10:11:52 INFO Executor: Fetching spark://192.168.0.17:34680/jars/jni-spark-0.1.jar with timestamp 1477818709567
16/10/30 10:11:52 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34680 after 40 ms (0 ms spent in bootstraps)
16/10/30 10:11:52 INFO Utils: Fetching spark://192.168.0.17:34680/jars/jni-spark-0.1.jar to /tmp/spark-bebfcfed-f7d0-4632-a3c1-04914e2c4cae/userFiles-94ae2b8c-81b5-4117-bb12-90dbc654dfbd/fetchFileTemp6817593723992318274.tmp
16/10/30 10:11:52 INFO Executor: Adding file:/tmp/spark-bebfcfed-f7d0-4632-a3c1-04914e2c4cae/userFiles-94ae2b8c-81b5-4117-bb12-90dbc654dfbd/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007fac58df39dd, pid=1322, tid=0x00007fac89ae4700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid1322.log
[thread 140379011507968 also had an error]
[thread 140379009402624 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:  1322 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:11:53 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:11:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:11:53 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:11:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:11:53 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:11:53 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:11:53 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:11:53 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:11:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:11:54 INFO Utils: Successfully started service 'sparkDriver' on port 42389.
16/10/30 10:11:54 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:11:54 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:11:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07068106-a2cf-4255-b2fb-67a882162cbc
16/10/30 10:11:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:11:54 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:11:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:11:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:11:54 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42389/jars/jni-spark-0.1.jar with timestamp 1477818714568
16/10/30 10:11:54 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:11:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37685.
16/10/30 10:11:54 INFO NettyBlockTransferService: Server created on 192.168.0.17:37685
16/10/30 10:11:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 37685)
16/10/30 10:11:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:37685 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 37685)
16/10/30 10:11:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 37685)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:11:56 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818716291
16/10/30 10:11:56 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-78c1e9d7-4169-48c1-ac49-5e12b1982b0f/userFiles-90d7d8d4-038e-4486-9623-bb33f6939411/SparkJNIPi.so
16/10/30 10:11:56 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:11:56 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:11:56 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:11:56 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:11:56 INFO DAGScheduler: Missing parents: List()
16/10/30 10:11:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:11:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:11:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:11:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:37685 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:11:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:11:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:11:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:11:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:56 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:11:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:11:56 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:11:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:11:56 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:11:56 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818716291
16/10/30 10:11:56 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-78c1e9d7-4169-48c1-ac49-5e12b1982b0f/userFiles-90d7d8d4-038e-4486-9623-bb33f6939411/SparkJNIPi.so
16/10/30 10:11:56 INFO Executor: Fetching spark://192.168.0.17:42389/jars/jni-spark-0.1.jar with timestamp 1477818714568
16/10/30 10:11:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42389 after 27 ms (0 ms spent in bootstraps)
16/10/30 10:11:57 INFO Utils: Fetching spark://192.168.0.17:42389/jars/jni-spark-0.1.jar to /tmp/spark-78c1e9d7-4169-48c1-ac49-5e12b1982b0f/userFiles-90d7d8d4-038e-4486-9623-bb33f6939411/fetchFileTemp8248153714014169901.tmp
16/10/30 10:11:57 INFO Executor: Adding file:/tmp/spark-78c1e9d7-4169-48c1-ac49-5e12b1982b0f/userFiles-90d7d8d4-038e-4486-9623-bb33f6939411/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f91d105c9dd, pid=1479, tid=0x00007f9201ce4700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid1479.log
[thread 140265062754048 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:  1479 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:11:58 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:11:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:11:58 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:11:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:11:58 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:11:58 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:11:58 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:11:58 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:11:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:11:58 INFO Utils: Successfully started service 'sparkDriver' on port 34981.
16/10/30 10:11:58 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:11:59 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:11:59 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7b0ee7d7-d5ca-4e85-9aac-c742cad55579
16/10/30 10:11:59 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:11:59 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:11:59 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:11:59 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:11:59 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34981/jars/jni-spark-0.1.jar with timestamp 1477818719354
16/10/30 10:11:59 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:11:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40576.
16/10/30 10:11:59 INFO NettyBlockTransferService: Server created on 192.168.0.17:40576
16/10/30 10:11:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40576)
16/10/30 10:11:59 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40576 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40576)
16/10/30 10:11:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40576)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:12:01 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818721206
16/10/30 10:12:01 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6f797980-21b4-437c-870a-8cadca95c01e/userFiles-70c8781d-0b9f-4644-858f-0d75408754e5/SparkJNIPi.so
16/10/30 10:12:01 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:12:01 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:12:01 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:12:01 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:12:01 INFO DAGScheduler: Missing parents: List()
16/10/30 10:12:01 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:12:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:12:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:12:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40576 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:12:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:12:01 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:12:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:12:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:01 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:01 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:01 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:01 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:12:01 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:12:01 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:12:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:12:01 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818721206
16/10/30 10:12:01 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6f797980-21b4-437c-870a-8cadca95c01e/userFiles-70c8781d-0b9f-4644-858f-0d75408754e5/SparkJNIPi.so
16/10/30 10:12:01 INFO Executor: Fetching spark://192.168.0.17:34981/jars/jni-spark-0.1.jar with timestamp 1477818719354
16/10/30 10:12:01 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34981 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:12:01 INFO Utils: Fetching spark://192.168.0.17:34981/jars/jni-spark-0.1.jar to /tmp/spark-6f797980-21b4-437c-870a-8cadca95c01e/userFiles-70c8781d-0b9f-4644-858f-0d75408754e5/fetchFileTemp4588814570946320831.tmp
16/10/30 10:12:02 INFO Executor: Adding file:/tmp/spark-6f797980-21b4-437c-870a-8cadca95c01e/userFiles-70c8781d-0b9f-4644-858f-0d75408754e5/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f8db63ca9dd, pid=1627, tid=0x00007f8dbe4e5700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid1627.log
[thread 140246762026752 also had an error]
[thread 140246760974080 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9:  1627 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:12:03 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:12:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:12:03 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:12:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:12:03 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:12:03 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:12:03 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:12:03 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:12:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:12:03 INFO Utils: Successfully started service 'sparkDriver' on port 46536.
16/10/30 10:12:03 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:12:03 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:12:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2ab8ae22-d2d8-450e-81af-cfcd42039701
16/10/30 10:12:03 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:12:04 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:12:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:12:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:12:04 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46536/jars/jni-spark-0.1.jar with timestamp 1477818724288
16/10/30 10:12:04 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:12:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45528.
16/10/30 10:12:04 INFO NettyBlockTransferService: Server created on 192.168.0.17:45528
16/10/30 10:12:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45528)
16/10/30 10:12:04 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45528 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45528)
16/10/30 10:12:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45528)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:12:06 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818726085
16/10/30 10:12:06 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-f1c3eeb7-1d3c-4742-8ef9-cef6b0748fcb/userFiles-5f02cb85-56cc-44eb-a6e6-1af7b86f7f4c/SparkJNIPi.so
16/10/30 10:12:06 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:12:06 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:12:06 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:12:06 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:12:06 INFO DAGScheduler: Missing parents: List()
16/10/30 10:12:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:12:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:12:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:12:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45528 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:12:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:12:06 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:12:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:12:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:06 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:06 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:06 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:06 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:12:06 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:12:06 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:12:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:12:06 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818726085
16/10/30 10:12:06 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-f1c3eeb7-1d3c-4742-8ef9-cef6b0748fcb/userFiles-5f02cb85-56cc-44eb-a6e6-1af7b86f7f4c/SparkJNIPi.so
16/10/30 10:12:06 INFO Executor: Fetching spark://192.168.0.17:46536/jars/jni-spark-0.1.jar with timestamp 1477818724288
16/10/30 10:12:06 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46536 after 27 ms (0 ms spent in bootstraps)
16/10/30 10:12:06 INFO Utils: Fetching spark://192.168.0.17:46536/jars/jni-spark-0.1.jar to /tmp/spark-f1c3eeb7-1d3c-4742-8ef9-cef6b0748fcb/userFiles-5f02cb85-56cc-44eb-a6e6-1af7b86f7f4c/fetchFileTemp6070671356964271952.tmp
16/10/30 10:12:06 INFO Executor: Adding file:/tmp/spark-f1c3eeb7-1d3c-4742-8ef9-cef6b0748fcb/userFiles-5f02cb85-56cc-44eb-a6e6-1af7b86f7f4c/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f5ecf8b9f30, pid=1761, tid=0x00007f5ed38f1700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid1761.log
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9:  1761 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:12:08 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:12:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:12:08 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:12:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:12:08 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:12:08 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:12:08 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:12:08 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:12:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:12:08 INFO Utils: Successfully started service 'sparkDriver' on port 33848.
16/10/30 10:12:08 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:12:08 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:12:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-32a1c525-23ff-46e5-92e1-71863e185195
16/10/30 10:12:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:12:09 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:12:09 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:12:09 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:12:09 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33848/jars/jni-spark-0.1.jar with timestamp 1477818729227
16/10/30 10:12:09 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:12:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45890.
16/10/30 10:12:09 INFO NettyBlockTransferService: Server created on 192.168.0.17:45890
16/10/30 10:12:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 45890)
16/10/30 10:12:09 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:45890 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 45890)
16/10/30 10:12:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 45890)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:12:11 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818731009
16/10/30 10:12:11 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2b2e2924-1c5f-4718-882a-50b137cd0d83/userFiles-3d0e80c7-e5dc-4e8f-9379-9a46e581235f/SparkJNIPi.so
16/10/30 10:12:11 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:12:11 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:12:11 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:12:11 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:12:11 INFO DAGScheduler: Missing parents: List()
16/10/30 10:12:11 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:12:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:12:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:12:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:45890 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:12:11 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:12:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:12:11 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:12:11 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:11 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:11 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:11 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:11 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:12:11 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:12:11 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:12:11 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:12:11 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818731009
16/10/30 10:12:11 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2b2e2924-1c5f-4718-882a-50b137cd0d83/userFiles-3d0e80c7-e5dc-4e8f-9379-9a46e581235f/SparkJNIPi.so
16/10/30 10:12:11 INFO Executor: Fetching spark://192.168.0.17:33848/jars/jni-spark-0.1.jar with timestamp 1477818729227
16/10/30 10:12:11 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33848 after 24 ms (0 ms spent in bootstraps)
16/10/30 10:12:11 INFO Utils: Fetching spark://192.168.0.17:33848/jars/jni-spark-0.1.jar to /tmp/spark-2b2e2924-1c5f-4718-882a-50b137cd0d83/userFiles-3d0e80c7-e5dc-4e8f-9379-9a46e581235f/fetchFileTemp972399347488061628.tmp
16/10/30 10:12:11 INFO Executor: Adding file:/tmp/spark-2b2e2924-1c5f-4718-882a-50b137cd0d83/userFiles-3d0e80c7-e5dc-4e8f-9379-9a46e581235f/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f37eebe2f30, pid=1882, tid=0x00007f385f8fc700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid1882.log
[thread 139880086554368 also had an error]
[thread 139880088659712 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:  1882 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:12:12 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:12:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:12:13 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:12:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:12:13 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:12:13 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:12:13 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:12:13 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:12:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:12:13 INFO Utils: Successfully started service 'sparkDriver' on port 42880.
16/10/30 10:12:13 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:12:13 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:12:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-190fffea-5365-40b7-a7fb-dd9e9c2f1fb5
16/10/30 10:12:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:12:13 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:12:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:12:13 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:12:13 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:42880/jars/jni-spark-0.1.jar with timestamp 1477818733940
16/10/30 10:12:14 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:12:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34616.
16/10/30 10:12:14 INFO NettyBlockTransferService: Server created on 192.168.0.17:34616
16/10/30 10:12:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 34616)
16/10/30 10:12:14 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:34616 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 34616)
16/10/30 10:12:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 34616)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:12:15 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818735672
16/10/30 10:12:15 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-97508a2f-a4c2-4ff9-85e2-1859cfdfe887/userFiles-778ef66a-50cb-45d6-8d39-1ca5eb07242e/SparkJNIPi.so
16/10/30 10:12:15 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:12:15 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:12:15 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:12:15 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:12:15 INFO DAGScheduler: Missing parents: List()
16/10/30 10:12:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:12:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:12:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:12:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:34616 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:12:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:12:16 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:12:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:12:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:16 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:16 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:16 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:12:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:12:16 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:12:16 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:12:16 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:12:16 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818735672
16/10/30 10:12:16 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-97508a2f-a4c2-4ff9-85e2-1859cfdfe887/userFiles-778ef66a-50cb-45d6-8d39-1ca5eb07242e/SparkJNIPi.so
16/10/30 10:12:16 INFO Executor: Fetching spark://192.168.0.17:42880/jars/jni-spark-0.1.jar with timestamp 1477818733940
16/10/30 10:12:16 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:42880 after 34 ms (0 ms spent in bootstraps)
16/10/30 10:12:16 INFO Utils: Fetching spark://192.168.0.17:42880/jars/jni-spark-0.1.jar to /tmp/spark-97508a2f-a4c2-4ff9-85e2-1859cfdfe887/userFiles-778ef66a-50cb-45d6-8d39-1ca5eb07242e/fetchFileTemp8018651068463044791.tmp
16/10/30 10:12:16 INFO Executor: Adding file:/tmp/spark-97508a2f-a4c2-4ff9-85e2-1859cfdfe887/userFiles-778ef66a-50cb-45d6-8d39-1ca5eb07242e/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007ff0533de7c5, pid=2005, tid=0x00007ff02c1f3700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libc.so.6+0x7e7c5][thread 140667684587264 also had an error]
[thread 140667685639936 also had an error]
# [ timer expired, abort... ]
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
./runTimesModes.sh: line 9:  2005 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:17 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:17 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:17 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:17 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:17 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:17 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:18 INFO Utils: Successfully started service 'sparkDriver' on port 41986.
16/10/30 10:14:18 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:18 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cb02ae83-41fc-400f-9eef-96f77122f7b6
16/10/30 10:14:18 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:18 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:18 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:18 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41986/jars/jni-spark-0.1.jar with timestamp 1477818858727
16/10/30 10:14:18 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44069.
16/10/30 10:14:18 INFO NettyBlockTransferService: Server created on 192.168.0.17:44069
16/10/30 10:14:18 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44069)
16/10/30 10:14:18 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44069 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44069)
16/10/30 10:14:18 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44069)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:20 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818860349
16/10/30 10:14:20 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2f18601f-cc9b-44f8-8b6a-a577b36945fe/userFiles-94c19d86-966f-415e-9367-47ff8cfd81ba/SparkJNIPi.so
16/10/30 10:14:20 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:20 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:20 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:20 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:20 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:20 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:20 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:20 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:20 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44069 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:20 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:20 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:20 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:20 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:20 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:20 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:20 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:20 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:20 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:20 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:20 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:20 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818860349
16/10/30 10:14:20 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2f18601f-cc9b-44f8-8b6a-a577b36945fe/userFiles-94c19d86-966f-415e-9367-47ff8cfd81ba/SparkJNIPi.so
16/10/30 10:14:20 INFO Executor: Fetching spark://192.168.0.17:41986/jars/jni-spark-0.1.jar with timestamp 1477818858727
16/10/30 10:14:21 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41986 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:14:21 INFO Utils: Fetching spark://192.168.0.17:41986/jars/jni-spark-0.1.jar to /tmp/spark-2f18601f-cc9b-44f8-8b6a-a577b36945fe/userFiles-94c19d86-966f-415e-9367-47ff8cfd81ba/fetchFileTemp1530643570073600933.tmp
16/10/30 10:14:21 INFO Executor: Adding file:/tmp/spark-2f18601f-cc9b-44f8-8b6a-a577b36945fe/userFiles-94c19d86-966f-415e-9367-47ff8cfd81ba/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f9140bdd9dd, pid=2176, tid=0x00007f91b0ec4700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x29dd]  pci_device_next+0xdd
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# [thread 140263716312832 also had an error]
/home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid2176.log
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:  2176 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:22 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:22 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:22 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:22 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:22 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:22 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:22 INFO Utils: Successfully started service 'sparkDriver' on port 37818.
16/10/30 10:14:22 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:22 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5a0c3275-5305-4e3b-b676-12d3467b780f
16/10/30 10:14:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:23 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:23 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:37818/jars/jni-spark-0.1.jar with timestamp 1477818863263
16/10/30 10:14:23 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44905.
16/10/30 10:14:23 INFO NettyBlockTransferService: Server created on 192.168.0.17:44905
16/10/30 10:14:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44905)
16/10/30 10:14:23 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44905 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44905)
16/10/30 10:14:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44905)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:24 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818864952
16/10/30 10:14:24 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8953c342-6b6d-4a14-9cc8-6a903c088901/userFiles-fc4fae41-2ccc-4895-9c49-c8fd18e700c9/SparkJNIPi.so
16/10/30 10:14:25 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:25 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:25 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:25 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:25 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:25 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44905 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:25 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:25 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:25 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:25 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:25 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:25 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:25 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:25 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:25 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:25 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:25 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818864952
16/10/30 10:14:25 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8953c342-6b6d-4a14-9cc8-6a903c088901/userFiles-fc4fae41-2ccc-4895-9c49-c8fd18e700c9/SparkJNIPi.so
16/10/30 10:14:25 INFO Executor: Fetching spark://192.168.0.17:37818/jars/jni-spark-0.1.jar with timestamp 1477818863263
16/10/30 10:14:25 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:37818 after 36 ms (0 ms spent in bootstraps)
16/10/30 10:14:25 INFO Utils: Fetching spark://192.168.0.17:37818/jars/jni-spark-0.1.jar to /tmp/spark-8953c342-6b6d-4a14-9cc8-6a903c088901/userFiles-fc4fae41-2ccc-4895-9c49-c8fd18e700c9/fetchFileTemp2927771450344334372.tmp
16/10/30 10:14:25 INFO Executor: Adding file:/tmp/spark-8953c342-6b6d-4a14-9cc8-6a903c088901/userFiles-fc4fae41-2ccc-4895-9c49-c8fd18e700c9/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
[xcb] Unknown sequence number while processing reply
[xcb] Most likely this is a multi-threaded client and XInitThreads has not been called
[xcb] Aborting, sorry about that.
java: ../../src/xcb_io.c:635: _XReply: Assertion `!xcb_xlib_threads_sequence_lost' failed.
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9:  2286 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:26 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:27 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:27 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:27 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:27 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:27 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:27 INFO Utils: Successfully started service 'sparkDriver' on port 34749.
16/10/30 10:14:27 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:27 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3969ad7d-ba6e-4eca-a7a8-9b4940aa1644
16/10/30 10:14:27 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:27 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:27 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34749/jars/jni-spark-0.1.jar with timestamp 1477818867860
16/10/30 10:14:27 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42570.
16/10/30 10:14:27 INFO NettyBlockTransferService: Server created on 192.168.0.17:42570
16/10/30 10:14:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 42570)
16/10/30 10:14:27 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:42570 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 42570)
16/10/30 10:14:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 42570)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:29 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818869560
16/10/30 10:14:29 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-2e7e4144-5a82-46c6-8ca2-909b3d9b3120/userFiles-894a3c26-5874-4354-a91c-cbd8b7603289/SparkJNIPi.so
16/10/30 10:14:29 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:29 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:29 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:29 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:29 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:42570 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:30 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:30 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:30 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:30 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:30 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:30 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:30 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:30 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:30 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818869560
16/10/30 10:14:30 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-2e7e4144-5a82-46c6-8ca2-909b3d9b3120/userFiles-894a3c26-5874-4354-a91c-cbd8b7603289/SparkJNIPi.so
16/10/30 10:14:30 INFO Executor: Fetching spark://192.168.0.17:34749/jars/jni-spark-0.1.jar with timestamp 1477818867860
16/10/30 10:14:30 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34749 after 34 ms (0 ms spent in bootstraps)
16/10/30 10:14:30 INFO Utils: Fetching spark://192.168.0.17:34749/jars/jni-spark-0.1.jar to /tmp/spark-2e7e4144-5a82-46c6-8ca2-909b3d9b3120/userFiles-894a3c26-5874-4354-a91c-cbd8b7603289/fetchFileTemp5605481787706156085.tmp
16/10/30 10:14:30 INFO Executor: Adding file:/tmp/spark-2e7e4144-5a82-46c6-8ca2-909b3d9b3120/userFiles-894a3c26-5874-4354-a91c-cbd8b7603289/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
[xcb] Unknown sequence number while processing reply
[xcb] Most likely this is a multi-threaded client and XInitThreads has not been called
[xcb] Aborting, sorry about that.
java: ../../src/xcb_io.c:635: _XReply: Assertion `!xcb_xlib_threads_sequence_lost' failed.
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
./runTimesModes.sh: line 9:  2389 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:31 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:31 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:31 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:31 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:31 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:31 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:32 INFO Utils: Successfully started service 'sparkDriver' on port 41460.
16/10/30 10:14:32 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:32 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-811ccd62-c8cd-4c50-84c8-90c85633e948
16/10/30 10:14:32 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:32 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:32 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:32 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:41460/jars/jni-spark-0.1.jar with timestamp 1477818872596
16/10/30 10:14:32 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36094.
16/10/30 10:14:32 INFO NettyBlockTransferService: Server created on 192.168.0.17:36094
16/10/30 10:14:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 36094)
16/10/30 10:14:32 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:36094 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 36094)
16/10/30 10:14:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 36094)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:34 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818874230
16/10/30 10:14:34 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-bb0ae45a-c83d-48b2-a537-28dcdfd91dd2/userFiles-4fc6591c-7f0a-4300-8605-9065a6934417/SparkJNIPi.so
16/10/30 10:14:34 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:34 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:34 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:34 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:34 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:36094 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:34 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:34 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:34 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:34 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:34 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:34 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:34 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:34 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818874230
16/10/30 10:14:34 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-bb0ae45a-c83d-48b2-a537-28dcdfd91dd2/userFiles-4fc6591c-7f0a-4300-8605-9065a6934417/SparkJNIPi.so
16/10/30 10:14:34 INFO Executor: Fetching spark://192.168.0.17:41460/jars/jni-spark-0.1.jar with timestamp 1477818872596
16/10/30 10:14:34 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41460 after 22 ms (0 ms spent in bootstraps)
16/10/30 10:14:34 INFO Utils: Fetching spark://192.168.0.17:41460/jars/jni-spark-0.1.jar to /tmp/spark-bb0ae45a-c83d-48b2-a537-28dcdfd91dd2/userFiles-4fc6591c-7f0a-4300-8605-9065a6934417/fetchFileTemp4905909899772813745.tmp
16/10/30 10:14:34 INFO Executor: Adding file:/tmp/spark-bb0ae45a-c83d-48b2-a537-28dcdfd91dd2/userFiles-4fc6591c-7f0a-4300-8605-9065a6934417/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007efcbfdfcf30, pid=2489, tid=0x00007efd34fb8700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid2489.log
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9:  2489 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:37 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:37 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:37 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:37 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:37 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:37 INFO Utils: Successfully started service 'sparkDriver' on port 46637.
16/10/30 10:14:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-feacc1d0-69b7-4bd2-b240-cd669b841696
16/10/30 10:14:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:46637/jars/jni-spark-0.1.jar with timestamp 1477818877772
16/10/30 10:14:37 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39804.
16/10/30 10:14:37 INFO NettyBlockTransferService: Server created on 192.168.0.17:39804
16/10/30 10:14:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39804)
16/10/30 10:14:37 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39804 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39804)
16/10/30 10:14:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39804)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818879523
16/10/30 10:14:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-0233628b-cc9e-4163-9fd9-3bbbf5461c7e/userFiles-fa598cb5-1c45-46e3-9419-42800d1d530b/SparkJNIPi.so
16/10/30 10:14:39 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:39 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:39 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:39 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:39 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39804 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:40 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:40 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:40 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:40 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818879523
16/10/30 10:14:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-0233628b-cc9e-4163-9fd9-3bbbf5461c7e/userFiles-fa598cb5-1c45-46e3-9419-42800d1d530b/SparkJNIPi.so
16/10/30 10:14:40 INFO Executor: Fetching spark://192.168.0.17:46637/jars/jni-spark-0.1.jar with timestamp 1477818877772
16/10/30 10:14:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46637 after 29 ms (0 ms spent in bootstraps)
16/10/30 10:14:40 INFO Utils: Fetching spark://192.168.0.17:46637/jars/jni-spark-0.1.jar to /tmp/spark-0233628b-cc9e-4163-9fd9-3bbbf5461c7e/userFiles-fa598cb5-1c45-46e3-9419-42800d1d530b/fetchFileTemp1067055921852340205.tmp
16/10/30 10:14:40 INFO Executor: Adding file:/tmp/spark-0233628b-cc9e-4163-9fd9-3bbbf5461c7e/userFiles-fa598cb5-1c45-46e3-9419-42800d1d530b/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007faf6cfcbf30, pid=2606, tid=0x00007fafddfe7700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid2606.log
Calling method randToSum
[thread 140393308845824 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetDeviceIDs
./runTimesModes.sh: line 9:  2606 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:41 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:42 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:42 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:42 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:42 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:42 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:42 INFO Utils: Successfully started service 'sparkDriver' on port 43949.
16/10/30 10:14:42 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:42 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-75d844ca-cc51-4d2a-b2a2-f6566f3b17f9
16/10/30 10:14:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:42 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:43 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43949/jars/jni-spark-0.1.jar with timestamp 1477818883074
16/10/30 10:14:43 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41121.
16/10/30 10:14:43 INFO NettyBlockTransferService: Server created on 192.168.0.17:41121
16/10/30 10:14:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 41121)
16/10/30 10:14:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:41121 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 41121)
16/10/30 10:14:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 41121)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:44 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818884806
16/10/30 10:14:44 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-134adc71-90cc-4842-b42c-966e99e22c81/userFiles-344733cd-f9f7-4b5f-b0af-b9c6f7a5ad63/SparkJNIPi.so
16/10/30 10:14:45 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:45 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:41121 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:45 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:45 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:45 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:45 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:45 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:45 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818884806
16/10/30 10:14:45 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:45 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:45 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:45 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-134adc71-90cc-4842-b42c-966e99e22c81/userFiles-344733cd-f9f7-4b5f-b0af-b9c6f7a5ad63/SparkJNIPi.so
16/10/30 10:14:45 INFO Executor: Fetching spark://192.168.0.17:43949/jars/jni-spark-0.1.jar with timestamp 1477818883074
16/10/30 10:14:45 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43949 after 24 ms (0 ms spent in bootstraps)
16/10/30 10:14:45 INFO Utils: Fetching spark://192.168.0.17:43949/jars/jni-spark-0.1.jar to /tmp/spark-134adc71-90cc-4842-b42c-966e99e22c81/userFiles-344733cd-f9f7-4b5f-b0af-b9c6f7a5ad63/fetchFileTemp7256588118764314298.tmp
16/10/30 10:14:45 INFO Executor: Adding file:/tmp/spark-134adc71-90cc-4842-b42c-966e99e22c81/userFiles-344733cd-f9f7-4b5f-b0af-b9c6f7a5ad63/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:14:46 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 10:14:46 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:41121 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:14:46 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108481 bytes result sent via BlockManager)
16/10/30 10:14:46 INFO MemoryStore: Block taskresult_2 stored as bytes in memory (estimated size 2.0 MB, free 360.3 MB)
16/10/30 10:14:46 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 360.3 MB)
16/10/30 10:14:46 INFO BlockManagerInfo: Added taskresult_2 in memory on 192.168.0.17:41121 (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:14:46 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2108481 bytes result sent via BlockManager)
16/10/30 10:14:46 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:41121 (size: 2.0 MB, free: 360.3 MB)
16/10/30 10:14:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 10:14:46 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 358.3 MB)
16/10/30 10:14:46 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:41121 (size: 2.0 MB, free: 358.3 MB)
16/10/30 10:14:46 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108554 bytes result sent via BlockManager)
16/10/30 10:14:46 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:41121 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:14:46 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:41121 in memory (size: 2.0 MB, free: 360.3 MB)
16/10/30 10:14:46 INFO BlockManagerInfo: Removed taskresult_2 on 192.168.0.17:41121 in memory (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:14:46 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:41121 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:14:46 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:41121 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:14:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1004 ms on localhost (1/4)
16/10/30 10:14:46 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1000 ms on localhost (2/4)
16/10/30 10:14:46 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1027 ms on localhost (3/4)
16/10/30 10:14:46 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1037 ms on localhost (4/4)
16/10/30 10:14:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:14:46 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.115 s
16/10/30 10:14:46 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.357178 s
Result: 3.1418304 in 1.613 seconds
16/10/30 10:14:46 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:14:46 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:14:46 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:14:46 INFO MemoryStore: MemoryStore cleared
16/10/30 10:14:46 INFO BlockManager: BlockManager stopped
16/10/30 10:14:46 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:14:46 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:14:46 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:14:46 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:14:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-134adc71-90cc-4842-b42c-966e99e22c81
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:47 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:48 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:48 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:48 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:48 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:48 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:48 INFO Utils: Successfully started service 'sparkDriver' on port 33532.
16/10/30 10:14:48 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:48 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f0ee3e95-64b6-479e-9093-22481acaa3fc
16/10/30 10:14:48 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:48 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:48 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33532/jars/jni-spark-0.1.jar with timestamp 1477818888797
16/10/30 10:14:48 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46463.
16/10/30 10:14:48 INFO NettyBlockTransferService: Server created on 192.168.0.17:46463
16/10/30 10:14:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 46463)
16/10/30 10:14:48 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:46463 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 46463)
16/10/30 10:14:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 46463)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:50 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818890449
16/10/30 10:14:50 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-821cbe0b-7127-4744-b5e8-a1fac018c37e/userFiles-92844c2e-c236-4e3d-a248-e0bc1e9356f4/SparkJNIPi.so
16/10/30 10:14:50 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:50 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:50 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:50 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:50 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:50 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:50 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:50 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:46463 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:50 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:51 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:51 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:51 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:51 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:51 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:51 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818890449
16/10/30 10:14:51 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-821cbe0b-7127-4744-b5e8-a1fac018c37e/userFiles-92844c2e-c236-4e3d-a248-e0bc1e9356f4/SparkJNIPi.so
16/10/30 10:14:51 INFO Executor: Fetching spark://192.168.0.17:33532/jars/jni-spark-0.1.jar with timestamp 1477818888797
16/10/30 10:14:51 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33532 after 28 ms (0 ms spent in bootstraps)
16/10/30 10:14:51 INFO Utils: Fetching spark://192.168.0.17:33532/jars/jni-spark-0.1.jar to /tmp/spark-821cbe0b-7127-4744-b5e8-a1fac018c37e/userFiles-92844c2e-c236-4e3d-a248-e0bc1e9356f4/fetchFileTemp4931598688392892829.tmp
16/10/30 10:14:51 INFO Executor: Adding file:/tmp/spark-821cbe0b-7127-4744-b5e8-a1fac018c37e/userFiles-92844c2e-c236-4e3d-a248-e0bc1e9356f4/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:14:51 INFO MemoryStore: Block taskresult_2 stored as bytes in memory (estimated size 2.0 MB, free 364.3 MB)
16/10/30 10:14:51 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 2.0 MB, free 362.3 MB)
16/10/30 10:14:51 INFO BlockManagerInfo: Added taskresult_2 in memory on 192.168.0.17:46463 (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:14:51 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 2.0 MB, free 360.3 MB)
16/10/30 10:14:51 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:46463 (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:14:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2108481 bytes result sent via BlockManager)
16/10/30 10:14:51 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:46463 (size: 2.0 MB, free: 360.3 MB)
16/10/30 10:14:51 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2108481 bytes result sent via BlockManager)
16/10/30 10:14:51 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2108481 bytes result sent via BlockManager)
16/10/30 10:14:51 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 2.0 MB, free 358.3 MB)
16/10/30 10:14:51 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:46463 (size: 2.0 MB, free: 358.3 MB)
16/10/30 10:14:51 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2108481 bytes result sent via BlockManager)
16/10/30 10:14:51 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:46463 after 1 ms (0 ms spent in bootstraps)
16/10/30 10:14:52 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:46463 in memory (size: 2.0 MB, free: 360.3 MB)
16/10/30 10:14:52 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:46463 in memory (size: 2.0 MB, free: 362.3 MB)
16/10/30 10:14:52 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:46463 in memory (size: 2.0 MB, free: 364.3 MB)
16/10/30 10:14:52 INFO BlockManagerInfo: Removed taskresult_2 on 192.168.0.17:46463 in memory (size: 2.0 MB, free: 366.3 MB)
16/10/30 10:14:52 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 1011 ms on localhost (1/4)
16/10/30 10:14:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1071 ms on localhost (2/4)
16/10/30 10:14:52 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1048 ms on localhost (3/4)
16/10/30 10:14:52 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 1055 ms on localhost (4/4)
16/10/30 10:14:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:14:52 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 1.110 s
16/10/30 10:14:52 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 1.370124 s
Result: 3.1405754 in 1.641 seconds
16/10/30 10:14:52 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:14:52 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:14:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:14:52 INFO MemoryStore: MemoryStore cleared
16/10/30 10:14:52 INFO BlockManager: BlockManager stopped
16/10/30 10:14:52 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:14:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:14:52 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:14:52 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:14:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-821cbe0b-7127-4744-b5e8-a1fac018c37e
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:14:53 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:14:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:14:53 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:14:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:14:53 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:14:53 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:14:53 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:14:53 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:14:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:14:54 INFO Utils: Successfully started service 'sparkDriver' on port 44096.
16/10/30 10:14:54 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:14:54 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:14:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c0f6f877-c9b5-4f08-9990-3892c02e969b
16/10/30 10:14:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:14:54 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:14:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:14:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:14:54 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:44096/jars/jni-spark-0.1.jar with timestamp 1477818894523
16/10/30 10:14:54 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:14:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40074.
16/10/30 10:14:54 INFO NettyBlockTransferService: Server created on 192.168.0.17:40074
16/10/30 10:14:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 40074)
16/10/30 10:14:54 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:40074 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 40074)
16/10/30 10:14:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 40074)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:14:56 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818896216
16/10/30 10:14:56 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-6a385c04-4e69-4fc4-a8a2-0df5b27a0fd5/userFiles-dcb68fdc-04bc-44df-9262-5c3ef39959ac/SparkJNIPi.so
16/10/30 10:14:56 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:14:56 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:14:56 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:14:56 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:14:56 INFO DAGScheduler: Missing parents: List()
16/10/30 10:14:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:14:56 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:14:56 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:14:56 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:40074 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:14:56 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:14:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:14:56 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:14:56 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:56 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:56 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:56 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:14:56 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:14:56 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:14:56 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818896216
16/10/30 10:14:56 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:14:56 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:14:56 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-6a385c04-4e69-4fc4-a8a2-0df5b27a0fd5/userFiles-dcb68fdc-04bc-44df-9262-5c3ef39959ac/SparkJNIPi.so
16/10/30 10:14:56 INFO Executor: Fetching spark://192.168.0.17:44096/jars/jni-spark-0.1.jar with timestamp 1477818894523
16/10/30 10:14:56 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:44096 after 34 ms (6 ms spent in bootstraps)
16/10/30 10:14:56 INFO Utils: Fetching spark://192.168.0.17:44096/jars/jni-spark-0.1.jar to /tmp/spark-6a385c04-4e69-4fc4-a8a2-0df5b27a0fd5/userFiles-dcb68fdc-04bc-44df-9262-5c3ef39959ac/fetchFileTemp8456430400491353370.tmp
16/10/30 10:14:56 INFO Executor: Adding file:/tmp/spark-6a385c04-4e69-4fc4-a8a2-0df5b27a0fd5/userFiles-dcb68fdc-04bc-44df-9262-5c3ef39959ac/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:15:02 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:15:02 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:40074 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:15:02 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:02 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:15:02 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:40074 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:15:02 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:02 INFO MemoryStore: Block taskresult_2 stored as bytes in memory (estimated size 16.1 MB, free 318.1 MB)
16/10/30 10:15:02 INFO BlockManagerInfo: Added taskresult_2 in memory on 192.168.0.17:40074 (size: 16.1 MB, free: 318.1 MB)
16/10/30 10:15:02 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:03 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:40074 after 26 ms (0 ms spent in bootstraps)
16/10/30 10:15:03 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 302.0 MB)
16/10/30 10:15:03 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:40074 (size: 16.1 MB, free: 302.0 MB)
16/10/30 10:15:03 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:03 INFO BlockManagerInfo: Removed taskresult_2 on 192.168.0.17:40074 in memory (size: 16.1 MB, free: 318.1 MB)
16/10/30 10:15:03 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:40074 in memory (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:15:03 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6390 ms on localhost (1/4)
16/10/30 10:15:03 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6462 ms on localhost (2/4)
16/10/30 10:15:03 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:40074 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:15:03 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 6522 ms on localhost (3/4)
16/10/30 10:15:03 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:40074 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:15:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6637 ms on localhost (4/4)
16/10/30 10:15:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:15:03 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 6.671 s
16/10/30 10:15:03 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 6.920708 s
Result: 3.1409879 in 7.215 seconds
16/10/30 10:15:03 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:15:03 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:15:03 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:15:03 INFO MemoryStore: MemoryStore cleared
16/10/30 10:15:03 INFO BlockManager: BlockManager stopped
16/10/30 10:15:03 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:15:03 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:15:03 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:15:03 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:15:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a385c04-4e69-4fc4-a8a2-0df5b27a0fd5
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:15:04 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:15:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:15:05 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:15:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:15:05 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:15:05 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:15:05 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:15:05 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:15:05 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:15:05 INFO Utils: Successfully started service 'sparkDriver' on port 39854.
16/10/30 10:15:05 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:15:05 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:15:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-21b3a10b-f1f3-4103-8964-3a52365e6a38
16/10/30 10:15:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:15:05 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:15:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:15:05 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:15:05 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:39854/jars/jni-spark-0.1.jar with timestamp 1477818905846
16/10/30 10:15:05 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:15:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43551.
16/10/30 10:15:05 INFO NettyBlockTransferService: Server created on 192.168.0.17:43551
16/10/30 10:15:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 43551)
16/10/30 10:15:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:43551 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 43551)
16/10/30 10:15:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 43551)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:15:07 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818907543
16/10/30 10:15:07 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-fc0584af-baf2-40dd-b138-1dd90f64e695/userFiles-90fcc404-1a3f-4651-b6e5-c28e7aa6d3fd/SparkJNIPi.so
16/10/30 10:15:07 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:15:07 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:15:07 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:15:07 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:15:07 INFO DAGScheduler: Missing parents: List()
16/10/30 10:15:07 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:15:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:15:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:15:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:43551 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:15:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:15:08 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:15:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:15:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:08 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:08 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:08 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:08 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:08 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:08 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:08 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818907543
16/10/30 10:15:08 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-fc0584af-baf2-40dd-b138-1dd90f64e695/userFiles-90fcc404-1a3f-4651-b6e5-c28e7aa6d3fd/SparkJNIPi.so
16/10/30 10:15:08 INFO Executor: Fetching spark://192.168.0.17:39854/jars/jni-spark-0.1.jar with timestamp 1477818905846
16/10/30 10:15:08 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39854 after 27 ms (0 ms spent in bootstraps)
16/10/30 10:15:08 INFO Utils: Fetching spark://192.168.0.17:39854/jars/jni-spark-0.1.jar to /tmp/spark-fc0584af-baf2-40dd-b138-1dd90f64e695/userFiles-90fcc404-1a3f-4651-b6e5-c28e7aa6d3fd/fetchFileTemp5087666169957027707.tmp
16/10/30 10:15:08 INFO Executor: Adding file:/tmp/spark-fc0584af-baf2-40dd-b138-1dd90f64e695/userFiles-90fcc404-1a3f-4651-b6e5-c28e7aa6d3fd/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:15:13 INFO MemoryStore: Block taskresult_2 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:15:13 INFO BlockManagerInfo: Added taskresult_2 in memory on 192.168.0.17:43551 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:15:13 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:13 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:15:13 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:43551 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:15:13 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:13 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43551 after 2 ms (0 ms spent in bootstraps)
16/10/30 10:15:13 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 318.1 MB)
16/10/30 10:15:13 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:43551 (size: 16.1 MB, free: 318.1 MB)
16/10/30 10:15:13 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:13 INFO BlockManagerInfo: Removed taskresult_2 on 192.168.0.17:43551 in memory (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:15:13 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 318.1 MB)
16/10/30 10:15:13 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:43551 (size: 16.1 MB, free: 318.1 MB)
16/10/30 10:15:13 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:13 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:43551 in memory (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:15:13 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 5443 ms on localhost (1/4)
16/10/30 10:15:13 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:43551 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:15:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5575 ms on localhost (2/4)
16/10/30 10:15:13 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 5670 ms on localhost (3/4)
16/10/30 10:15:13 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:43551 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:15:13 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 5732 ms on localhost (4/4)
16/10/30 10:15:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:15:13 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 5.787 s
16/10/30 10:15:13 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 6.041793 s
Result: 3.1421738 in 6.319 seconds
16/10/30 10:15:13 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:15:13 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:15:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:15:13 INFO MemoryStore: MemoryStore cleared
16/10/30 10:15:13 INFO BlockManager: BlockManager stopped
16/10/30 10:15:13 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:15:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:15:13 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:15:13 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:15:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-fc0584af-baf2-40dd-b138-1dd90f64e695
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:15:15 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:15:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:15:16 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:15:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:15:16 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:15:16 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:15:16 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:15:16 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:15:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:15:16 INFO Utils: Successfully started service 'sparkDriver' on port 33237.
16/10/30 10:15:16 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:15:16 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:15:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-7e1b60b6-fdc9-47e1-8966-e67703b5a4ec
16/10/30 10:15:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:15:16 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:15:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:15:16 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:15:16 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:33237/jars/jni-spark-0.1.jar with timestamp 1477818916988
16/10/30 10:15:17 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:15:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33767.
16/10/30 10:15:17 INFO NettyBlockTransferService: Server created on 192.168.0.17:33767
16/10/30 10:15:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 33767)
16/10/30 10:15:17 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:33767 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 33767)
16/10/30 10:15:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 33767)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	int* CPPSumArray::getsum(){
		return sum;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum_length = sum_lengtharg;
	sum = sumarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:15:18 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818918663
16/10/30 10:15:18 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-3867eb36-997a-4c27-826c-a0ba7f9d6ab1/userFiles-93b15447-a8a5-4808-a86f-150b2731225f/SparkJNIPi.so
16/10/30 10:15:18 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:15:18 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:15:18 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:15:18 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:15:18 INFO DAGScheduler: Missing parents: List()
16/10/30 10:15:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:15:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:15:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:15:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:33767 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:15:19 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:15:19 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:15:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:15:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:19 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:19 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:19 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:19 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:19 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:19 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:19 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818918663
16/10/30 10:15:19 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-3867eb36-997a-4c27-826c-a0ba7f9d6ab1/userFiles-93b15447-a8a5-4808-a86f-150b2731225f/SparkJNIPi.so
16/10/30 10:15:19 INFO Executor: Fetching spark://192.168.0.17:33237/jars/jni-spark-0.1.jar with timestamp 1477818916988
16/10/30 10:15:19 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:33237 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:15:19 INFO Utils: Fetching spark://192.168.0.17:33237/jars/jni-spark-0.1.jar to /tmp/spark-3867eb36-997a-4c27-826c-a0ba7f9d6ab1/userFiles-93b15447-a8a5-4808-a86f-150b2731225f/fetchFileTemp3345647700022139444.tmp
16/10/30 10:15:19 INFO Executor: Adding file:/tmp/spark-3867eb36-997a-4c27-826c-a0ba7f9d6ab1/userFiles-93b15447-a8a5-4808-a86f-150b2731225f/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:15:24 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f75899cef30, pid=3191, tid=0x00007f7600cc2700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_111-b14) (build 1.8.0_111-b14)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.111-b14 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  [libpciaccess.so.0+0x5f30]16/10/30 10:15:24 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:33767 (size: 16.1 MB, free: 350.2 MB)

#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# An error report file with more information is saved as:
# /home/tudor/Desktop/SparkCL/spark-ucores/spark-2.0.1-bin-hadoop2.7/hs_err_pid3191.log
16/10/30 10:15:24 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
[thread 140144797300480 also had an error]
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
./runTimesModes.sh: line 9:  3191 Aborted                 (core dumped) ./bin/spark-submit --class examples.sparkJNIPi.SparkJNIPi --master local[$noexecs] --conf spark.executor.memory=4G /home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar $slicesVal $sliceSizeVal $noexecs
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:15:25 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:15:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:15:25 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:15:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:15:25 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:15:25 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:15:25 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:15:25 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:15:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:15:26 INFO Utils: Successfully started service 'sparkDriver' on port 34196.
16/10/30 10:15:26 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:15:26 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:15:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ac58149d-dca4-4983-a316-aa5cca41fc2d
16/10/30 10:15:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:15:26 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:15:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:15:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:15:26 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:34196/jars/jni-spark-0.1.jar with timestamp 1477818926741
16/10/30 10:15:26 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:15:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39623.
16/10/30 10:15:26 INFO NettyBlockTransferService: Server created on 192.168.0.17:39623
16/10/30 10:15:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 39623)
16/10/30 10:15:26 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:39623 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 39623)
16/10/30 10:15:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 39623)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:15:28 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818928417
16/10/30 10:15:28 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-8a5ee3a5-8a6a-4918-8539-810154085707/userFiles-2a0fe637-aea6-4c5e-8482-97d438c3acd5/SparkJNIPi.so
16/10/30 10:15:28 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:15:28 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:15:28 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:15:28 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:15:28 INFO DAGScheduler: Missing parents: List()
16/10/30 10:15:28 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:15:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:15:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:15:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:39623 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:15:28 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:15:28 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:15:28 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:15:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:28 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:28 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:28 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:29 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:29 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:29 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:29 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818928417
16/10/30 10:15:29 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-8a5ee3a5-8a6a-4918-8539-810154085707/userFiles-2a0fe637-aea6-4c5e-8482-97d438c3acd5/SparkJNIPi.so
16/10/30 10:15:29 INFO Executor: Fetching spark://192.168.0.17:34196/jars/jni-spark-0.1.jar with timestamp 1477818926741
16/10/30 10:15:29 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:34196 after 33 ms (0 ms spent in bootstraps)
16/10/30 10:15:29 INFO Utils: Fetching spark://192.168.0.17:34196/jars/jni-spark-0.1.jar to /tmp/spark-8a5ee3a5-8a6a-4918-8539-810154085707/userFiles-2a0fe637-aea6-4c5e-8482-97d438c3acd5/fetchFileTemp8527645206025751656.tmp
16/10/30 10:15:29 INFO Executor: Adding file:/tmp/spark-8a5ee3a5-8a6a-4918-8539-810154085707/userFiles-2a0fe637-aea6-4c5e-8482-97d438c3acd5/jni-spark-0.1.jar to class loader
Calling method randToSum
Calling method randToSum
Calling method randToSum
16/10/30 10:15:34 INFO MemoryStore: Block taskresult_2 stored as bytes in memory (estimated size 16.1 MB, free 350.2 MB)
16/10/30 10:15:34 INFO BlockManagerInfo: Added taskresult_2 in memory on 192.168.0.17:39623 (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:15:34 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:34 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:39623 after 12 ms (0 ms spent in bootstraps)
Calling method randToSum
16/10/30 10:15:35 INFO MemoryStore: Block taskresult_1 stored as bytes in memory (estimated size 16.1 MB, free 334.1 MB)
16/10/30 10:15:35 INFO BlockManagerInfo: Added taskresult_1 in memory on 192.168.0.17:39623 (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:15:35 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:35 INFO MemoryStore: Block taskresult_3 stored as bytes in memory (estimated size 16.1 MB, free 318.1 MB)
16/10/30 10:15:35 INFO BlockManagerInfo: Added taskresult_3 in memory on 192.168.0.17:39623 (size: 16.1 MB, free: 318.1 MB)
16/10/30 10:15:35 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:35 INFO MemoryStore: Block taskresult_0 stored as bytes in memory (estimated size 16.1 MB, free 302.0 MB)
16/10/30 10:15:35 INFO BlockManagerInfo: Added taskresult_0 in memory on 192.168.0.17:39623 (size: 16.1 MB, free: 302.0 MB)
16/10/30 10:15:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 16860298 bytes result sent via BlockManager)
16/10/30 10:15:35 INFO BlockManagerInfo: Removed taskresult_2 on 192.168.0.17:39623 in memory (size: 16.1 MB, free: 318.1 MB)
16/10/30 10:15:35 INFO BlockManagerInfo: Removed taskresult_1 on 192.168.0.17:39623 in memory (size: 16.1 MB, free: 334.1 MB)
16/10/30 10:15:35 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 6315 ms on localhost (1/4)
16/10/30 10:15:35 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 6409 ms on localhost (2/4)
16/10/30 10:15:35 INFO BlockManagerInfo: Removed taskresult_3 on 192.168.0.17:39623 in memory (size: 16.1 MB, free: 350.2 MB)
16/10/30 10:15:35 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 6471 ms on localhost (3/4)
16/10/30 10:15:35 INFO BlockManagerInfo: Removed taskresult_0 on 192.168.0.17:39623 in memory (size: 16.1 MB, free: 366.3 MB)
16/10/30 10:15:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6576 ms on localhost (4/4)
16/10/30 10:15:35 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) finished in 6.616 s
16/10/30 10:15:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
16/10/30 10:15:35 INFO DAGScheduler: Job 0 finished: reduce at SparkJNIPi.java:38, took 6.848101 s
Result: 3.1419256 in 7.137 seconds
16/10/30 10:15:35 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:15:35 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:15:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:15:35 INFO MemoryStore: MemoryStore cleared
16/10/30 10:15:35 INFO BlockManager: BlockManager stopped
16/10/30 10:15:35 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:15:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:15:35 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:15:35 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:15:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-8a5ee3a5-8a6a-4918-8539-810154085707
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clGetPlatformIDs
platforms:1
	(0)CL_SUCCESS on clGetDeviceIDs
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clCreateContext
	(0)CL_SUCCESS on clCreateCommandQueue
	(0)CL_SUCCESS on clCreateProgramWithSource
	(0)CL_SUCCESS on clEnqueueReadBuffer
	(0)CL_SUCCESS on clCreateKernel
	(0)CL_SUCCESS on clEnqueueWriteBuffer
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clSetKernelArg
	(0)CL_SUCCESS on clEnqueueNDRangeKernel
	(0)CL_SUCCESS on clEnqueueReadBuffer
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:15:36 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:15:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:15:37 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:15:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:15:37 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:15:37 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:15:37 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:15:37 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:15:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:15:37 INFO Utils: Successfully started service 'sparkDriver' on port 45932.
16/10/30 10:15:37 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:15:37 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:15:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-20ea4e27-a548-4a7a-9dea-d517f19afc61
16/10/30 10:15:37 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:15:37 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:15:37 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:15:37 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:15:37 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:45932/jars/jni-spark-0.1.jar with timestamp 1477818937998
16/10/30 10:15:38 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:15:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44493.
16/10/30 10:15:38 INFO NettyBlockTransferService: Server created on 192.168.0.17:44493
16/10/30 10:15:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 44493)
16/10/30 10:15:38 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:44493 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 44493)
16/10/30 10:15:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 44493)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:15:39 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818939756
16/10/30 10:15:39 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-d1a9bf4f-56fc-40e9-9274-3ae2d2b1eba6/userFiles-1d32e386-7c12-4cca-809b-e969e23976ae/SparkJNIPi.so
16/10/30 10:15:40 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:15:40 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:15:40 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:15:40 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:15:40 INFO DAGScheduler: Missing parents: List()
16/10/30 10:15:40 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:15:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:15:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:15:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:44493 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:15:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:15:40 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:15:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:15:40 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:40 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:40 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:40 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:40 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:40 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:40 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:40 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818939756
16/10/30 10:15:40 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-d1a9bf4f-56fc-40e9-9274-3ae2d2b1eba6/userFiles-1d32e386-7c12-4cca-809b-e969e23976ae/SparkJNIPi.so
16/10/30 10:15:40 INFO Executor: Fetching spark://192.168.0.17:45932/jars/jni-spark-0.1.jar with timestamp 1477818937998
16/10/30 10:15:40 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:45932 after 25 ms (0 ms spent in bootstraps)
16/10/30 10:15:40 INFO Utils: Fetching spark://192.168.0.17:45932/jars/jni-spark-0.1.jar to /tmp/spark-d1a9bf4f-56fc-40e9-9274-3ae2d2b1eba6/userFiles-1d32e386-7c12-4cca-809b-e969e23976ae/fetchFileTemp8327937736665566293.tmp
16/10/30 10:15:40 INFO Executor: Adding file:/tmp/spark-d1a9bf4f-56fc-40e9-9274-3ae2d2b1eba6/userFiles-1d32e386-7c12-4cca-809b-e969e23976ae/jni-spark-0.1.jar to class loader
16/10/30 10:15:41 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:41 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:41 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-3,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:41 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:15:41 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:41 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:15:41 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:15:41 ERROR TaskSetManager: Task 3 in stage 0.0 failed 1 times; aborting job
16/10/30 10:15:41 INFO TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) on executor localhost: java.lang.OutOfMemoryError (Java heap space) [duplicate 1]
16/10/30 10:15:41 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerTaskEnd(0,0,ResultTask,ExceptionFailure(java.lang.OutOfMemoryError,Java heap space,[Ljava.lang.StackTraceElement;@5385ec62,java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
,Some(org.apache.spark.ThrowableSerializationWrapper@1a4831df),Vector(AccumulableInfo(1,Some(internal.metrics.executorRunTime),Some(711),None,true,true,None), AccumulableInfo(2,Some(internal.metrics.resultSize),Some(0),None,true,true,None), AccumulableInfo(3,Some(internal.metrics.jvmGCTime),Some(575),None,true,true,None)),Vector(LongAccumulator(id: 1, name: Some(internal.metrics.executorRunTime), value: 711), LongAccumulator(id: 2, name: Some(internal.metrics.resultSize), value: 0), LongAccumulator(id: 3, name: Some(internal.metrics.jvmGCTime), value: 575))),org.apache.spark.scheduler.TaskInfo@56408481,org.apache.spark.executor.TaskMetrics@395880ee)
16/10/30 10:15:41 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 1.296024 s
16/10/30 10:15:41 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 1.048 s
16/10/30 10:15:41 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@2f8d5be5)
Exception in thread "main" org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:816)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:816)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1685)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1604)
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1798)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1287)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1797)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:559)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:215)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1877)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:187)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:187)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:177)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/10/30 10:15:41 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1477818941318,JobFailed(org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down))
16/10/30 10:15:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:15:41 INFO MemoryStore: MemoryStore cleared
16/10/30 10:15:41 INFO BlockManager: BlockManager stopped
16/10/30 10:15:41 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:15:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:15:41 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:15:41 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:15:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-d1a9bf4f-56fc-40e9-9274-3ae2d2b1eba6
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:15:42 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:15:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:15:43 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:15:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:15:43 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:15:43 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:15:43 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:15:43 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:15:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:15:43 INFO Utils: Successfully started service 'sparkDriver' on port 36458.
16/10/30 10:15:43 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:15:43 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:15:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-70be2aa4-e3b5-4069-87b1-b4ed2c8393c5
16/10/30 10:15:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:15:43 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:15:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:15:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:15:43 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:36458/jars/jni-spark-0.1.jar with timestamp 1477818943718
16/10/30 10:15:43 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:15:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32807.
16/10/30 10:15:43 INFO NettyBlockTransferService: Server created on 192.168.0.17:32807
16/10/30 10:15:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 32807)
16/10/30 10:15:43 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:32807 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 32807)
16/10/30 10:15:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 32807)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:15:45 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818945487
16/10/30 10:15:45 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-27c60b43-5faf-441f-8acc-67bf6118fb13/userFiles-165235b6-7397-4e86-8c75-847771dfd70c/SparkJNIPi.so
16/10/30 10:15:45 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:15:45 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:15:45 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:15:45 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:15:45 INFO DAGScheduler: Missing parents: List()
16/10/30 10:15:45 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:15:45 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:15:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:15:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:32807 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:15:45 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:15:45 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:15:45 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:15:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:46 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:46 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:46 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:46 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:46 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:46 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:46 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818945487
16/10/30 10:15:46 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-27c60b43-5faf-441f-8acc-67bf6118fb13/userFiles-165235b6-7397-4e86-8c75-847771dfd70c/SparkJNIPi.so
16/10/30 10:15:46 INFO Executor: Fetching spark://192.168.0.17:36458/jars/jni-spark-0.1.jar with timestamp 1477818943718
16/10/30 10:15:46 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:36458 after 23 ms (0 ms spent in bootstraps)
16/10/30 10:15:46 INFO Utils: Fetching spark://192.168.0.17:36458/jars/jni-spark-0.1.jar to /tmp/spark-27c60b43-5faf-441f-8acc-67bf6118fb13/userFiles-165235b6-7397-4e86-8c75-847771dfd70c/fetchFileTemp2014701388921365203.tmp
16/10/30 10:15:46 INFO Executor: Adding file:/tmp/spark-27c60b43-5faf-441f-8acc-67bf6118fb13/userFiles-165235b6-7397-4e86-8c75-847771dfd70c/jni-spark-0.1.jar to class loader
16/10/30 10:15:46 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:46 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:46 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:46 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-3,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:47 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:15:47 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:15:47 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
16/10/30 10:15:47 INFO TaskSchedulerImpl: Cancelling stage 0
16/10/30 10:15:47 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:47 INFO TaskSchedulerImpl: Stage 0 was cancelled
16/10/30 10:15:47 INFO Executor: Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:47 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 1.046 s
16/10/30 10:15:47 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 1.297527 s
16/10/30 10:15:47 INFO TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) on executor localhost: java.lang.OutOfMemoryError (Java heap space) [duplicate 1]
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:47 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:15:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:15:47 INFO MemoryStore: MemoryStore cleared
16/10/30 10:15:47 INFO BlockManager: BlockManager stopped
16/10/30 10:15:47 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:15:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:15:47 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:15:47 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:15:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-27c60b43-5faf-441f-8acc-67bf6118fb13
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:15:48 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:15:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:15:48 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:15:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:15:48 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:15:48 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:15:48 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:15:48 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:15:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:15:49 INFO Utils: Successfully started service 'sparkDriver' on port 38126.
16/10/30 10:15:49 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:15:49 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:15:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-18f388ac-3bdc-4aaf-a7a3-af0500dc98ce
16/10/30 10:15:49 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:15:49 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:15:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:15:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:15:49 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:38126/jars/jni-spark-0.1.jar with timestamp 1477818949377
16/10/30 10:15:49 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:15:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38094.
16/10/30 10:15:49 INFO NettyBlockTransferService: Server created on 192.168.0.17:38094
16/10/30 10:15:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 38094)
16/10/30 10:15:49 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:38094 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 38094)
16/10/30 10:15:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 38094)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:15:51 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818951050
16/10/30 10:15:51 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-801b1b6f-be1a-4e3d-88ec-7a7a203488e7/userFiles-a0bde05b-19e8-4c20-96db-2f79782cc748/SparkJNIPi.so
16/10/30 10:15:51 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:15:51 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:15:51 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:15:51 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:15:51 INFO DAGScheduler: Missing parents: List()
16/10/30 10:15:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:15:51 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:15:51 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:15:51 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:38094 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:15:51 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:15:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:15:51 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:15:51 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:51 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:51 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:51 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:51 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:51 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:51 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:51 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818951050
16/10/30 10:15:51 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:51 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-801b1b6f-be1a-4e3d-88ec-7a7a203488e7/userFiles-a0bde05b-19e8-4c20-96db-2f79782cc748/SparkJNIPi.so
16/10/30 10:15:51 INFO Executor: Fetching spark://192.168.0.17:38126/jars/jni-spark-0.1.jar with timestamp 1477818949377
16/10/30 10:15:51 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:38126 after 30 ms (0 ms spent in bootstraps)
16/10/30 10:15:51 INFO Utils: Fetching spark://192.168.0.17:38126/jars/jni-spark-0.1.jar to /tmp/spark-801b1b6f-be1a-4e3d-88ec-7a7a203488e7/userFiles-a0bde05b-19e8-4c20-96db-2f79782cc748/fetchFileTemp294869100261774389.tmp
16/10/30 10:15:51 INFO Executor: Adding file:/tmp/spark-801b1b6f-be1a-4e3d-88ec-7a7a203488e7/userFiles-a0bde05b-19e8-4c20-96db-2f79782cc748/jni-spark-0.1.jar to class loader
16/10/30 10:15:52 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:52 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:52 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:52 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:15:52 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker-2,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:52 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:15:52 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:15:52 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
16/10/30 10:15:52 INFO TaskSchedulerImpl: Cancelling stage 0
16/10/30 10:15:52 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:52 INFO TaskSchedulerImpl: Stage 0 was cancelled
16/10/30 10:15:52 INFO Executor: Executor is trying to kill task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:52 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 1.050 s
16/10/30 10:15:52 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@2f36a693)
16/10/30 10:15:52 INFO TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) on executor localhost: java.lang.OutOfMemoryError (Java heap space) [duplicate 1]
16/10/30 10:15:52 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 1.308803 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:52 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1477818952636,JobFailed(org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:))
16/10/30 10:15:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:15:52 INFO MemoryStore: MemoryStore cleared
16/10/30 10:15:52 INFO BlockManager: BlockManager stopped
16/10/30 10:15:52 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:15:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:15:52 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:15:52 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:15:52 INFO ShutdownHookManager: Deleting directory /tmp/spark-801b1b6f-be1a-4e3d-88ec-7a7a203488e7
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/10/30 10:15:53 INFO SparkContext: Running Spark version 2.0.1
16/10/30 10:15:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/10/30 10:15:54 WARN Utils: Your hostname, tudor-Latitude-E7470 resolves to a loopback address: 127.0.1.1; using 192.168.0.17 instead (on interface wlan0)
16/10/30 10:15:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
16/10/30 10:15:54 INFO SecurityManager: Changing view acls to: tudor
16/10/30 10:15:54 INFO SecurityManager: Changing modify acls to: tudor
16/10/30 10:15:54 INFO SecurityManager: Changing view acls groups to: 
16/10/30 10:15:54 INFO SecurityManager: Changing modify acls groups to: 
16/10/30 10:15:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tudor); groups with view permissions: Set(); users  with modify permissions: Set(tudor); groups with modify permissions: Set()
16/10/30 10:15:54 INFO Utils: Successfully started service 'sparkDriver' on port 43461.
16/10/30 10:15:54 INFO SparkEnv: Registering MapOutputTracker
16/10/30 10:15:54 INFO SparkEnv: Registering BlockManagerMaster
16/10/30 10:15:54 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0e4182bd-e21c-488e-9266-9e8f08e3579a
16/10/30 10:15:54 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
16/10/30 10:15:54 INFO SparkEnv: Registering OutputCommitCoordinator
16/10/30 10:15:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
16/10/30 10:15:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.0.17:4040
16/10/30 10:15:55 INFO SparkContext: Added JAR file:/home/tudor/dev/SparkJNI/target/jni-spark-0.1.jar at spark://192.168.0.17:43461/jars/jni-spark-0.1.jar with timestamp 1477818955054
16/10/30 10:15:55 INFO Executor: Starting executor ID driver on host localhost
16/10/30 10:15:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35457.
16/10/30 10:15:55 INFO NettyBlockTransferService: Server created on 192.168.0.17:35457
16/10/30 10:15:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.17, 35457)
16/10/30 10:15:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.17:35457 with 366.3 MB RAM, BlockManagerId(driver, 192.168.0.17, 35457)
16/10/30 10:15:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.17, 35457)
/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPRandNumArray.cpp : 
#include "CPPRandNumArray.h"
	std::mutex CPPRandNumArray::mtx;
CPPRandNumArray::CPPRandNumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_randNumArray = env->GetFieldID(replaceMeClassName, "randNumArray", "[F");
	jobject randNumArray_obj = env->GetObjectField(replaceMeObjectName, j_randNumArray);

	mtx.lock();
	randNumArrayArr = reinterpret_cast<jfloatArray>(randNumArray_obj);
	randNumArray_length = env->GetArrayLength(randNumArrayArr);
	randNumArray = (float*)env->GetFloatArrayElements(randNumArrayArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int CPPRandNumArray::getrandNumArray_length(){
		return randNumArray_length;
	}

	float* CPPRandNumArray::getrandNumArray(){
		return randNumArray;
	}

	jobject CPPRandNumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPRandNumArray::CPPRandNumArray(float* randNumArrayarg, int randNumArray_lengtharg, jclass jClass, JNIEnv* jniEnv){
	randNumArray_length = randNumArray_lengtharg;
	randNumArray = randNumArrayarg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([F)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jfloatArray randNumArrayArr = env->NewFloatArray(randNumArray_lengtharg);
	env->SetFloatArrayRegion(randNumArrayArr, 0, randNumArray_lengtharg, randNumArray);
	jniJavaClassRef = env->NewObject(jClass, constructor, randNumArrayArr);
}
CPPRandNumArray::CPPRandNumArray(){
	jniJavaClassRef = NULL;
}
CPPRandNumArray::~CPPRandNumArray() {
	if(jniCreated != 0){
env->ReleaseFloatArrayElements(randNumArrayArr, randNumArray, 0);	}
}

/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/CPPSumArray.cpp : 
#include "CPPSumArray.h"
	std::mutex CPPSumArray::mtx;
CPPSumArray::CPPSumArray(jclass replaceMeClassName, jobject replaceMeObjectName, JNIEnv* env){

	jniJavaClassRef = replaceMeObjectName;
	jfieldID j_sum = env->GetFieldID(replaceMeClassName, "sum", "[I");
	jobject sum_obj = env->GetObjectField(replaceMeObjectName, j_sum);

	mtx.lock();
	sumArr = reinterpret_cast<jintArray>(sum_obj);
	sum_length = env->GetArrayLength(sumArr);
	sum = (int*)env->GetIntArrayElements(sumArr, NULL);
	mtx.unlock();
	this->env = env;
	jniCreated = 1;
}
	int* CPPSumArray::getsum(){
		return sum;
	}

	int CPPSumArray::getsum_length(){
		return sum_length;
	}

	jobject CPPSumArray::getJavaObject(){
		return jniJavaClassRef;
	}
CPPSumArray::CPPSumArray(int* sumarg, int sum_lengtharg, jclass jClass, JNIEnv* jniEnv){
	sum = sumarg;
	sum_length = sum_lengtharg;
	env = jniEnv;
	if(jClass == NULL){
		printf("Provided java class object is null..!");
		return;
	}
	jmethodID constructor = env->GetMethodID(jClass, "<init>", "([I)V");
	if(constructor == NULL){
		printf("Constructor object method is null");
		return;
	}
	jintArray sumArr = env->NewIntArray(sum_lengtharg);
	env->SetIntArrayRegion(sumArr, 0, sum_lengtharg, sum);
	jniJavaClassRef = env->NewObject(jClass, constructor, sumArr);
}
CPPSumArray::CPPSumArray(){
	jniJavaClassRef = NULL;
}
CPPSumArray::~CPPSumArray() {
	if(jniCreated != 0){
env->ReleaseIntArrayElements(sumArr, sum, 0);	}
}

[Exception] Invalid formatting for file cpprandnumarray_jObject0){ at line 
"	jclass cpprandnumarray_jClass = jniEnv->GetObjectClass(cpprandnumarray_jObject0);"
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
make: Entering directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
g++ CPPRandNumArray.cpp CPPSumArray.cpp SparkJNIPi.cpp -o SparkJNIPi.so   -I/usr/lib/jvm/java-8-oracle/include -I/usr/lib/jvm/java-8-oracle/include/linux  -shared -fPIC -std=c++11 -O3 -m64 -lrt -lpthread -fopenmp   -lOpenCL
make: Leaving directory `/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi'
16/10/30 10:15:56 INFO SparkContext: Added file /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so at file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818956768
16/10/30 10:15:56 INFO Utils: Copying /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so to /tmp/spark-7f6bf9ac-318c-46c5-ba85-6080d8119929/userFiles-4ed1b00c-269c-4c16-88bb-582f395dbe1a/SparkJNIPi.so
16/10/30 10:15:57 INFO SparkContext: Starting job: reduce at SparkJNIPi.java:38
16/10/30 10:15:57 INFO DAGScheduler: Got job 0 (reduce at SparkJNIPi.java:38) with 4 output partitions
16/10/30 10:15:57 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at SparkJNIPi.java:38)
16/10/30 10:15:57 INFO DAGScheduler: Parents of final stage: List()
16/10/30 10:15:57 INFO DAGScheduler: Missing parents: List()
16/10/30 10:15:57 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37), which has no missing parents
16/10/30 10:15:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.7 KB, free 366.3 MB)
16/10/30 10:15:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1699.0 B, free 366.3 MB)
16/10/30 10:15:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.0.17:35457 (size: 1699.0 B, free: 366.3 MB)
16/10/30 10:15:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/10/30 10:15:57 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at map at SparkJNIPi.java:37)
16/10/30 10:15:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 4 tasks
16/10/30 10:15:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:57 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:57 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, localhost, partition 2, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:57 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, localhost, partition 3, PROCESS_LOCAL, 5603 bytes)
16/10/30 10:15:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:57 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:57 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)
16/10/30 10:15:57 INFO Executor: Fetching file:/home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so with timestamp 1477818956768
16/10/30 10:15:57 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)
16/10/30 10:15:57 INFO Utils: /home/tudor/dev/SparkJNI/cppSrc/examples/SparkJNIPi/SparkJNIPi.so has been previously copied to /tmp/spark-7f6bf9ac-318c-46c5-ba85-6080d8119929/userFiles-4ed1b00c-269c-4c16-88bb-582f395dbe1a/SparkJNIPi.so
16/10/30 10:15:57 INFO Executor: Fetching spark://192.168.0.17:43461/jars/jni-spark-0.1.jar with timestamp 1477818955054
16/10/30 10:15:57 INFO TransportClientFactory: Successfully created connection to /192.168.0.17:43461 after 34 ms (0 ms spent in bootstraps)
16/10/30 10:15:57 INFO Utils: Fetching spark://192.168.0.17:43461/jars/jni-spark-0.1.jar to /tmp/spark-7f6bf9ac-318c-46c5-ba85-6080d8119929/userFiles-4ed1b00c-269c-4c16-88bb-582f395dbe1a/fetchFileTemp4421056433296102132.tmp
16/10/30 10:15:57 INFO Executor: Adding file:/tmp/spark-7f6bf9ac-318c-46c5-ba85-6080d8119929/userFiles-4ed1b00c-269c-4c16-88bb-582f395dbe1a/jni-spark-0.1.jar to class loader
16/10/30 10:15:58 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:58 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-3,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:58 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-2,5,main]
java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:58 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

16/10/30 10:15:58 INFO SparkContext: Invoking stop() from shutdown hook
16/10/30 10:15:58 ERROR TaskSetManager: Task 2 in stage 0.0 failed 1 times; aborting job
16/10/30 10:15:58 INFO TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) on executor localhost: java.lang.OutOfMemoryError (Java heap space) [duplicate 1]
16/10/30 10:15:58 INFO TaskSchedulerImpl: Cancelling stage 0
16/10/30 10:15:58 INFO Executor: Executor is trying to kill task 0.0 in stage 0.0 (TID 0)
16/10/30 10:15:58 INFO Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
16/10/30 10:15:58 INFO TaskSchedulerImpl: Stage 0 was cancelled
16/10/30 10:15:58 INFO DAGScheduler: ResultStage 0 (reduce at SparkJNIPi.java:38) failed in 1.040 s
16/10/30 10:15:58 INFO DAGScheduler: Job 0 failed: reduce at SparkJNIPi.java:38, took 1.296023 s
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1953)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:984)
	at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384)
	at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45)
	at examples.sparkJNIPi.SparkJNIPi.main(SparkJNIPi.java:38)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
16/10/30 10:15:58 INFO SparkUI: Stopped Spark web UI at http://192.168.0.17:4040
16/10/30 10:15:58 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(0,1477818958353,JobFailed(org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 0.0 failed 1 times, most recent failure: Lost task 2.0 in stage 0.0 (TID 2, localhost): java.lang.OutOfMemoryError: Java heap space
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:97)
	at examples.sparkJNIPi.SparkJNIPi$1.call(SparkJNIPi.java:94)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1028)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:185)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1336)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:988)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$15.apply(RDD.scala:986)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.SparkContext$$anonfun$32.apply(SparkContext.scala:1952)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:))
16/10/30 10:15:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/10/30 10:15:58 INFO MemoryStore: MemoryStore cleared
16/10/30 10:15:58 INFO BlockManager: BlockManager stopped
16/10/30 10:15:58 INFO BlockManagerMaster: BlockManagerMaster stopped
16/10/30 10:15:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/10/30 10:15:58 INFO SparkContext: Successfully stopped SparkContext
16/10/30 10:15:58 INFO ShutdownHookManager: Shutdown hook called
16/10/30 10:15:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f6bf9ac-318c-46c5-ba85-6080d8119929
